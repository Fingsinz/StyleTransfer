{"/StyleTransfer/ref_and_notes/evaluation.html":{"title":"风格迁移评价","content":" title: 风格迁移评价 keywords: 迁移评价 desc: 风格迁移评价 date: 2025 03 27 id: evaluation *仅讨论评估方法* ## ResNet 18 预训练模型和结构相似性 [Evaluation of Painting Artistic Style Transfer Based on Generative Adversarial Network](https://ieeexplore.ieee.org/document/10154714) *TANG Z, WU C, XIAO Y, et al. Evaluation of Painting Artistic Style Transfer Based on Generative Adversarial Network[J].* > Unlike most image style transfer tasks, which are to convert a photograph into an image with a certain artist’s style, this paper focuses on artistic style transfer in Monet painting, which is transferring a Monet painting into an image in another artist’s style. In our experiments, we successfully implemented the Cycle Consistent GAN model and applied Neural Style Transfer (NST) model for contrasting effects. In order to evaluate the result of artistic style transfer quantitatively and effectively with a low requirement for computational resources, we also proposed a quantitative method called style transfer indicator to make the comparison more obvious as the comparisons of the effect of image style transfer were mostly done by subjective analysis previously. This method takes both the style and content of the transferred image into account because whether the transferred image belongs to the new style is as important as whether the content of the image is saved. A ResNet18 pre trained model and structural similarity index are used for the evaluation of style and content respectively. The human survey that we conducted also proved the validity of our style transfer indicator. Moreover, our proposed indicator could also be applied for the evaluation of other image style transfer tasks. **摘要**：大多数图像风格转换任务都是将一张照片转换成具有某个艺术家风格的图像，而本文主要研究的是莫奈绘画中的艺术风格转换，即将莫奈的一幅画转换成另一个艺术家风格的图像。在我们的实验中，我们成功地实现了循环一致的 GAN 模型，并应用了神经风格迁移（NST）模型来对比效果。为了在对计算资源要求较低的情况下，对艺术风格迁移的效果进行定量有效的评价，我们还提出了一种称为风格迁移指标的定量方法，使图像风格迁移效果的比较更加明显。该方法同时考虑了转换图像的样式和内容，因为转换的图像是否应用新风格与图像的内容是否保持一致同样重要。ResNet 18 预训练模型和结构相似性指数分别用于风格和内容的评估。我们在人们中调查也证明了我们的风格转移指标的有效性。此外，我们提出的指标也可以应用于其他图像风格迁移任务的评估。 *在 IV. Evaluation 部分* ### 风格评分 在风格的定量分析方面，采用了风格分类器的方法。 经过训练的分类器能够区分图像的风格，可以判断风格迁移模型是否将图像迁移到正确的风格以及风格转移的程度。 其中，ResNet 是一种深度卷积神经网络模型，经过大型图像数据集彻底训练，可以学习图像分类和识别所需的许多特征。 使用预训练的 ResNet18 模型（需要微调）对生成图像进行风格分类。 通过 SoftMax 输出概率值作为风格得分（0 1），衡量生成图像与目标风格的匹配度。 ### 内容评分 在内容的定量分析方面，采用结构相似性指数 SSIM 指标。 将 SSIM 的三个组成部分，**亮度**、**对比度** 和 **结构** 相乘生成 SSIM： $$ S(x,y) f(l(x, y), c(x, y), s(x, y)) l(x, y) \\cdot c(x, y) \\cdot s(x, y) $$ 其中，$l$, $c$, $s$ 代表评估亮度、对比度和结构；$x$ 和 $y$ 分别是 2 张图像。 #### 亮度测量 亮度通过图像的均值 $\\mu$ 衡量： $$ l(x,y) \\frac{2 \\mu_x \\mu_y + C_1}{\\mu_x^2 + \\mu_y^2 + C_1} $$ 其中： $\\mu_x$ 和 $\\mu_y$ 是图像 $x$ 和 $y$ 的均值。 $C_1$ 是一个常数，用于防止分母为零的情况，通常 $C_1 (k_1L)^2$，$L$ 为像素动态范围，$k_1\\ll 1$。 #### 对比度测量 对比度通过标准差 $\\sigma$ 衡量： $$ c(x,y) \\frac{2 \\sigma_x \\sigma_y + C_2}{\\sigma_x^2 + \\sigma_y^2 + C_1} $$ 其中： $\\sigma_x$ 和 $\\sigma_y$ 是图像 $x$ 和 $y$ 的标准差。 $C_2$ 是一个常数，用于防止分母为零的情况，通常 $C_2 (k_2L)^2$，$L$ 为像素动态范围，$k_2\\ll 1$。 #### 结构测量 结构通过协方差 $\\sigma_{xy}$ 衡量： $$ s(x,y) \\frac{\\sigma_{xy} + C_3}{\\sigma_x \\sigma_y + C_3} $$ 其中： $\\sigma_{xy}$ 是图像 $x$ 和 $y$ 的协方差。 $C_3$ 是一个常数，通常取 $C_2/2$。"},"/StyleTransfer/ref_and_notes/fast_patch_based.html":{"title":"快速基于补丁的任意风格的风格转移","content":" title: 快速基于补丁的任意风格的风格转移 keywords: Fast Patch based desc: Fast Patch based Style Transfer of Arbitrary Style date: 2025 04 01 id: fast_patch_based [Fast Patch based Style Transfer of Arbitrary Style](https://arxiv.org/abs/1612.04337) *CHEN T, SCHMIDT M. Fast Patch based Style Transfer of Arbitrary Style[J]. Cornell University arXiv,Cornell University arXiv, 2016.* > Artistic style transfer is an image synthesis problem where the content of an image is reproduced with the style of another. Recent works show that a visually appealing style transfer can be achieved by using the hidden activations of a pretrained convolutional neural network. However, existing methods either apply (i) an optimization procedure that works for any style image but is very expensive, or (ii) an efficient feedforward network that only allows a limited number of trained styles. In this work we propose a simpler optimization objective based on local matching that combines the content structure and style textures in a single layer of the pretrained network. We show that our objective has desirable properties such as a simpler optimization landscape, intuitive parameter tuning, and consistent frame by frame performance on video. Furthermore, we use 80,000 natural images and 80,000 paintings to train an inverse network that approximates the result of the optimization. This results in a procedure for artistic style transfer that is efficient but also allows arbitrary content and style images. **摘要**：艺术风格转移是一个图像合成问题，其中一个图像的内容与另一个图像的风格复制。最近的研究表明，视觉上吸引人的风格转移可以通过使用预训练卷积神经网络的隐藏激活来实现。然而，现有的方法要么应用一个适用于任何风格图像但非常昂贵的优化过程，要么一个只允许有限数量的训练风格的有效前馈网络。在这项工作中，我们提出了一个基于局部匹配的更简单的优化目标，将内容结构和风格纹理结合在预训练网络的单层中。我们展示了我们的目标具有理想的属性，例如更简单的优化场景、直观的参数调优以及视频上一致的逐帧性能。此外，我们使用 80,000 张自然图像和 80,000 幅绘画来训练一个近似优化结果的逆网络。结果是得到一种高效的艺术风格转移过程，允许任意内容和风格图像。 ## 风格交换 Style Swap 设 $C$ 表示内容图像，$S$ 表示风格图像。$\\Phi(\\cdot)$ 表示预训练 CNN 模型的全卷积部分表示的函数，将图像从 RGB 映射到某个中间激活空间。计算激活值 $\\Phi(C)$ 和 $\\Phi(S)$ 后，**风格交换**如下： 1. 从内容和风格的激活中提取一组 Patches，表示为 $\\{\\phi_i (C)\\}_{i\\in n_c}$ 和 $\\{\\phi_j (S)\\}_{j\\in n_s}$，其中 $n_c$ 和 $n_s$ 为提取的 Patch 个数。提取的 Patch 应该有足够的重叠，并且包含所有的激活通道。 2. 对于每个内容激活的 Patch，根据归一化互相关度量确定最接近匹配的风格 Patch： $$ \\phi_i^{ss}(C, S) : \\arg \\mathop{\\max}\\limits_{\\phi_j(S),j 1,...,n_s} \\frac {<\\phi_i (C), \\phi_j (S)>} {\\vert\\vert \\phi_i (C) \\vert\\vert \\cdot \\vert\\vert \\phi_j (S) \\vert\\vert} $$ 3. 将每个内容激活 Patch $\\phi_i (C)$ 与其最匹配的风格 Patch $\\phi_i^{ss}(C, S)$ 进行交换。 4. 通过对步骤 3 中可能具有不同值的重叠区域进行平均，重建完整的内容激活 $\\Phi^{ss} (C, S)$。 ## 优化目标 目标是**最小化合成图像激活与目标激活 $\\Phi^{ss} (C, S)$ 的平方误差**，并加入总变差正则化（TV Loss）以平滑图像： $$ I_{stylized}(C, S) \\arg \\mathop{\\min}\\limits_{I\\in \\mathbb{R}^{h\\times w\\times d}} \\vert\\vert \\Phi(I) \\Phi^{ss}(C, S) \\vert\\vert _F^2 + \\lambda\\mathcal{l}_{TV}(I) $$ 优化过程通过反向传播完成，但由于耗时，作者进一步提出逆网络。 ## 逆网络 训练目标：学习从风格交换后的激活图 $\\Phi^{ss} (C, S)$ 直接生成图像，绕过逐次优化。 关键设计： 使用混合数据集（8 万自然图像 + 8 万绘画）训练，增强泛化能力。 引入风格交换后的激活图作为训练数据，解决 CNN 的非满射性问题。 网络架构基于转置卷积层和实例归一化（InstanceNorm），提升生成质量。 ## 实现 Torch Github：https://github.com/rtqichen/style swap PyTorch Github：https://github.com/irasin/Pytorch_Style_Swap"},"/StyleTransfer/ref_and_notes/gan.html":{"title":"GAN：生成对抗网络","content":" title: GAN：生成对抗网络 keywords: GAN desc: GAN文献及笔记 date: 2025 02 01 id: ref_GAN [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) *Goodfellow I , Pouget Abadie J , Mirza M ,et al.Generative Adversarial Nets[J].MIT Press, 2014.DOI:10.3156/JSOFT.29.5_177_2.* > We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. **摘要**：论文提出一个通过对抗过程估计生成模型的新框架。在对抗过程中，同时训练两个模型：一个捕获数据分布的生成器 G，和一个估计样本来自训练数据还是生成器 G 的判别器 D。生成器 G 的训练过程是最大化判别器 D 犯错的概率。这个框架相当于一个极大极小二人博弈游戏。在任意的 G 和 D 函数空间中，存在唯一解，使得生成器 G 能复刻训练集的数据分布，同时判别器 D 对于生成器 G 生成的任意样本作出的判断都是真假参半（真假概率各半）。如果生成器 G 和判别器 D 都定义为多层感知器，那么整个系统可以使用误差反向传播进行训练。在模型的训练过程以及样本的生成中，不需要使用马尔科夫链或者展开的近似推理网络。通过对生成的样本进行定性和定量评估，实验证明了该框架的潜力。 ## GAN 模型 GAN包含两个模型： 1. **生成器（Generator, G）**：将随机噪声映射到数据空间，目标是生成与真实数据分布 $p_{\\text{data}}$ 一致的样本。 2. **判别器（Discriminator, D）**：区分输入样本来自真实数据还是生成器，输出为样本真实性的概率。 两者通过**极小极大博弈**进行训练： 为了学习生成器在数据 $x$ 上的分布 $p_g$，定义输入噪声变量 $p_z(z)$ 的先验，然后将数据空间的映射表示为 $G(z; \\theta_g)$，其中 $G$ 是由具有参数 $\\theta_g$ 的多层感知器表示的可微函数。 定义第二个多层感知器 $D(x; \\theta_d)$，其输出一个标量。$D(x)$ 表示 $x$ 来自真实数据而不是 $p_g$ 的概率。 **训练判别器 D，最大化正确分类训练样本和生成器 G 生成样本的概率；同时训练生成器 G，最小化 $\\log(1−D(G(z)))$，即让生成样本 $G(z)$ 被判别器误判为真实样本（$D(G(z))→1$）**。 综上所述，GAN 的训练过程可表示为： $$ \\min_G \\max_D V(D, G) \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log(1 D(G(z)))]\\tag{1} $$ 当生成器 G 分布 $p_g p_{data}$，判别器 D 的最优解为 $D^*(x) \\frac{1}{2}$ 时，目标函数达到最小值 $\\log({\\frac{1}{2}}) + \\log(1 \\frac{1}{2}) \\log{4}$。 ## 网络理论 ### 算法步骤 *小批量随机梯度下降，$k$ 为超参数。* ``` for i 1, iterations do for k steps do m 个噪声样本的小批量样本 {z(1),., z(m)}，来自噪声先验 pg(z)。 m 个真实样本的小批量样本 {x(1),., x(m)}，来自真实数据集。 通过公式 2 提升判别器随机梯度来更新判别器。 end for m 个噪声样本的小批量样本 {z(1),., z(m)}，来自噪声先验 pg(z)。 通过公式 3 降低生成器随机梯度来更新生成器。 end for ``` $$ \\nabla_{\\theta_d} \\frac{1}{m} \\sum_{i 1}^m \\left[\\log D(x^{(i)}) + \\log(1 D(G(z^{(i)}))) \\right] \\tag{2} $$ $$ \\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i 1}^m \\log\\left(1 D(G(z^{(i)}))\\right) \\tag{3} $$ 训练过程的巧思： 1. **在 $k$ 步优化判别器 D 和 $1$ 步优化生成器 G 之间交替进行**：在 one step 的内部循环中优化判别器 D 在计算上是不可行的，并且容易在有限的数据集上导致过拟合。 2. **最大化 $\\log D(G(z))$ 代替最小化 $\\log(1−D(G(z)))$ 训练生成器 G**：在生成器 G 效果很差时，判别器 D可以以高置信度拒绝样本，这种情况下，$\\log(1−D(G(z)))$ 不起作用。 原文中提供下图： ![](../static/images/GAN/fig1.png) *注：黑色散点线为真实数据的分布；绿色实线为生成器 $G$ 生成数据的分布；蓝色虚线为判别器 $D$ 的分布，区分黑色散点与绿色实线。最下面的直线为均匀采样 $z$ 的域；其上面的直线是 $x$ 域的一部分。向上的箭头表示 $x G(z)$ 的映射关系。* 图 (a)：对抗接近收敛，$p_g$ 接近 $p_{data}$，判别器部分分类正确（能否分辨出真实数据和生成数据）。 图 (b)：在算法的内部循环中，判别器 D 向着分类数据训练，收敛在 $D^*(x) \\frac {p_{data}(x)}{p_{data}(x) + p_g(x)}$。 图 (c)：更新生成器 $G$ 后，判别器 $D$ 的梯度引导 $G(z)$ 偏向更有可能被归类为真实数据的区域。 图 (d)：经过若干步训练后，如果 $G$ 和 $D$ 有足够的容量，它们会收敛到 $p_g p_{data}$，此时 $D$ 无法区分出真实数据和生成数据，即 $D(x) \\frac {1}{2}$。 ### 解释全局最优解 $p_g p_{data}$ **命题 1.** 对于固定的生成器 $G$，最优判别器 $D$ 为： $$ D^*_G(x) \\frac {p_{data}(x)}{p_{data}(x) + p_g(x)} $$ **证明：** 给定任何生成器 $G$ 的判别器 $D$ 的训练标准是最大化 $V(G, D)$。 $$ V(G,D) \\int_x p_{data}(x) \\log(D(x))dx + \\int_z p_z(z) \\log(1 D(g(z)))dz\\tag{4} $$ 通过变量替换 $G(z) x \\sim p_g(x)$，式(4)改写为： $$ V(G,D) \\int_x \\left[p_{data}(x) \\log(D(x)) + p_g(x) \\log(1 D(x))\\right] dx $$ > *生成器 $G$ 将噪声输入 $z \\sim p_z(z)$ 映射为样本 $x G(z)$，隐式定义了生成样本的分布 $p_g(x)$。当 $z$ 服从噪声先验 $p_z(z)$ 分布时，$x G(z)$ 的分布即为 $p_g(x)$；若 $z \\sim p_z(z)$，则 $x G(z)\\sim p_g(x)$。* > *对于变量替换定理，对于任意函数 $h(x)$，若 $x$ 是随机变量 $z$ 的映射 $x G(z)$，则关于 $z$ 的期望可以转换为关于 $x$ 的期望：$\\mathbb{E}_{z \\sim p_z(z)}[h(G(z))] \\mathbb{E}_{x \\sim p_g(x)}[h(x)]$。* 对于每个样本 $x$，求 $D(x)$ 使得 $V(D,G)$ 最大化。这是一个单变量优化问题，最优解为： $$ D^*_G(x) \\frac {p_{data}(x)}{p_{data}(x) + p_{g}(x)} $$ 将最优判别器代入到目标函数有： $$ \\begin{aligned} C(G) & \\max_D V(G,D)\\\\ & \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[\\log D^*_G(x) \\right] + \\mathbb{E}_{z \\sim p_z} \\left[\\log(1 D^*_G(G(z))) \\right] \\\\ & \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[\\log D^*_G(x) \\right] + \\mathbb{E}_{x \\sim p_g} \\left[\\log(1 D^*_G(x)) \\right]\\\\ & \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[\\log \\frac {p_{data}(x)}{p_{data}(x) + p_{g}(x)} \\right] + \\mathbb{E}_{x \\sim p_g} \\left[\\log \\frac {p_{g}(x)}{p_{data}(x) + p_{g}(x)} \\right] \\end{aligned}\\tag{5} $$ **定理 1.** 当且仅当 $p_g p_{data}$ 时，$C(G)$ 达到的全局最小值 $ \\log 4$。 **证明：** 对于 $p_g p_{data}$，有 $D^*_G(x) \\frac{1}{2}$。因此： $$ C(G) \\mathbb{E}_{x\\sim p_{data}} \\frac {p_{data}(x)}{p_{data}(x) + p_{g}(x)} + \\mathbb{E}_{x\\sim p_{g}} \\frac {p_{g}(x)}{p_{data}(x) + p_{g}(x)} \\log \\frac {1}{2} + \\log \\frac {1}{2} \\log 4 $$ GAN 的优化目标是最小化生成模型分布 $p_g$ 和真实数据分布 $p_{data}$ 之间的差异，将目标函数中的对数概率表达为 KL 散度的形式。引入变形以关联 KL 散度： $$ \\begin{aligned} \\log \\frac {p_{data}(x)}{p_{data}(x) + p_{g}(x)} \\log \\frac {p_{data}(x)}{\\frac {p_{data}(x)+p_g(x)}{2}} \\log 2 \\\\ \\log \\frac {p_{g}(x)}{p_{data}(x) + p_{g}(x)} \\log \\frac {p_{g}(x)}{\\frac {p_{data}(x)+p_g(x)}{2}} \\log 2 \\end{aligned} $$ 所以， $$ \\begin{aligned} C(G) & \\log 4 + \\mathbb{E}_{x\\sim p_{data}} \\left[\\log \\frac {p_{data}(x)}{\\frac {p_{data}(x)+p_g(x)}{2}} \\right] + \\mathbb{E}_{x\\sim p_{g}} \\left[\\log \\frac {p_{g}(x)}{\\frac {p_{data}(x)+p_g(x)}{2}} \\right] \\\\ & \\log 4 + \\text{KL}\\left(p_{data}\\frac{p_{data}+p_g}{2}\\right) + \\text{KL}\\left(p_g\\frac{p_{data}+p_g}{2}\\right) \\\\ & \\log 4 + 2 \\cdot \\text{JSD}(p_{data} p_g) \\end{aligned} $$ >其中，KL 散度是衡量两个分布差异的常见方法，定义为：$\\text{KL}(pq) \\mathbb{E}_{x\\sim p} \\log \\frac {p(x)}{q(x)}$。Jensen Shannon 散度（JSD）是 KL 散度的一个对称版本，是衡量两个分布差异的对称性指标，定义为：$\\text{JSD}(pq) \\frac {1}{2} \\text{KL}(p \\frac {p + q}{2}) + \\frac {1}{2} \\text{KL}(q \\frac {p + q}{2})$。 由于 JSD 非负，当且仅当 $p q$ 时为零，因此： $$ C(G) \\geq \\log 4, 且等号成立当且仅当 p_g p_{data} $$ 证毕。 ### 解释算法收敛性 **命题 2.** 如果生成器 $G$ 和判别器 $D$ 都有足够的容量，并且算法的每一步都允许判别器在给定 $G$ 的情况下达到最优，并更新 $p_g$ 以改进标准 $\\mathbb{E}_{x\\sim p_{data}}[\\log D^*_G(x)] + \\mathbb{E}_{x\\sim p_{g}}[\\log (1 D^*_G(x))]$，则 $p_g$ 收敛到 $p_{data}$。 **证明：** 考虑 $V(G, D) U(p_g, D)$ 作为 $p_g$ 的函数。$U(p_g, D)$ 是凸函数，且全局最优解唯一。通过梯度下降更新 $p_g$，其参数更新方向始终朝向 JSD 减小的方向，从而保证收敛。 ## 代码实验 实验代码如下，详细代码位于[Github](https://github.com/Fingsinz/StyleTransfer/blob/main/src/01.ref_and_note/01.GAN.py)： <details> <summary>GAN MNIST 实验代码</summary> ```python ''' Created on 2025.02.01 @Author: Fingsinz (fingsinz@foxmail.com) @Reference: 1. https://arxiv.org/abs/1406.2661 ''' import time import os import torch import numpy as np import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader from torchvision import datasets, transforms import matplotlib.pyplot as plt # 配置参数 class Config(): data_folder: str './data' # 数据集路径, 此处用 MNIST 做测试 batch_size: int 128 # batch 大小 epochs: int 10 # 训练轮数 lr: float 0.0002 # 学习率 betas: tuple (0.5, 0.999) # Adam 的超参数 k_steps: int 5 # k 值 latent_dim: int 100 # 隐变量维度 device: str 'cuda' if torch.cuda.is_available() else 'cpu' # 生成器 class Generator(nn.Module): def __init__(self, latent_dim): super().__init__() self.model nn.Sequential( nn.Linear(latent_dim, 256), nn.LeakyReLU(0.2), nn.Linear(256, 512), nn.LeakyReLU(0.2), nn.Linear(512, 1024), nn.LeakyReLU(0.2), nn.Linear(1024, 28 * 28), nn.Tanh() ) def forward(self, x): return self.model(x).view( 1, 1, 28, 28) # 判别器 class Discriminator(nn.Module): def __init__(self): super().__init__() self.model nn.Sequential( nn.Flatten(), nn.Linear(28 * 28, 1024), nn.LeakyReLU(0.2, inplace True), nn.Linear(1024, 512), nn.LeakyReLU(0.2, inplace True), nn.Linear(512, 256), nn.LeakyReLU(0.2, inplace True), nn.Linear(256, 1), nn.Sigmoid() ) def forward(self, x): return self.model(x) # GAN 模型 class GAN(): def __init__(self, config): self.config config self.generator Generator(config.latent_dim).to(config.device) self.discriminator Discriminator().to(config.device) self.criterion nn.BCELoss() self.g_optimizer optim.Adam(self.generator.parameters(), lr config.lr, betas config.betas) self.d_optimizer optim.Adam(self.discriminator.parameters(), lr config.lr, betas config.betas) self.real_label 1 self.fake_label 0 def get_data(self): transform transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) train_dataset datasets.MNIST(root self.config.data_folder, train True, download True, transform transform) train_loader DataLoader(train_dataset, batch_size self.config.batch_size, shuffle True) return train_loader def train(self): train_loader self.get_data() epochs self.config.epochs g_loss 0 d_real_loss 0 d_fake_loss 0 for epoch in range(epochs): for i, (images, _) in enumerate(train_loader): batch_size images.size(0) images images.to(self.config.device) # 判别器训练 if (i + 1) % self.config.k_steps ! 0: self.d_optimizer.zero_grad() # 训练真实数据 labels torch.full((batch_size,), self.real_label, device self.config.device).float() output self.discriminator(images) loss_real self.criterion(output.view( 1), labels) loss_real.backward() # 训练假数据 z torch.randn(batch_size, self.config.latent_dim, device self.config.device) fake_images self.generator(z) labels.fill_(self.fake_label).float() output self.discriminator(fake_images.detach()) loss_fake self.criterion(output.view( 1), labels) loss_fake.backward() self.d_optimizer.step() d_real_loss loss_real.item() d_fake_loss loss_fake.item() # 判别器训练 # 生成器训练 else: self.g_optimizer.zero_grad() labels.fill_(self.real_label).float() output self.discriminator(fake_images) loss_g self.criterion(output.view( 1), labels) loss_g.backward() self.g_optimizer.step() g_loss loss_g.item() # 生成器训练 if i % 100 0: print(f\"[{time.strftime('%Y %m %d %H:%M:%S', time.localtime())}] \" + f\"Epoch [{epoch}/{epochs}], Step [{i}/{len(train_loader)}], \" f\"D Loss: {d_real_loss:.4f} + {d_fake_loss:.4f}, G Loss: {g_loss:.4f}\") self.save_generated_images(epoch + 1) def save_generated_images(self, epoch): \"\"\" 保存训练效果图片 参数: epoch (int): 当前轮数 \"\"\" z torch.randn(64, self.config.latent_dim, device self.config.device) fake_images self.generator(z) fake_images fake_images.cpu().detach().numpy() fake_images np.transpose(fake_images, (0, 2, 3, 1)) fig, axes plt.subplots(8, 8, figsize (8, 8)) for i in range(8): for j in range(8): axes[i, j].imshow(fake_images[i * 8 + j, :, :, 0], cmap 'gray') axes[i, j].axis('off') if not os.path.exists('gan_generated_images'): os.makedirs('gan_generated_images') plt.savefig(f'./gan_generated_images/epoch_{epoch}.png') plt.close() if __name__ '__main__': config Config() gan GAN(config) gan.train() ``` </details> GAN 对于 MNIST 数据集效果可如下所示： <table> <th>第一轮生成的图片</th> <th>第十轮生成的图片</th> <tr> <td><img src \"../static/images/GAN/epoch_1.png\"></td> <td><img src \"../static/images/GAN/epoch_10.png\"></td> </tr> </table> ## 论文优缺点及未来工作 优点（DeepSeek总结）： 生成样本质量高：GAN生成的图像、音频等数据具有极高的逼真度，尤其在图像生成任务中表现出色（如人脸、艺术作品生成）。生成器通过对抗训练不断优化，以欺骗判别器，最终生成的样本细节丰富、接近真实数据分布。 无需显式建模数据分布：GAN通过对抗过程直接学习数据分布，无需预先定义概率密度函数（如VAE需要假设潜在变量的分布），适用于复杂高维数据（如自然图像）。 生成多样性：在理想情况下，GAN能够覆盖真实数据的所有模式，生成多样化样本。相比之下，某些模型（如朴素自回归模型）可能因逐像素生成导致模式单一化。 无监督学习能力：GAN仅需未标注数据即可训练，适合缺乏标签的场景（如艺术创作、数据增强）。 灵活的应用扩展：GAN框架可轻松扩展为条件生成（cGAN）、图像翻译（CycleGAN）、超分辨率（SRGAN）等任务，适应多种生成需求。 缺点（DeepSeek总结）： 训练不稳定 模式坍缩（Mode Collapse）：生成器可能仅生成少数几种样本，忽略数据多样性。 梯度问题：若判别器过强，生成器梯度消失；若生成器过强，判别器无法提供有效反馈。 评估困难 缺乏显式似然函数，难以直接计算生成样本的概率。 常用指标（如Inception Score、FID）依赖预训练模型，可能无法全面反映生成质量。 超参数敏感：学习率、网络结构、正则化方法等对训练结果影响显著，需反复调参。 理论分析复杂 收敛性难以保证，实际训练可能陷入局部最优。 均衡状态（纳什均衡）在有限模型容量下难以达到。 生成不可控性 生成过程缺乏显式约束，可能产生不合理样本（如人脸扭曲）。 对离散数据（如文本）生成效果较差，因梯度无法通过离散变量传递。 计算资源消耗大：训练高质量GAN需要大量数据和计算资源（如GPU），耗时较长。 未来工作： 更好地理解这种框架，例如在高维空间中的表现； 探索其他算法和数据结构，例如使用卷积神经网络作为掩码器； 探索其他训练方法，例如使用梯度反向传递训练模型； 探索其他损失函数，例如使用KL损失函数； 探索其他训练方法，例如使用GAN训练模型。"},"/StyleTransfer/ref_and_notes/gatys.html":{"title":"Gatys 等人提出的风格迁移方法","content":" title: Gatys 等人提出的风格迁移方法 keywords: Gatys desc: Gatys 等人提出的风格迁移方法 date: 2025 04 09 id: gatys [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) *GATYS L, ECKER A, BETHGE M. A Neural Algorithm of Artistic Style[J/OL]. Journal of Vision, 2016: 326. http://dx.doi.org/10.1167/16.12.326. DOI:10.1167/16.12.326.* > In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery. **摘要**：在美术中，尤其是绘画，人类已经掌握了通过构成图像内容和风格之间复杂的相互作用来创造独特视觉体验的技能。到目前为止，这一过程的算法基础尚不清楚，也没有具有类似能力的人工系统。然而，在视觉感知的其他关键领域，如物体和人脸识别，最近一类被称为深度神经网络的生物启发视觉模型证明了接近人类的表现。在这里，我们介绍了一个基于深度神经网络的人工系统，它可以创建高感知质量的艺术图像。该系统利用神经表征对任意图像的内容和风格进行分离和重组，为艺术图像的创作提供了神经算法。此外，鉴于性能优化的人工神经网络和生物视觉之间惊人的相似性，我们的工作为理解人类如何创造和感知艺术图像的算法提供了一条道路。 ## 主要内容 ### 内容与风格的分离 利用卷积神经网络（CNN）不同层次的特征表示： 内容特征：由 CNN 高层（如 `conv4_2`）捕获，保留图像的高层语义信息（如物体及其布局），但忽略细节像素。 风格特征：通过计算多层特征图的 Gram 矩阵（特征相关性）来捕捉纹理、颜色和局部结构，形成多尺度的风格表示。 ### 图像生成方法 损失函数：联合优化内容损失（$\\mathcal{L}_{content}$）和风格损失（$\\mathcal{L}_{style}$）： $$ \\mathcal{L}_{total} \\alpha \\mathcal{L}_{content} + \\beta \\mathcal{L}_{style} $$ 内容损失：基于目标图像与生成图像在指定层的特征差异（均方误差）。 风格损失：基于 Gram 矩阵的差异，通过多层（如`conv1_1`至`conv5_1`）加权求和。 优化过程：从白噪声图像出发，通过梯度下降逐步调整，使生成图像同时匹配目标内容和风格。 ### 网络架构与改进 1. 使用 VGG 19 网络，移除全连接层，仅保留卷积和池化层。 2. 将最大池化替换为平均池化，以改善梯度流动和生成效果。 ## 关键创新点 1. Gram 矩阵表征风格：Gram 矩阵通过计算不同特征图之间的相关性，有效捕捉纹理的统计特性（如颜色分布、笔触方向），从而将风格抽象为多尺度的统计信息。 2. 分层控制风格与内容： 高层内容层（如 `conv4_2`）保留全局结构，适合内容重建。 多层级风格层（低层到高层）分别捕捉不同尺度的局部纹理（低层）和整体色彩协调（高层）。 通过调整使用的层数（如仅用低层生成局部纹理）和损失权重（$\\alpha/\\beta$），可灵活控制生成效果。 ## 实验结果 成功将名画风格（如梵高《星空》、蒙克《呐喊》）应用到同一张照片（图宾根内卡河畔），生成图像既保留原图内容，又复现艺术风格。 参数影响分析： 层数选择：使用更高层风格特征（如包含 `conv5_1`）会生成更平滑、连贯的视觉效果。 权重调整：增大 $\\alpha/\\beta$（侧重内容）保留更多原图结构；减小 $\\alpha/\\beta$（侧重风格）则强化纹理，弱化内容。 ## 实现 TensorFlow Github：https://github.com/lengstrom/fast style transfer"},"/StyleTransfer/ref_and_notes/adain.html":{"title":"自适应实例归一化 AdaIn","content":" title: 自适应实例归一化 AdaIn keywords: AdaIn desc: 自适应实例归一化 AdaIn date: 2025 04 02 id: adain [Arbitrary Style Transfer in Real time with Adaptive Instance Normalization](https://arxiv.org/abs/1703.06868) *HUANG X, BELONGIE S. Arbitrary Style Transfer in Real time with Adaptive Instance Normalization[C/OL]//2017 IEEE International Conference on Computer Vision (ICCV), Venice. 2017. http://dx.doi.org/10.1109/iccv.2017.167. DOI:10.1109/iccv.2017.167.* > Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre defined set of styles. In addition, our approach allows flexible user controls such as content style trade off, style interpolation, color & spatial controls, all using a single feed forward neural network. **摘要**：Gatys 等人最近介绍了一种神经算法，该算法将内容图像呈现为另一图像的风格，实现了所谓的风格迁移。然而，他们的框架需要一个缓慢的迭代优化过程，这限制了其实际应用。目前已经提出了前馈神经网络的快速近似来加速神经风格的转换。不幸的是，速度的提高是有代价的：网络通常与一组固定的风格相关联，无法适应任意的新风格。在本文中，我们提出了一种简单而有效的方法，首次实现了任意风格的实时转换。我们方法的核心是一个新颖的自适应实例归一化（AdaIN）层，它将内容特征的均值和方差与样式特征的均值和方差对齐。我们的方法与最快的现有方法速度相当，且不受预定义样式集的限制。此外，我们的方法允许灵活的用户控制，如内容风格权衡，风格插值，颜色和空间控制，所有这些都使用单个前馈神经网络。 ## 自适应实例归一化（AdaIN） 提出了一种新颖的归一化层，通过对**齐内容特征和风格特征的均值和方差（统计量）**，直接在特征空间实现风格迁移。AdaIN 的公式为： $$ \\text{AdaIN}(x, y) \\sigma(y) \\left( \\frac{x \\mu(x)}{\\sigma(x)} \\right) + \\mu(y) $$ 其中，$x$ 为内容特征，$y$ 为风格特征。AdaIN 无需可学习参数，仅通过风格特征的统计量调整内容特征，实现高效风格对齐。 结合预训练的 VGG 编码器和轻量级解码器，构建了一个端到端的前馈网络。该网络支持对任意未见过的风格进行实时处理（如 512 × 512 图像达 15 FPS），无需针对新风格重新训练。 ## 方法框架 ### 编码器 解码器架构 编码器：固定使用 VGG 19 的前几层（至 relu4_1），提取内容和风格图像的高层特征。 AdaIN层：将内容特征的均值和方差对齐到风格特征，生成目标特征。 解码器：随机初始化，通过反卷积将AdaIN输出的特征逆映射到图像空间。解码器未使用归一化层以避免风格固化。 ### 损失函数： 使用预训练 VGG 计算内容损失和风格损失： 内容损失：目标特征（AdaIN 输出）与生成图像特征的 L2 距离。 风格损失：生成图像与风格图像在各 VGG 层上的均值和方差差异的 L2 距离。 ## 关键创新点 从特征统计量视角解释 IN 的作用： 作者通过实验验证，实例归一化（IN）的有效性源于其对图像风格的归一化，而非仅对比度调整。IN通过消除内容图像的原始风格信息，使网络更易学习目标风格。 对比现有方法： 优化方法（如 Gatys）：灵活但速度慢（分钟级）。 单风格前馈网络（如 Ulyanov）：速度快（毫秒级）但风格受限。 风格交换（如 Chen）：支持任意风格但计算量大（95% 时间用于风格交换）。 本文方法：结合前馈速度（接近单风格方法）与任意风格灵活性，且无计算瓶颈。 ## 实现 Torch Github：https://github.com/xunhuang1995/AdaIN style PyTorch Github：https://github.com/naoto0804/pytorch AdaIN"},"/StyleTransfer/ref_and_notes/cgan.html":{"title":"cGAN：条件 GAN","content":" title: cGAN：条件 GAN keywords: cGAN desc: cGAN 简介 date: 2025 03 06 id: cGAN [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) *Mirza M , Osindero S .Conditional Generative Adversarial Nets[J].Computer Science, 2014:2672 2680.DOI:10.48550/arXiv.1411.1784.* [Image to Image Translation with Conditional Adversarial Networks](https://ieeexplore.ieee.org/document/8100115) *Isola P , Zhu J Y , Zhou T ,et al.Image to Image Translation with Conditional Adversarial Networks[C]//IEEE Conference on Computer Vision & Pattern Recognition.IEEE, 2016.DOI:10.1109/CVPR.2017.632.* ## cGAN 条件生成对抗网络（Conditional Generative Adversarial Networks, cGAN）是生成对抗网络（GAN）的一种扩展形式，通过引入 **条件信息**（如标签、文本、图像等），使生成器和判别器能够根据特定条件生成或判别数据。 核心思想是通过条件约束，控制生成内容的属性和结构，从而解决普通 GAN 生成结果不可控的问题。 ### cGAN 的核心原理 条件信息的引入： 生成器（Generator）：输入不仅包含随机噪声 $z$，还包括条件信息 $c$（如类别标签、另一张图像等）。生成器需根据 $c$ 生成对应的数据 $G(zc)$。 判别器（Discriminator）：输入包含真实数据 $x$ 或生成数据 $G(zc)$，同时结合条件信息 $c$。判别器的任务是判断数据是否真实且与条件匹配，即 $D(xc)$ 或 $D(G(zc)c)$。 cGAN 的损失函数在普通 GAN 的基础上加入了条件约束： $$ \\mathcal{L}_{cGAN}(G,D) \\mathbb{E}_{x,c}[\\log D(xc)] + \\mathbb{E}_{z,c}[\\log(1 D(G(zc)c)]] $$ 生成器 $G$ 的目标：生成与条件 $c$ 匹配的逼真数据，使 $D(G(zc)c)$ 趋近于1。 判别器 $D$ 的目标：区分真实数据 $xc$ 和生成数据 $G(zc)c$。 ### cGAN 对比普通 GAN 特性 普通GAN 条件GAN（cGAN） : :: :: : 输入 随机噪声 $z$ 随机噪声 $z$ + 条件信息 $c$ 生成控制 完全随机 通过条件 $c$ 控制生成内容 应用场景 无约束生成（如随机图像生成） 需特定条件生成（如根据文本生成图像） 典型任务 生成随机人脸、艺术品 图像到图像转换（pix2pix）、文本到图像生成、可控生成（如风格迁移）、图像修复、图像翻译（如黑白→彩色） ### 代码示例 ```python # 生成器（U Net结构为例） class Generator(nn.Module): def __init__(self): super().__init__() # 输入：噪声z + 条件图像c self.encoder Encoder() # 下采样层 self.decoder Decoder() # 上采样层（含跳跃连接） def forward(self, z, c): x torch.cat([z, c], dim 1) # 拼接噪声和条件 return self.decoder(self.encoder(x)) # 判别器（PatchGAN结构为例） class Discriminator(nn.Module): def __init__(self): super().__init__() # 输入：真实/生成图像 + 条件图像c self.conv_blocks nn.Sequential( nn.Conv2d(3 + 3, 64, kernel_size 4, stride 2), # 假设条件c是3通道图像 nn.LeakyReLU(0.2), # 更多卷积层... ) def forward(self, x, c): x torch.cat([x, c], dim 1) # 拼接图像和条件 return self.conv_blocks(x) ``` ## Image to Image Translation with cGAN 本文提出使用条件 GANs 作为通用解决方案，通过对抗训练自动学习任务相关的损失函数，避免人工设计损失函数的复杂性。 > *条件生成对抗网络（Conditional Generative Adversarial Networks, cGANs）最初由 Mehdi Mirza 和 Simon Osindero 在 2014 年的论文 《Conditional Generative Adversarial Nets》 中提出。这篇论文首次将条件信息（如类别标签或辅助数据）引入 GAN 框架，使生成器和判别器能够基于特定条件进行训练和生成。* 条件 GANs 的优势： 条件输入：生成器和判别器均以输入图像为条件，确保输出与输入的结构对齐（*如下图输入边缘图生成对应照片案例中，生成器和判别器都观察输入的边缘*）。 ![](../static/images/cGAN/fig1.png) 结合L1损失：在对抗损失基础上引入 L1 损失，保留低频信息（如整体布局），而对抗损失负责高频细节（如纹理和锐度），解决传统 L2 损失导致的模糊问题。 ### 方法细节 #### 目标函数 cGAN 的目标可以表示为： $$ \\mathcal{L}_{cGAN}(G,D) \\mathbb{E}_{x,c}[\\log D(xc)] + \\mathbb{E}_{z,c}[\\log(1 D(G(zc)c)]] $$ 目标函数上，总损失函数为对抗损失与 L1 损失的加权和： $$ G^* \\arg\\min_G \\max_D \\mathcal{L}_{cGAN}(G, D) + \\lambda \\mathcal{L}_{L1}(G) $$ 对抗损失：迫使生成器输出逼真的图像，判别器区分生成图像与真实图像。 L1 损失：约束生成图像与真实图像在像素级的一致性，减少模糊（所以不使用 L2 损失）。 随机性的引入：生成器的输入包含随机噪声（通过 Dropout 实现），但实验表明生成结果仍具有较低随机性。这表明当前方法在建模条件分布的多样性方面仍有改进空间。 #### 网络架构 在网络架构上： 生成器：带跳跃连接。 判别器（马尔可夫随机场）：[PatchGAN](./patchgan.html)，尝试对图像中的每个 N × N 块进行真假分类。在图像上卷积运行这个鉴别器，平均所有响应来提供 D 的最终输出。 #### 训练与推演过程 训练中： 遵循 GAN 中的优化算法，交替 $D$ 和 $G$ 的 step 训练。 在优化 $D$ 时将目标函数除以 2，减慢 $D$ 相对于 $G$ 学习的速率。 使用小批量 SGD 并应用 Adam 求解器，学习率为 0.0002，动量参数 $\\beta_1 0.5$，$\\beta_2 0.999$。 推演时： 与训练阶段相同的方式运行生成器。 ### 实验与验证 论文的第四部分通过广泛的实验验证了条件生成对抗网络（cGAN）在多种图像到图像转换任务中的有效性和通用性. #### 实验任务与数据集 作者在以下任务中测试了框架的通用性，涵盖图形和视觉任务： 语义标签 ↔ 照片（Cityscapes数据集）：将语义分割标签转换为真实街景照片。 建筑标签 → 照片（CMP Facades数据集）：将建筑立面线框图转换为真实建筑照片。 地图 ↔ 航拍图（Google Maps数据）：实现卫星地图与航拍图的双向转换。 黑白 → 彩色（ImageNet数据）：为灰度图像自动着色。 边缘 → 照片（UT Zappos50K、Amazon Handbag数据）：从边缘图生成鞋类、手提包等实物图像。 草图 → 照片（人类手绘草图）：扩展边缘到照片的模型至非结构化输入。 白天 → 夜晚（Webcam数据）：转换场景光照条件。 热成像 → 彩色照片（多光谱行人检测数据集）：融合热成像与可见光信息。 图像修复（Paris StreetView数据集）：补全图像中缺失的像素区域。 每个任务均使用相同架构（U Net 生成器 + PatchGAN 判别器）和损失函数（L1 + cGAN），仅更换训练数据。[跳转链接](https://phillipi.github.io/pix2pix/) #### 数据需求与训练效率 小数据集表现： 建筑标签任务仅需400张图像，训练时间不到2小时（单块Titan X GPU）。 昼夜转换任务使用91个摄像头的图像，训练17个周期即收敛。 推理速度： 所有模型在GPU上运行时间均小于1秒，支持实时应用。 #### 评估方法 (1) AMT 感知实验 设计：通过 Amazon Mechanical Turk 平台进行“真实 vs. 生成”的二选一测试，参与者需在 1 秒内观察图像后判断真伪。 结果： 地图→航拍图任务中，18.9%的生成图像被误认为真实（显著优于L1基线）。 航拍图→地图任务中，生成图像仅6.1%被误判（与L1基线无显著差异），因地图的几何结构更易暴露细节错误。 (2) FCN score 设计：使用预训练的FCN 8s模型（在Cityscapes上训练）对生成图像进行语义分割，计算分割精度（像素准确率、类别准确率、IoU）。 意义：衡量生成图像是否保留了输入标签的语义结构。 关键结果： L1 + cGAN 组合在Cityscapes任务中取得最高分数（像素准确率66%，接近真实数据的80%）。 单独使用 L1 损失会导致模糊，单独使用cGAN则可能生成结构错误但逼真的图像。 #### 目标函数分析 通过消融实验验证各损失组件的贡献： L1 损失：强制像素级匹配，减少模糊但导致色彩单调。 cGAN 损失：提升图像逼真性和高频细节（如边缘锐利、色彩丰富），但可能引入结构错误。 L1 + cGAN：结合二者优势，在逼真性和结构准确性间取得平衡。 #### 生成器架构分析 ![](../static/images/cGAN/fig2.png) U Net vs. 编码器 解码器： U Net 通过跳跃连接保留低级特征（如边缘位置），在图像着色等任务中显著优于普通编码器 解码器。 即使仅用 L1 损失训练，U Net 仍优于编码器 解码器，证明跳跃连接对信息传递的关键作用。 #### PatchGAN 尺寸分析 ![](../static/images/cGAN/fig3.png) *Patch 大小的影响。对于不同的损失函数，输出中的不确定性表现不同。在 L1 下，不确定区域变得模糊和去饱和。1x1 PixelGAN 鼓励更大的色彩多样性，但对空间统计没有影响。16x16 PatchGAN 创建了局部清晰的结果，但也导致了超出其可观察范围的平铺伪影。70×70 PatchGAN强制输出在空间和光谱（色彩）维度上都是清晰的，即使不正确。完整的 286×286 ImageGAN 生成的结果在视觉上与 70×70 PatchGAN 相似，但根据FCN评分指标，质量略低。请参阅 https://phillipi.github.io/pix2pix/ 了解更多示例。* 测试不同感受野的判别器： 1×1 PixelGAN：仅提升色彩多样性，对空间结构无影响。 16×16 PatchGAN：生成局部锐利图像，但出现拼贴伪影。 70×70 PatchGAN：最佳平衡，生成全局一致且细节清晰的图像。 286×286 ImageGAN（全图判别器）：参数量大、训练困难，且 FCN score 下降。 #### 全卷积扩展性测试 PatchGAN 固定大小的 Patch 可以应用于任意大的图像。如在 256×256 分辨率训练生成器，直接应用于 512×512 图像。结果是生成高分辨率图像时仍保持质量，证明框架的扩展性。 #### 语义分割任务 实验设计：将 cGAN 应用于照片→语义标签的逆任务。 结果： 仅用 cGAN（无L1）可生成粗略标签，但准确率低于 L1 回归。 作者认为，结构化输出任务（如分割）因目标明确，更适合传统回归损失。 #### 失败案例分析 ![](../static/images/cGAN/fig4.png) *每对图像中，左侧为输入，右侧为输出。* 常见问题： 输入稀疏或异常时，生成器产生伪影（如缺失边缘的区域）。 对非常规输入（如抽象草图）的泛化能力有限。 ### pix2pix 代码示例 [Github](https://github.com/junyanz/pytorch CycleGAN and pix2pix/blob/master/models/pix2pix_model.py) ```python class Pix2PixModel(BaseModel): # ... 省略 ... def __init__(self, opt): BaseModel.__init__(self, opt) # 指定要打印的训练损失。训练/测试脚本将调用<BaseModel.get_current_losses> self.loss_names ['G_GAN', 'G_L1', 'D_real', 'D_fake'] # 指定要保存/显示的图像。训练/测试脚本将调用<BaseModel.get_current_visuals> self.visual_names ['real_A', 'fake_B', 'real_B'] # 指定要保存到磁盘的模型。训练/测试脚本将调用<BaseModel.save_networks>和<BaseModel.load_networks> if self.isTrain: self.model_names ['G', 'D'] else: # 在测试期间，只加载G self.model_names ['G'] # 定义网络（生成器和鉴别器） self.netG networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids) if self.isTrain: # 定义一个鉴别器；条件gan需要同时获取输入和输出图像；因此，D 的 channels input_nc + output_nc self.netD networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids) if self.isTrain: # 定义损失函数 self.criterionGAN networks.GANLoss(opt.gan_mode).to(self.device) self.criterionL1 torch.nn.L1Loss() # 初始化优化器；优化器将由函数自动创建<BaseModel.setup>。 self.optimizer_G torch.optim.Adam(self.netG.parameters(), lr opt.lr, betas (opt.beta1, 0.999)) self.optimizer_D torch.optim.Adam(self.netD.parameters(), lr opt.lr, betas (opt.beta1, 0.999)) self.optimizers.append(self.optimizer_G) self.optimizers.append(self.optimizer_D) # ... 省略 ... def forward(self): self.fake_B self.netG(self.real_A) # G(A) ``` #### 判别器的反向传播（`backward_D`） 1. 生成假样本输入判别器 生成器生成假图像 `fake_B`，将其与输入图像 `real_A` 拼接为 `fake_AB`。 通过 `fake_AB.detach()` 切断梯度回传，防止生成器参数在判别器训练时被更新。 判别器对假样本的预测结果 `pred_fake` 与标签 `False` 计算损失 `loss_D_fake`。 2. 处理真实样本 将真实图像对 `real_A` 和 `real_B` 拼接为 `real_AB`。 判别器对真实样本的预测结果 `pred_real` 与标签 `True` 计算损失 `loss_D_real`。 3. 计算总损失并反向传播 总损失为真假样本损失的平均值：$\\text{loss}_{\\text{D}} (\\text{loss}_{\\text{D_fake}} + \\text{loss}_{\\text{D_real}}) / 2$ 执行 `loss_D.backward()` 计算梯度，通过 `optimizer_D.step()` 更新判别器参数。 ```python def backward_D(self): \"\"\"计算鉴别器的 GAN 损失\"\"\" # Fake；通过分离 fake_B 来停止对生成器的反向传播 # 使用条件 GAN，需要将输入和输出都提供给网络 fake_AB torch.cat((self.real_A, self.fake_B), 1) pred_fake self.netD(fake_AB.detach()) self.loss_D_fake self.criterionGAN(pred_fake, False) # Real real_AB torch.cat((self.real_A, self.real_B), 1) pred_real self.netD(real_AB) self.loss_D_real self.criterionGAN(pred_real, True) # 结合损失和计算梯度 self.loss_D (self.loss_D_fake + self.loss_D_real) * 0.5 self.loss_D.backward() ``` #### 生成器的反向传播（`backward_G`） 1. 对抗损失（GAN Loss） 将生成的假图像 `fake_B` 与输入图像 `real_A` 拼接为 `fake_AB`，输入判别器得到预测结果 `pred_fake`。 计算对抗损失 `loss_G_GAN`，目标是让判别器认为生成的图像为真（标签 `True`）。 2. L1 重建损失 计算生成图像 `fake_B` 与真实图像 `real_B` 的像素级 L1 损失 `loss_G_L1`，乘以权重系数 `lambda_L1`（通过 `opt.lambda_L1` 控制）。 3. 计算总损失并反向传播 总损失为对抗损失与L1损失之和：$\\text{loss}_{\\text{G}} \\text{loss}_{\\text{G_GAN}} + \\text{loss}_{\\text{G_L1}}$ 执行 `loss_G.backward()` 计算梯度，通过 `optimizer_G.step()` 更新生成器参数。 ```python def backward_G(self): \"\"\"计算生成器的 GAN 和 L1 损失\"\"\" # 1. G(A) 应该骗过判别器 fake_AB torch.cat((self.real_A, self.fake_B), 1) pred_fake self.netD(fake_AB) self.loss_G_GAN self.criterionGAN(pred_fake, True) # 2. G(A) B self.loss_G_L1 self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1 # 结合损失并计算梯度 self.loss_G self.loss_G_GAN + self.loss_G_L1 self.loss_G.backward() ``` #### 训练流程（`optimize_parameters`） 1. 前向传播生成假图像：`self.forward()` 调用生成器生成 `fake_B`。 2. 更新判别器： 解冻判别器参数（`set_requires_grad(self.netD, True)`）。 清零梯度（`optimizer_D.zero_grad()`）。 计算判别器损失并反向传播（`backward_D()`）。 更新参数（`optimizer_D.step()`）。 3. 更新生成器： 冻结判别器参数（`set_requires_grad(self.netD, False)`）。 清零梯度（`optimizer_G.zero_grad()`）。 计算生成器损失并反向传播（`backward_G()`）。 更新参数（`optimizer_G.step()`）。 ```python def optimize_parameters(self): self.forward() # 计算生成器生成的假图像: G(A) # 更新 D self.set_requires_grad(self.netD, True) # 启用 D 的反向传播 self.optimizer_D.zero_grad() self.backward_D() self.optimizer_D.step() # 更新 G self.set_requires_grad(self.netD, False) # D 在更新 G 时不需要梯度 self.optimizer_G.zero_grad() self.backward_G() self.optimizer_G.step() ``` ### 局限性与启示 随机性不足：生成结果偏向确定性，难以建模多模态输出（如同一输入对应多种合理输出）。 复杂任务表现：在高度结构化任务（如语义分割）中，cGANs 效果不及纯 L1 回归，表明对抗训练更适用于需细节生成的图形任务。 社区应用：开源代码（pix2pix）被广泛用于艺术创作（如草图转肖像、背景去除），验证了其易用性和扩展性。"},"/StyleTransfer/ref_and_notes/cyclegan.html":{"title":"CycleGAN：循环 GAN","content":" title: CycleGAN：循环 GAN keywords: CycleGAN desc: 循环 GAN date: 2025 03 11 id: CycleGAN [Unpaired Image to Image Translation using Cycle Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593) *Zhu J Y , Park T , Isola P ,et al.Unpaired Image to Image Translation using Cycle Consistent Adversarial Networks[J].IEEE, 2017.DOI:10.1109/ICCV.2017.244.* > Image to image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G:X→Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under constrained, we couple it with an inverse mapping F:Y→X and introduce a cycle consistency loss to push F(G(X))≈X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach. **摘要**：图像到图像的转换是一类视觉和图形的问题，其目标是使用对齐的图像对进行学习输入图像和输出图像之间的映射。但是对于许多任务而言，并不存在大量配对的训练数据。我们提出了一种学习在没有配对例子的情况下，将图像从源域 X 转换到目标域 Y 的方法。我们的目标是学习一个映射 G：X → T，使得使用对抗性损失无法区分是来自 G(x) 的图像分布还是 Y 的图像分布。因为这个映射是高度欠约束的，我们将这个映射与其逆映射 F：Y → X 耦合，并引入循环一致性损失，以推动 F(G(X)) ≈ X（反之亦然）。在不存在配对训练数据的几个任务上，如风格转移、对象变形、季节转移、照片增强等，给出了定性结果。与先前几种方法的定量比较表明了我们方法的优越性。 ## CycleGAN 模型 引入循环一致性损失，实现了无需配对训练数据的跨域图像转换。 双向映射：同时学习 $G:X\\rightarrow Y$ 和 $F:Y\\rightarrow X$，确保生成的图像在目标域中分布一致。 循环一致性约束：约束 $F(G(x))\\approx x$ 和 $G(F(y))\\approx y$，防止生成器陷入模式崩溃。 ## CycleGAN 具体框架 该框架的目标是通过给定训练集 $\\{x_i\\}^{N}_{i 1}$ 和 $\\{y_i\\}^{M}_{j 1}$，学习源域和目标域之间的映射。 其中，$x_i \\in X$，$y_i \\in Y$；$x \\sim p_{data}(x)$，$y \\sim p_{data}(y)$。 ![](../static/images/CycleGAN/fig1.png) 如上图所示，**模型包含两个映射：$G:X\\rightarrow Y$ 和 $F:Y\\rightarrow X$**。 另外，还加上两个对抗判别器 $D_X$ 和 $D_Y$： $D_X$ 用于区分图像 $\\{x\\}$ 和生成图像 $\\{F(y)\\}$； $D_Y$ 用于区分图像 $\\{y\\}$ 和生成图像 $\\{G(x)\\}$。 引入对抗性损失和循环一致性损失： **对抗性损失用于匹配生成图像的分布和目标域的分布；** **引入循环一致性损失，用于防止学习到的映射 $G$ 和 $F$ 相互矛盾。** ### 对抗损失 对于映射 $G:X\\rightarrow Y$ 和判别器 $D_Y$，定义对抗损失为： $$ \\mathcal{L}_{GAN}(G,D_Y,X,Y) \\mathbb{E}_{y \\sim p_{data}(y)} \\log D_Y(y) + \\mathbb{E}_{x \\sim p_{data}(x)} \\log(1 D_Y(G(x))) $$ $G$ 试图生成与目标域 $Y$ 相似的图像 $G(x)$，而判别器 $D_Y$ 试图区分生成图像 $G(x)$ 和真实图像 $y$。 **$G$ 的目标是最小化这个目标，而判别器 $D_Y$ 则试图最大化这个目标，即：$\\min_{G}\\max_{D_Y}\\mathcal{L}(G,D_Y,X,Y)$。** 类似地引入映射 $F:Y\\rightarrow X$ 和 $D_X$ 的对抗损失： $$ \\mathcal{L}_{GAN}(F,D_X,Y,X) \\mathbb{E}_{x \\sim p_{data}(x)} \\log D_X(x) + \\mathbb{E}_{y \\sim p_{data}(y)} \\log(1 D_X(F(y))) $$ **$F$ 的目标是最小化这个目标，而判别器 $D_X$ 则试图最大化这个目标，即：$\\min_{F}\\max_{D_X}\\mathcal{L}(F,D_X,Y,X)$。** ### 循环一致性损失 这两个映射转换应该是可以循环的，即 $x\\rightarrow G(x) \\rightarrow F(G(x)) \\approx x$ 和 $y\\rightarrow F(y) \\rightarrow G(F(y)) \\approx y$。 所以使用循环一致性损失以促进转换循环： $$ \\mathcal{L}_{cyc}(G,F) \\mathbb{E}_{x \\sim p_{data}(x)} \\left[\\vert\\vert F(G(x)) x \\vert\\vert _1 \\right] + \\mathbb{E}_{y \\sim p_{data}(y)} \\left[\\vert\\vert G(F(y)) y \\vert\\vert _1 \\right] $$ ### 完整的目标函数 总目标为： $$ \\mathcal{L}(G,F,D_X,D_Y) \\mathcal{L}_{GAN}(G,D_Y,X,Y) + \\mathcal{L}_{GAN}(F,D_X,Y,X) + \\lambda \\mathcal{L}_{cyc}(G,F) $$ $\\lambda$ 控制两个损失的权重。 存在解： $$ G^*, F^* \\arg\\min_{G,F}\\max_{D_X,D_Y} \\mathcal{L}(G,F,D_X,D_Y) $$ ## CycleGAN 实现细节 ### 生成器 基于 Johnson 等人提出的残差网络（ResNet）结构，适用于图像生成任务（如风格迁移、超分辨率）。 组件： 初始卷积层：`c7s1 k` 表示一个 7 × 7 的卷积层，步长为 1，输出通道数为 k，后接实例归一化（InstanceNorm）和 ReLU 激活。 下采样层：`dk` 表示 3 × 3 卷积层，步长为 2，通道数为 k，用于逐步降低分辨率。 残差块：`Rk` 包含两个 3 × 3 卷积层，每层通道数为 k，保留输入与输出的残差连接。 上采样层：`uk` 表示 3 × 3 分数步长卷积（反卷积），步长为 1/2，用于恢复分辨率。 末端卷积层：`c7s1 3` 生成 RGB 三通道输出。 结构配置： 低分辨率（128 × 128）：6 个残差块，结构为：`c7s1 64 → d128 → d256 → 6×R256 → u128 → u64 → c7s1 3`。 高分辨率（256 × 256 及以上）：9 个残差块，结构为：`c7s1 64 → d128 → d256 → 9×R256 → u128 → u64 → c7s1 3`。 关键设计： 实例归一化（Instance Normalization）：提升生成图像的视觉质量，尤其在风格迁移任务中抑制内容无关的变量。 反射填充（Reflection Padding）：减少边缘伪影，提升生成图像的平滑度。 ### 判别器 来自 Isola 等人的 70 × 70 PatchGAN。 层级结构： 初始层：C64 为 4 × 4 卷积，步长为 2，输出 64 通道，无实例归一化，使用 LeakyReLU（斜率 0.2）。 后续层：C128 → C256 → C512，每层为 4 × 4 卷积 + 实例归一化 + LeakyReLU，逐步增加通道数。 末端输出：通过卷积生成 1 通道的判别结果（局部区域的真假概率图）。 关键设计： 局部判别：关注图像局部区域（70 × 70 patch）的真实性，而非全局，降低计算量。 全卷积结构：支持任意尺寸输入，适用于不同分辨率的任务。 ### 训练策略 #### 损失函数改进 最小二乘 GAN（LSGAN）代替负对数似然，采用均方误差（MSE）目标函数，缓解梯度消失问题，生成图像质量更高，训练更稳定。 训练 $G$ 最小化 $\\mathbb{E}_{x \\sim p_{data}(x)}[(D(G(x)) 1)^2]$； 训练 $D$ 最小化 $\\mathbb{E}_{y \\sim p_{data}(y)}[(D(y) 1)^2] + \\mathbb{E}_{x \\sim p_{data}(x)}[D(G(x))^2]$。 原来的损失函数： $$ \\begin{cases} \\mathcal{L}_{GAN}(G,D_Y,X,Y) \\mathbb{E}_{y \\sim p_{data}(y)} \\log D_Y(y) + \\mathbb{E}_{x \\sim p_{data}(x)} \\log(1 D_Y(G(x)))\\\\ \\mathcal{L}_{GAN}(F,D_X,Y,X) \\mathbb{E}_{x \\sim p_{data}(x)} \\log D_X(x) + \\mathbb{E}_{y \\sim p_{data}(y)} \\log(1 D_X(F(y))) \\end{cases} $$ 用 LSGAN 代替后： $$ \\begin{cases} \\mathcal{L}_{GAN}(G,D_Y,X,Y) \\mathbb{E}_{y \\sim p_{data}(y)} [(D_Y(y) 1)^2] + \\mathbb{E}_{x \\sim p_{data}(x)} [D_Y(G(x))^2]\\\\ \\mathcal{L}_{GAN}(F,D_X,Y,X) \\mathbb{E}_{x \\sim p_{data}(x)} [(D_X(x) 1)^2] + \\mathbb{E}_{y \\sim p_{data}(y)} [D_X(F(y))^2] \\end{cases} $$ 对于 $G$ 和 $D_Y$，采用的是 Least Squares GAN（LSGAN）形式，其具体表达式为上面的 $\\mathcal{L}_{GAN}(G,D_Y,X,Y)$，包含两部分： 第一项 $\\mathbb{E}_{y\\sim p_{data}(y)}[(D_Y(y) 1)^2]$，用于训练判别器 $D_Y$ ，要求对真实图像 $y$ 的输出接近 1，即让 $D_Y$ 识别真实图像为真实。 第二项 $\\mathbb{E}_{x\\sim p_{data}(x)}[D_Y(G(x))^2]$，用于训练判别器 $D_Y$ ，要求对生成图像 $G(x)$ 的输出接近 0，即让 $D_Y$ 识别生成图像为假。 同时，在训练生成器 $G$ 时，只关心让生成图像 $G(x)$ 看起来足够真实，也就是使得 $D_Y(G(x))$ 输出接近 1。因此，生成器 $G$ 的目标就是最小化：$\\mathbb{E}_{x\\sim p_{data}(x)}[(D_Y(G(x)) 1)^2]$，这一目标正好出现在上式中的第一项，但生成器只会更新这一部分，而判别器 $D_Y$ 则同时考虑了真实图像和生成图像两部分的损失。 对于反方向（$F$ 与 $D_X$），也有相似的表达 $ L_{GAN}(F,D_X,Y,X)$。这里，生成器 $F$ 训练时的目标是最小化 $\\mathbb{E}_{y\\sim p_{data}(y)}[(D_X(F(y)) 1)^2]$。 将两个方向的对抗性损失合并，再加上后续的循环一致性损失，形成了总损失函数： $$ \\mathcal{L}(G,F,D_X,D_Y) \\mathcal{L}_{GAN}(G,D_Y,X,Y) + \\mathcal{L}_{GAN}(F,D_X,Y,X) + \\lambda \\mathcal{L}_{cyc}(G,F) $$ 在上式中： $L_{GAN}(G,D_Y,X,Y)$ 部分包含了生成器 $G$ 的训练目标 $\\mathbb{E}_{x\\sim p_{data}(x)}[(D_Y(G(x)) 1)^2]$（生成器更新时使用）以及判别器 $D_Y$ 的训练目标（结合真实图像的 $\\mathbb{E}_{y\\sim p_{data}(y)}[(D_Y(y) 1)^2]$ 和生成图像的 $\\mathbb{E}_{x\\sim p_{data}(x)}[D_Y(G(x))^2]$）。 $L_{GAN}(F,D_X,Y,X)$ 部分类似地，包含了生成器 $F$ 与判别器 $D_X$ 的对应目标。 #### 恒等映射损失（Identity Loss） 在特定任务中，强制生成器在输入目标域图像时保持原图不变： $$ \\mathcal{L}_{identity} \\mathbb{E}_y [\\vert\\vert G(y) y \\vert\\vert _1] + \\mathbb{E}_x [\\vert\\vert F(x) x \\vert\\vert _1] $$ 作用：防止生成器过度改变输入的颜色或光照（例如避免将白天的照片转为日落色调）。 #### 训练稳定性优化 图像缓冲区（Image Buffer）：存储最近生成的50张图像，用于更新判别器。避免判别器仅依赖当前生成器的最新输出，减少模式崩溃风险。 学习率调度：初始学习率设为 0.0002（Adam优化器），前 100 个 epoch 保持不变。后100个epoch线性衰减至0，逐步收敛模型。 权重初始化：从高斯分布 $N(0,0.02)$ 采样，避免初始值过大或过小。 批量大小：固定为 1（单张图像训练），节省显存并适应高分辨率任务。 任务特定调整： 艺术风格迁移：加入恒等映射损失（权重为 $0.5\\lambda$，$\\lambda 10$）。 季节转换/物体变换：使用 ImageNet 子集，图像尺寸统一为 256 × 256。 #### 训练集与训练参数 数据集配置： Cityscapes（语义标签↔照片）：2975 张训练图像，分辨率 128 × 128。 Google Maps（地图↔航拍图）：1096 张图像，分辨率 256 × 256，按地理纬度划分训练/测试集。 艺术风格迁移：从 Wikiart和 Flickr 收集数据，筛选后包含莫奈（1074 张）、梵高（401 张）等风格。 季节转换：从 Flickr 下载约 2000 张冬夏景观图，分辨率 256 × 256。 超参数设置： 循环一致性损失权重：$\\lambda 10$（通过消融实验验证其重要性）。 训练周期：通常为 200 个 epoch（前 100 固定学习率，后 100 线性衰减）。 硬件与框架：基于 PyTorch 和 Torch 实现，使用 NVIDIA GPU 加速。 ### 代码分析 [CycleGAN/models/cycle_gan_model.py](https://github.com/junyanz/pytorch CycleGAN and pix2pix/blob/master/models/cycle_gan_model.py) ```python class CycleGANModel(BaseModel): \"\"\" CycleGAN paper: https://arxiv.org/pdf/1703.10593.pdf \"\"\" def __init__(self, opt): \"\"\"初始化\"\"\" BaseModel.__init__(self, opt) self.loss_names ['D_A', 'G_A', 'cycle_A', 'idt_A', 'D_B', 'G_B', 'cycle_B', 'idt_B'] visual_names_A ['real_A', 'fake_B', 'rec_A'] visual_names_B ['real_B', 'fake_A', 'rec_B'] if self.isTrain and self.opt.lambda_identity > 0.0: visual_names_A.append('idt_B') visual_names_B.append('idt_A') self.visual_names visual_names_A + visual_names_B if self.isTrain: self.model_names ['G_A', 'G_B', 'D_A', 'D_B'] else: self.model_names ['G_A', 'G_B'] ``` > 做初始化、损失配置等等。 ```python self.netG_A networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids) self.netG_B networks.define_G(opt.output_nc, opt.input_nc, opt.ngf, opt.netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids) if self.isTrain: # define discriminators self.netD_A networks.define_D(opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids) self.netD_B networks.define_D(opt.input_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids) ``` > 定义生成器和判别器。 > 此处 G_A 为 G, G_B 为 F, D_A 为 D_Y, D_B 为 D_X。 ```python if self.isTrain: if opt.lambda_identity > 0.0: # 仅当输入和输出图像具有相同数量的通道时才有效 assert(opt.input_nc opt.output_nc) self.fake_A_pool ImagePool(opt.pool_size) self.fake_B_pool ImagePool(opt.pool_size) ``` > 创建图像缓冲区以存储以前生成的图像 ```python self.criterionGAN networks.GANLoss(opt.gan_mode).to(self.device) # define GAN loss. self.criterionCycle torch.nn.L1Loss() self.criterionIdt torch.nn.L1Loss() self.optimizer_G torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), lr opt.lr, betas (opt.beta1, 0.999)) self.optimizer_D torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr opt.lr, betas (opt.beta1, 0.999)) self.optimizers.append(self.optimizer_G) self.optimizers.append(self.optimizer_D) ``` > 定义损失函数和优化器（对抗损失：`MSELoss`；循环一致性损失：`L1Loss`；循环重建损失：`L1Loss`）。 > 初始化优化器；schedulers 将由函数 `BaseModel.setup` 自动创建。 ```python def forward(self): self.fake_B self.netG_A(self.real_A) # G_A(A) self.rec_A self.netG_B(self.fake_B) # G_B(G_A(A)) self.fake_A self.netG_B(self.real_B) # G_B(B) self.rec_B self.netG_A(self.fake_A) # G_A(G_B(B)) ``` > 前向传播。`netG_A` 生成假图像 B，`netG_B` 重建图像 A；`netG_B` 生成假图像 A，`netG_A` 重建图像 B。 > 生成假图像与循环重建。 ```python def backward_D_basic(self, netD, real, fake): pred_real netD(real) loss_D_real self.criterionGAN(pred_real, True) pred_fake netD(fake.detach()) loss_D_fake self.criterionGAN(pred_fake, False) loss_D (loss_D_real + loss_D_fake) * 0.5 loss_D.backward() return loss_D ``` > 判别器对真实图像进行预测，接着计算对真实图像损失；然后对假图像进行预测，计算对假图像的损失。 > 最后将两个损失相加，然后反向传播。 ```python def backward_D_A(self): \"\"\"Calculate GAN loss for discriminator D_A\"\"\" fake_B self.fake_B_pool.query(self.fake_B) self.loss_D_A self.backward_D_basic(self.netD_A, self.real_B, fake_B) def backward_D_B(self): \"\"\"Calculate GAN loss for discriminator D_B\"\"\" fake_A self.fake_A_pool.query(self.fake_A) self.loss_D_B self.backward_D_basic(self.netD_B, self.real_A, fake_A) ``` > 在缓冲池种随机选择一个假图像，然后计算判别器的损失。 ```python def backward_G(self): lambda_idt self.opt.lambda_identity lambda_A self.opt.lambda_A lambda_B self.opt.lambda_B if lambda_idt > 0: self.idt_A self.netG_A(self.real_B) self.loss_idt_A self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt self.idt_B self.netG_B(self.real_A) self.loss_idt_B self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt else: self.loss_idt_A 0 self.loss_idt_B 0 self.loss_G_A self.criterionGAN(self.netD_A(self.fake_B), True) self.loss_G_B self.criterionGAN(self.netD_B(self.fake_A), True) self.loss_cycle_A self.criterionCycle(self.rec_A, self.real_A) * lambda_A self.loss_cycle_B self.criterionCycle(self.rec_B, self.real_B) * lambda_B self.loss_G self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B self.loss_G.backward() ``` > 分别计算恒等损失、对抗损失和循环一致性损失。 ```python def optimize_parameters(self): self.forward() self.set_requires_grad([self.netD_A, self.netD_B], False) self.optimizer_G.zero_grad() self.backward_G() self.optimizer_G.step() self.set_requires_grad([self.netD_A, self.netD_B], True) self.optimizer_D.zero_grad() self.backward_D_A() self.backward_D_B() self.optimizer_D.step() ``` > 1. 前向计算生成图像。 > 2. 冻结判别器，计算梯度，更新参数。 > 3. 解冻生成器，计算梯度，更新参数。 ### 代码复现 复现代码：[Github/01.ref_and_note/05.CycleGAN.py](https://github.com/Fingsinz/StyleTransfer/blob/main/src/01.ref_and_note/05.CycleGAN.py) ## CycleGAN 实验结果 ### 与其他基线模型的性能对比 定量评估： AMT感知测试（地图↔航拍图任务）： CycleGAN 的欺骗率显著优于基线方法（如 CoGAN、SimGAN 等），在 256 × 256 分辨率下： 地图→航拍图：26.8% vs 基线的 0.6% 2.6% 航拍图→地图：23.2% vs 基线的 0.3% 2.9% FCN 分数（Cityscapes 标签→照片任务）： CycleGAN 的 FCN 分数（0.58）接近有监督方法pix2pix（0.85），远超其他无监督方法（如 BiGAN/ALI 的 0.41）。 语义分割指标（照片→标签任务）： CycleGAN 的类别平均交并比（Class IOU）为0.16，显著高于SimGAN（0.07）和CoGAN（0.08）。 定性分析： 生成图像在视觉质量上接近有监督方法 pix2pix（见图5、图6），尤其在风格迁移和物体变换任务中表现突出。 ### 消融实验（Ablation Study） 关键结论： 对抗损失（GAN Loss）：移除后生成图像质量严重下降（FCN 分数从 0.58 降至 0.22 ），表明对抗训练对生成真实性至关重要。 循环一致性损失（Cycle Loss）：移除后模型无法保持输入与输出的合理映射（如标签→照片任务中生成无意义图像）。 单向循环约束（仅前向或后向）：导致模式崩溃或训练不稳定（如马→斑马任务中生成单一结果）。 ### 多任务应用效果 风格迁移： 成功将照片转换为莫奈、梵高等艺术家的整体风格，而非单幅作品风格。 加入身份映射损失（Identity Loss）后，有效保留输入色彩（如避免白昼→黄昏的错误转换）。 物体变换： 马↔斑马、苹果↔橙子等任务中，生成图像在纹理和颜色上高度逼真。 季节转换： 冬季↔夏季景观转换中，能合理调整植被颜色和光照（如积雪→绿树）。 照片增强： 智能手机拍摄的花卉照片→DSLR浅景深效果，部分成功。 ### 局限性分析 几何变换能力弱：在需改变物体形状的任务（如狗↔猫）中，生成器仅调整颜色/纹理，无法改变结构（见图17）。 数据分布依赖性：训练数据缺乏多样性时易失败（如未包含“骑马”场景的斑马数据集）。 与有监督方法的差距：复杂任务（如语义分割）中，CycleGAN可能混淆类别（如树↔建筑），需弱监督辅助。 ### 与神经风格迁移（Gatys et al.）的对比 优势： CycleGAN可学习整体风格集合（如莫奈全部作品），而Gatys方法依赖单一样本。 生成结果更自然逼真（见图15 16），尤其在需要高真实性的任务（如照片增强）中表现更优。 劣势： Gatys方法在单样本风格迁移中更灵活，而CycleGAN需针对每个风格集合重新训练。 然而，CycleGAN在几何变换和复杂语义任务中仍有局限，未来需结合弱监督或更强大的生成架构进一步突破。"},"/StyleTransfer/ref_and_notes/unet.html":{"title":"U-Net 卷积网络","content":" title: U Net 卷积网络 keywords: UNet desc: U Net文献及笔记 date: 2025 02 26 id: ref_UNet [U Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) *Ronneberger O , Fischer P , Brox T .U Net: Convolutional Networks for Biomedical Image Segmentation[C]//International Conference on Medical Image Computing and Computer Assisted Intervention.Springer International Publishing, 2015.DOI:10.1007/978 3 319 24574 4_28.* > There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end to end from very few images and outperforms the prior best method (a sliding window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at [this](http://lmb.informatik.uni freiburg.de/people/ronneber/u net) . **摘要**：人们普遍认为，深度网络的成功训练需要数千个带注释的训练样本。在本文中，我们提出了一种网络和训练策略，该策略依赖于大量据增强来更有效地使用可用的带注释样本。该架构由捕获上下文的收缩路径和实现精确定位的对称扩展路径组成。我们证明，这种网络可以从很少的图像进行端到端的训练，并且在电子显微镜堆栈中神经元结构分割的 ISBI 挑战中优于先前的最佳方法（滑动窗口卷积网络）。使用在透射光显微镜图像（相位对比和DIC）上训练的相同网络，我们在这些类别中以很大的优势赢得了2015年ISBI细胞跟踪挑战赛。此外，网络速度很快。在最近的GPU上，分割512x512图像所需的时间不到一秒钟。所有的实施（基于Caffe）和训练后的网络可在以下网址获得 [http://lmb.informatik.uni freiburg.de/people/ronneber/u net](http://lmb.informatik.uni freiburg.de/people/ronneber/u net). ## U Net 模型 U 型网络结构由一个收缩路径和一个对称的扩展路径组成，能够有效地捕捉上下文和实现精确的局部定位： 1. **收缩路径（下采样）**：通过重复 3 × 3 卷积、ReLU 激活和 2 × 2 最大池化捕获信息，提取高层语义特征。 2. **扩展路径（上采样）**：通过转置卷积（上卷积）恢复空间分辨率，同时跳跃连接将收缩路径的浅层特征与扩展路径的深层特征拼接，结合细节与语义信息，提升定位深度。 *通过卷积操作，无全连接层，支持任意尺寸输入。* **数据增强策略**：针对生物医学数据标注稀缺的问题，采用弹性变形（随机位移向量的平滑插值）、旋转、平移等增强方法，显著提升模型对形变和噪声的鲁棒性。 **加权损失函数**：通过预计算权重图，平衡类别频率并强化相邻细胞边界区域的学习。使用形态学操作计算边界距离，权重公式： $$ w(\\text{x}) w_c(\\text{x}) + w_0 \\cdot \\exp\\left( \\frac {(d_1(x) + d_2(x))^2} {2 \\sigma^2}\\right) $$ 能够有效地捕捉上下文和实现精确的局部定位 ## 网络理论 在许多视觉任务中，特别是在生物医学图像处理中，期望的输出应该包括定位，即为每个像素分配一个类标签。U Net 网络可以进行图像中的定位。 网络体系结构如下图： ![](../static/images/UNet/fig1.jpg) *包括一条收缩路径（左侧）和一条扩张路径（右侧）。每个蓝色框对应一个多通道特征图，通道数量显示在方框顶部，x y 尺寸位于方框的左下。白框表示复制的要素图。箭头表示不同的操作。* 收缩路径与普通的卷积网络相同，通过一系列的卷积和池化操作。两个 3 × 3 卷积（未填充卷积）的重复应用组成，每个卷积后面都有一个 ReLU 单元和一个 2 × 2 最大池化操作，步幅为 2，用于下采样。 扩展路径中的每一步都包括特征映射的上采样，然后进行 2 × 2 卷积（“上卷积”），将特征通道的数量减半，与收缩路径中相应裁剪的特征映射进行连接，以及两个 3 × 3 卷积，每个卷积后面都有一个 ReLU。 在最后一层，使用 1 × 1 卷积将每个 64 个组件的特征向量映射到所需的类数。这个网络总共有 23 个卷积层。 ## 网络训练 ### 输入与训练配置 1. 无填充卷积（Valid Convolution）：由于未填充的卷积，输出图像小于恒定边界宽度的输入，输出尺寸缩小，需对特征图进行裁剪以实现跳跃连接的尺寸匹配。 2. 单张图像作为批次（batch size 1）：为了最小化开销并最大限度地利用 GPU 内存，倾向于大输入块，而不是大批量。 ### 损失函数设计 **损失函数由最终特征图上的像素级的 soft max 与交叉熵损失函数相结合来计算。** 网络的最终输出通过逐像素soft max生成类别概率图，soft max公式为： $$ p_k(\\text{x}) \\frac {\\exp(a_k(\\text{x}))} {\\sum_{k' 1}^K \\exp(a_{k'}(\\text{x}))} $$ 其中，$a_k(\\text{x})$ 是像素 $\\text{x}$ 在第 $k$ 个通道的激活值，$K$ 为类别数。 $p_k(\\text{x})$ 是近似的最大值函数。例如，对于 $a_k(\\text{x})$ 最大的 $k$，$p_k(\\text{x})\\approx 1$；对于所有其他 $k$，$p_k(\\text{x})\\approx 0$。 交叉熵损失函数计算预测概率与真实标签的差异： $$ E \\sum_{\\text{x} \\in \\Omega} w(\\text{x}) \\log (p_{\\ell (\\text{x})}(\\text{x})) $$ $\\ell(\\text{x})$ 是像素的真实类别标签，$w(\\text{x})$ 是预计算的权重图。 权重图的作用： 类别平衡：某些类别（如细胞边界）像素稀少，通过权重图 $w(\\text{x})$ 增加其损失权重，避免模型忽略小目标。 强化边界学习：对相邻细胞的接触边界赋予更高权重，迫使网络精确区分相邻对象。权重计算公式为（其中，$d_1$ 和 $d_2$ 分别是像素到最近细胞边界和第二近细胞边界的距离，$w_0 10$ 和 $\\sigma \\approx 5$ 控制权重强度和衰减范围）： $$ w(\\text{x}) w_c(\\text{x}) + w_0 \\cdot \\exp\\left( \\frac {(d_1(x) + d_2(x))^2} {2 \\sigma^2}\\right) $$ ### 权重初始化策略 高斯分布初始化，其中标准差为 $\\sqrt{2 / N}$，$N$ 是每个神经元的输入节点数。 对于 3 × 3 卷积核且前一层的特征通道数为 64，则 $N 3 \\times 3 \\times 64 576$ ，标准差为 $\\sqrt{2/576}\\approx 0.059$。 确保每层输出的特征图具有近似单位方差，避免梯度爆炸或消失。 ### 数据增强方法 1. 弹性形变： 生成方式：在 3 × 3 的粗糙网格上生成随机位移向量（服从高斯分布，标准差 10 像素），通过双三次插值得到每个像素的平滑位移场，对训练图像施加形变。 作用：模拟生物组织的自然形变，增强模型对形状变化的鲁棒性，减少对标注数据量的依赖。 2. 平移与旋转：增加模型对位置和角度的不变性。 3. 灰度变换：适应显微镜图像的光照变化。 4. 隐式增强：在收缩路径末端加入Dropout层（随机丢弃部分神经元），防止过拟合并提升泛化能力。 ### 优化器与超参数 随机梯度下降（SGD）：使用 Caffe 框架的SGD实现，动量（momentum）设为0.99，加速收敛并平滑参数更新方向。 高动量的意义：在单批次训练下，高动量使优化过程受历史梯度影响更大，缓解单样本噪声带来的波动。 ## 代码 U Net 模型详细代码在 [Github milesial/Pytorch UNet](https://github.com/milesial/Pytorch UNet/tree/master)。 简单表示为： <details> <summary>U Net 模型</summary> ```python class UNet(nn.Module): def __init__(self, n_channels, n_classes, bilinear False): super(UNet, self).__init__() self.n_channels n_channels self.n_classes n_classes self.bilinear bilinear self.inc (DoubleConv(n_channels, 64)) self.down1 (Down(64, 128)) self.down2 (Down(128, 256)) self.down3 (Down(256, 512)) factor 2 if bilinear else 1 self.down4 (Down(512, 1024 // factor)) self.up1 (Up(1024, 512 // factor, bilinear)) self.up2 (Up(512, 256 // factor, bilinear)) self.up3 (Up(256, 128 // factor, bilinear)) self.up4 (Up(128, 64, bilinear)) self.outc (OutConv(64, n_classes)) def forward(self, x): x1 self.inc(x) x2 self.down1(x1) x3 self.down2(x2) x4 self.down3(x3) x5 self.down4(x4) x self.up1(x5, x4) x self.up2(x, x3) x self.up3(x, x2) x self.up4(x, x1) logits self.outc(x) return logits ``` </details> ## U Net 在风格迁移上的作用 U Net 可以作为风格迁移的生成器。 风格迁移通常包括两个部分： **内容损失**：这是风格迁移中要保留的图像结构部分，要求生成图像的内容尽可能与输入的内容图像保持一致。 **风格损失**：这部分要求生成图像在纹理和颜色等方面尽量接近风格图像。 ### U Net 的结构特点 U Net 是一个典型的 **编码器 解码器结构**，它由两个主要部分组成： **编码器（Contracting Path）**：通过一系列卷积层和池化层逐步压缩图像的空间维度，同时提取图像的高层次特征（如纹理、边缘等）。这部分类似于许多生成模型中的特征提取部分，从内容图像中提取出详细的结构特征和低层次的纹理信息。 **解码器（Expanding Path）**：通过上采样操作逐步恢复图像的空间分辨率，并结合从编码器获得的特征信息进行精细的图像重建。通过跳跃连接，模型将这些特征与风格图像的特征相结合，并生成最终的风格迁移图像。 其中，**跳跃连接（Skip Connections）** 是 U Net 的关键特性之一。跳跃连接能够将编码器中的高分辨率特征直接传递到解码器中，从而帮助解码器在生成图像时更好地保留细节和空间信息。 ### U Net 在风格迁移中的优势 **保留细节**：风格迁移的目标是将风格图像的艺术风格应用到内容图像上，而这需要保留内容图像中的细节和结构。U Net 的跳跃连接使得解码器能够从编码器中获取高分辨率的特征，这有助于保持内容图像的空间结构。对于风格迁移任务，这一点尤其重要，因为风格迁移不仅仅是改变图像的纹理或颜色，还需要尽量保留原图的形状和结构。 **生成精细的图像**：U Net 的解码器部分逐渐增加图像的分辨率，并逐步生成更精细的图像细节。在风格迁移任务中，解码器通过学习将风格图像的纹理特征应用到内容图像的不同区域，从而在保持内容的同时，赋予图像新的风格。 **灵活的训练方式**：U Net 模型可以通过预训练的网络（如 VGG16）提取图像的内容和风格特征，并在训练过程中最小化内容损失和风格损失的加权和。在风格迁移的过程中，U Net 的特征提取能力使得模型能够更好地理解图像的局部细节和整体结构，从而使得生成的图像既有风格图像的艺术效果，又保留了内容图像的特征。 ### 实际操作中的优势 **逐步生成图像**：通过解码器的逐步上采样，U Net 可以从低分辨率到高分辨率逐步生成细节，类似于其他生成模型（如 GAN）中的生成过程。在风格迁移中，这样的逐步生成有助于图像的细节平滑过渡，从而避免了粗糙的风格转化效果。 **适应不同的风格图像**：U Net 不依赖于特定的风格图像，而是通过学习生成适应不同风格图像的合成图像。因此，它可以适用于各种艺术风格的迁移。 ### 与其他生成模型（如 GAN）对比 虽然 GAN（生成对抗网络）在风格迁移中也有很大的应用，但 U Net 在一些风格迁移任务中仍然是一个不错的选择，特别是在一些细节保留和训练稳定性方面： **训练稳定性**：相比于 GAN，U Net 在训练过程中可能更稳定，因为它不涉及生成器和判别器的博弈。GAN 的训练容易出现不稳定的情况（如模式崩溃等问题），而 U Net 通过直接优化损失函数（内容损失和风格损失）来训练，因此在风格迁移的过程中，通常会更容易收敛。 **图像质量**：U Net 生成的图像质量和细节也非常高，尤其是在风格迁移中，跳跃连接帮助保留了图像的结构特征，使得风格转移更自然，避免了图像过度平滑或失真。 ### VGG19 和 U Net 进行风格迁移 风格迁移简单 demo —— VGG19 + U Net，详细代码在：[Github](https://github.com/Fingsinz/StyleTransfer/blob/main/src/01.ref_and_note/02.UNet_StyleTransfer.py) 使用 VGG19 的 conv1_2 和 conv2_2 计算风格损失（捕捉纹理） 使用 conv3_4 和 conv4_4 计算内容损失（保留结构） U Net架构： 4个下采样块，每个块包含两次卷积 使用转置卷积进行上采样 通过中心裁剪实现跳跃连接的特征对齐 U Net 实验效果如下： <table> <th>Content</th> <th>Style</th> <th>Result</th> <tr> <td><img src \"../static/images/UNet/content.JPEG\"></td> <td><img src \"../static/images/UNet/style.jpg\"></td> <td><img src \"../static/images/UNet/result.png\"></td> </tr> </table>"},"/StyleTransfer/ref_and_notes/vgg.html":{"title":"VGG 网络","content":" title: VGG 网络 keywords: VGG desc: VGG文献及笔记 date: 2025 03 05 id: ref_VGG [Very Deep Convolutional Networks for Large Scale Image Recognition](https://arxiv.org/abs/1409.1556) *Simonyan K , Zisserman A .Very Deep Convolutional Networks for Large Scale Image Recognition[J].Computer Science, 2014.DOI:10.48550/arXiv.1409.1556.* > In this work we investigate the effect of the convolutional network depth on its accuracy in the large scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior art configurations can be achieved by pushing the depth to 16 19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state of the art results. We have made our two best performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. **摘要**：在这项工作中，我们研究了卷积网络深度对其在大规模图像识别设置中的准确性的影响。我们的主要贡献是使用具有非常小（3x3）卷积滤波器的架构对增加深度的网络进行全面评估，这表明通过将深度推至 16 19 权重层可以实现对现有技术配置的显着改进。这些发现是我们 2014 年 ImageNet 挑战赛提交的基础，我们的团队分别在本地化和分类轨道中获得了第一名和第二名。我们还表明，我们的方法可以很好地推广到其他数据集，从而获得最先进的结果。我们已经公开了两个性能最好的卷积神经网络模型，以促进对计算机视觉中深度视觉表示的进一步研究。 ## VGG 网络架构 架构设计创新： 固定输入尺寸为 224 × 224，采用统一的小卷积核（3 × 3）和步长（1），通过填充保持空间分辨率，配合最大池化（2 × 2，步长 2）逐步下采样。 移除局部响应归一化（LRN），因其对性能提升有限且增加计算开销。 全连接层统一为 3 层（4096 4096 1000），最后一层为 SoftMax 分类器。 ## VGG 网络配置 翻译自论文中表格 1。 <table style \"text align:center\"> <caption>ConvNet Configuration</caption> <tr> <th>A</th> <th>A LRN</th> <th>B</th> <th>C</th> <th>D</th> <th>E</th> </tr> <tr> <td>11 层</td> <td>11 层</td> <td>13 层</td> <td>16 层</td> <td>16 层</td> <td>19 层</td> </tr> <tr> <td colspan \"6\">输入层 224 x 224 RGB 图像</td> </tr> <tr> <td>conv3 64</td> <td>conv3 64 <br /> <strong>LRN</strong> </td> <td>conv3 64 <br /> <strong>conv3 64</strong> </td> <td>conv3 64 <br /> conv3 64</td> <td>conv3 64 <br /> conv3 64</td> <td>conv3 64 <br /> conv3 64</td> </tr> <tr> <td colspan \"6\">最大池化</td> </tr> <tr> <td>conv3 128</td> <td>conv3 128</td> <td>conv3 128 <br /> <strong>conv3 128</strong> </td> <td>conv3 128 <br /> conv3 128 </td> <td>conv3 128 <br /> conv3 128 </td> <td>conv3 128 <br /> conv3 128 </td> </tr> <tr> <td colspan \"6\">最大池化</td> </tr> <tr> <td>conv3 256 <br /> conv3 256 </td> <td>conv3 256 <br /> conv3 256 </td> <td>conv3 256 <br /> conv3 256 </td> <td>conv3 256 <br /> conv3 256 <br /> <strong>conv1 256</strong></td> <td>conv3 256 <br /> conv3 256 <br /> <strong>conv3 256</strong></td> <td>conv3 256 <br /> conv3 256 <br /> conv3 256 <br /> <strong>conv3 256</strong></td> </tr> <tr> <td colspan \"6\">最大池化</td> </tr> <tr> <td>conv3 512 <br /> conv3 512 </td> <td>conv3 512 <br /> conv3 512 </td> <td>conv3 512 <br /> conv3 512 </td> <td>conv3 512 <br /> conv3 512 <br /> <strong>conv1 512</strong></td> <td>conv3 512 <br /> conv3 512 <br /> <strong>conv3 512</strong></td> <td>conv3 512 <br /> conv3 512 <br /> conv3 512 <br /> <strong>conv3 512</strong></td> </tr> <tr> <td colspan \"6\">最大池化</td> </tr> <tr> <td>conv3 512 <br /> conv3 512 </td> <td>conv3 512 <br /> conv3 512 </td> <td>conv3 512 <br /> conv3 512 </td> <td>conv3 512 <br /> conv3 512 <br /> <strong>conv1 512</strong></td> <td>conv3 512 <br /> conv3 512 <br /> <strong>conv3 512</strong></td> <td>conv3 512 <br /> conv3 512 <br /> conv3 512 <br /> <strong>conv3 512</strong></td> </tr> <tr> <td colspan \"6\">最大池化</td> </tr> <tr> <td colspan \"6\">全连接层（FC 4096）</td> </tr> <tr> <td colspan \"6\">全连接层（FC 4096）</td> </tr> <tr> <td colspan \"6\">全连接层（FC 1000）</td> </tr> <tr> <td colspan \"6\">soft max</td> </tr> </table> 通过逐步增加卷积层数（11 层到 19 层），验证了网络深度的增加能显著提升分类精度。最佳模型（**VGG 16** 和 **VGG 19**）在 ImageNet 2014 挑战赛的分类和定位任务中分别取得第二和第一的成绩。 用三个 3 × 3 的卷积层代替一个 7 × 7 的卷积层： 1. 结合了三个非线性校正层而不是单个校正层，这使得决策函数更具区分性； 2. 减少了参数的数量，三个 3 × 3 卷积层的叠加等效于一个 7 × 7 卷积层，但参数量减少 81%。 3. 这可以看作是对 7 × 7 卷积滤波器施加正则化，迫使它们通过 3 × 3 滤波器（中间注入非线性）进行分解。 1 × 1 卷积层（配置 C，表 1）的结合是一种在不影响卷积层感受野的情况下增加决策函数的非线性的方法。 ## 文中分类实验 实验证明，增加网络深度（16 19层）能显著提升模型表达能力。 小卷积核的效益：通过叠加 3 × 3 卷积，在保持感受野的同时减少参数量，并引入更多非线性激活（ReLU），增强模型表达能力。 全卷积网络：测试时通过全卷积化处理任意尺寸输入，避免重复计算多个裁剪区域，显著提升效率。 多尺度策略：训练和测试时的尺度抖动有效提升模型对尺寸变化的适应能力。 参数共享：通过浅层网络预训练初始化深层网络的前几层和全连接层，加速收敛并缓解梯度不稳定问题。 ## 代码自实现 基于 VGG 网络的 CIFAR 10 分类实验代码在 [Github](https://github.com/Fingsinz/StyleTransfer/blob/main/src/01.ref_and_note/04.VGG.py)。 实验记录如下： <table> <tr> <td><img src \"../static/images/VGG/fig1.png\" /></td> <td><img src \"../static/images/VGG/fig2.png\" /></td> </tr> </table> **_my 表示为自实现，_py 表示调用 torchvision.models 的模型* <details> <summary>VGG16</summary> ```python class VGG16(nn.Module): def __init__(self, in_channels 3, num_classes 10): super(VGG16, self).__init__() self.features nn.Sequential( self._make_conv_block(in_channels, 64, 2), # conv3 64 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(64, 128, 2), # conv3 128 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(128, 256, 3), # conv3 256 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(256, 512, 3), # conv3 512 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(512, 512, 3), # conv3 512 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool ) self.avgpool nn.AdaptiveAvgPool2d((7, 7)) # 全局平均池化 self.classifier nn.Sequential( nn.Linear(512 * 7 * 7, 4096), # 全连接层（FC 1） nn.ReLU(inplace True), nn.Dropout(0.5), nn.Linear(4096, 4096), # 全连接层（FC 2） nn.ReLU(inplace True), nn.Dropout(0.5), nn.Linear(4096, num_classes), # 全连接层（FC 3） ) # nn.CrossEntropyLoss() 计算损失时隐式 soft max def _make_conv_block(self, in_channels, out_channels, num_blocks, kernel_size 3, padding 1): layers [] for _ in range(num_blocks): layers.append(nn.Conv2d(in_channels, out_channels, kernel_size kernel_size, padding padding)) layers.append(nn.BatchNorm2d(out_channels)) # 每个卷积层后加入nn.BatchNorm2d，显著提升训练稳定性 layers.append(nn.ReLU(inplace True)) in_channels out_channels return nn.Sequential(*layers) def forward(self, x): x self.features(x) x self.avgpool(x) x torch.flatten(x, 1) x self.classifier(x) return x ``` </details> <details> <summary>VGG19</summary> ```python class VGG19(nn.Module): def __init__(self, in_channels 3, num_classes 10): super(VGG19, self).__init__() self.features nn.Sequential( self._make_conv_block(in_channels, 64, 2), # conv3 64 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(64, 128, 2), # conv3 128 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(128, 256, 4), # conv3 256 × 4 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(256, 512, 4), # conv3 512 × 4 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(512, 512, 4), # conv3 512 × 4 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool ) self.avgpool nn.AdaptiveAvgPool2d((7, 7)) # 全局平均池化 self.classifier nn.Sequential( nn.Linear(512 * 7 * 7, 4096), # FC 1 nn.ReLU(inplace True), nn.Dropout(0.5), nn.Linear(4096, 4096), # FC 2 nn.ReLU(inplace True), nn.Dropout(0.5), nn.Linear(4096, num_classes), # FC 3 ) def _make_conv_block(self, in_channels, out_channels, num_blocks, kernel_size 3, padding 1): layers [] for _ in range(num_blocks): layers.append(nn.Conv2d(in_channels, out_channels, kernel_size kernel_size, padding padding)) layers.append(nn.BatchNorm2d(out_channels)) # 每个卷积层后加入nn.BatchNorm2d，显著提升训练稳定性 layers.append(nn.ReLU(inplace True)) in_channels out_channels return nn.Sequential(*layers) def forward(self, x): x self.features(x) x self.avgpool(x) x torch.flatten(x, 1) x self.classifier(x) return x ``` </details>"},"/StyleTransfer/ref_and_notes/wct.html":{"title":"特征 Whiten-Color 转换","content":" title: 特征 Whiten Color 转换 keywords: WCT desc: 特征 Whiten Color 转换 date: 2025 03 31 id: wct [Universal Style Transfer via Feature Transforms](https://arxiv.org/abs/1705.08086) *LI Y, CHEN F, YANG J, et al. Universal Style Transfer via Feature Transforms[J]. Neural Information Processing Systems,Neural Information Processing Systems, 2017.* > Universal style transfer aims to transfer arbitrary visual styles to content images. Existing feed forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures by simple feature coloring. **摘要**：通用样式转移的目的是将任意的视觉风格转移到内容图像中。现有的基于前馈的方法虽然具有较高的推理效率，但主要存在不能泛化到未知样式或视觉质量较差的问题。在本文中，我们提出了一个简单而有效的方法来解决这些限制，而不需要训练任何预定义的样式。方法的关键是嵌入到图像重建网络中的一对特征变换，即增白和着色。增白和着色变换反映了内容图像的特征协方差与给定风格图像的直接匹配，这与神经风格迁移中基于 Gram 矩阵的代价优化具有相似的精神。我们通过生成高质量的风格化图像来证明我们算法的有效性，并与许多最近的方法进行了比较。我们还通过可视化白化特征和通过简单的特征着色合成纹理来分析我们的方法。 ## 编码器 解码器结构 使用 VGG 19 作为编码器，对其进行微调；并训练解码器网络，只需将 VGG 特征反转到原始图像。 ![](../static/images/WCT/fig1.png) 解码器设计成与 VGG 19 网络对称，使用最近邻上采样层用于扩大特征图。 使用像素重建损失和特征损失对输入图像进行重建： $$ L \\vert\\vert I_o I_i \\vert\\vert _2^2 + \\lambda\\vert\\vert \\Phi(I_o) \\Phi(I_i) \\vert\\vert\\tag{1} $$ 其中，$I_i$ 是输入图像，$I_o$ 是重建输出图像，$\\Phi$ 是 VGG 编码器提取 `Relu_X_1` 的特征，$\\lambda$ 是权重系数。 ## Whitening 和 Coloring 给定内容图像 $I_c$ 和风格图像 $I_s$，首先提取向量化后的 VGG 特征 $f_c \\in \\mathfrak{R}^{C\\times H_c W_c}$ 和 $f_s \\in \\mathfrak{R}^{C\\times H_s W_s}$。接下来提出一个白化和着色变换去调整 $f_c$ 相对于 $f_s$ 的统计。即 **WCT 的目标是直接变换 $f_c$ 去匹配 $f_s$ 的协方差矩阵**。 WCT 过程包括 Whitening（白化）和 Coloring（着色）变换。 ### 白化变换 首先，$f_c$ 通过减去其均值向量使得中心化： $$ f_c \\leftarrow f_c \\mu_c\\tag{2} $$ 然后对 $f_c$ 进行式 (3) 的线性变换，得到 $\\hat{f}_c$，使得特征映射是不相关的（$\\hat{f}_c \\hat{f}_c^\\top I$）。协方差矩阵 $f_c f_c^\\top \\in \\mathfrak{R}^{C\\times C}$，衡量内容特征各通道之间的相关性（即风格信息），且满足$f_c f_c^\\top E_c D_c E_c^\\top$ $$ \\hat{f}_c E_c D_c^{ \\frac {1}{2}} E_c^\\top f_c\\tag{3} $$ $D_c\\in \\mathfrak{R}^{C\\times C}$，对角特征值矩阵：对角线元素为协方差矩阵的特征值，反映各主方向的方差强度。 $E_c\\in \\mathfrak{R}^{C\\times C}$，正交特征向量矩阵：每一列是协方差矩阵的特征向量，构成正交基，表示特征空间的主方向。 $D_c^{ \\frac {1}{2}}$，特征值缩放：对特征值取平方根的倒数，使各方向的方差归一化为 1。 $\\hat{f}_c \\in \\mathfrak{R}^{C\\times H_c W_c}$，白化后的特征：满足 $\\hat{f}_c \\hat{f}_c^\\top I$（单位矩阵），即各通道不相关且方差为1。 公式 (3) 可以理解为： 1. 通过 $E_c^\\top f_c$ 将原始特征投影到协方差矩阵的特征向量空间，消除通道间的相关性。 2. 接着 $D_c^{ \\frac {1}{2}} E_c^\\top f_c$ 对投影后的特征进行缩放，使每个方向的方差变为 1。 3. 最后 $E_c D_c^{ \\frac {1}{2}} E_c^\\top f_c$ 将归一化后的特征逆投影回原始空间，得到白化特征 $\\hat{f}_c$。 白化的核心意义： 剥离风格信息：通过消除特征通道间的相关性（协方差），去除内容特征中的风格统计特性。 保留内容结构：白化后的特征仍保留全局内容结构（如图2中的桥梁、人脸轮廓），但细节纹理（如梵高笔触）被移除。 为着色变换做准备：白化后的特征协方差为 $I$，后续通过着色变换注入目标风格的协方差，实现风格迁移。 ### 着色变换 同样，$f_s$ 通过减去其均值向量使得中心化。然后进行着色变换，基本上是白化的逆过程。 对 $\\hat{f}_c$ 进行式 (4) 的线性变换，得到 $\\hat{f}_{cs}$，使得特征映射之间存在所需的相关性（$\\hat{f}_{cs} {\\hat{f}_{cs}}^\\top f_s f_s^\\top$）。协方差矩阵 $f_s f_s^\\top \\in \\mathfrak{R}^{C\\times C}$，衡量风格图像各通道间的相关性，满足 $f_s f_s^\\top E_s D_s E_s^\\top$ $$ \\hat{f}_{cs} E_s D_s^{\\frac {1}{2}} E_s^\\top \\hat{f}_c\\tag{4} $$ $E_s \\in \\mathfrak{R}^{C\\times C}$，正交特征向量矩阵：协方差矩阵的特征向量，表示风格特征空间的主方向（如纹理方向、色彩分布模式）。 $D_s \\in \\mathfrak{R}^{C\\times C}$，对角特征值矩阵：对角线元素为协方差矩阵的特征值，反映各主方向的方差强度（如风格纹理的能量分布）。 $D_s^{ \\frac {1}{2}}$，特征值缩放：对特征值取平方根，恢复风格特征的原始方差强度。 $\\hat{f}_{cs} \\in \\mathfrak{R}^{C\\times H_c W_c}$，着色后的特征：变换后的特征满足 $\\hat{f}_{cs} {\\hat{f}_{cs}}^\\top f_s f_s^\\top$，即协方差与风格特征完全一致，且均值通过重新中心化对齐。 公式 (4) 可以理解为： 1. $E_s^\\top \\hat{f}_c$ 将白化后的内容特征投影到风格协方差矩阵的特征向量空间中。 2. $D_s^{\\frac {1}{2}} E_s^\\top \\hat{f}_c$ 则对投影后的特征进行缩放，使其方差与风格一致。 3. 然后 $E_s D_s^{\\frac {1}{2}} E_s^\\top \\hat{f}_c$ 将缩放后的特征逆投影回原始空间，得到协方差匹配的风格化特征。 4. 最后添加风格均值向量 $m_s$，使变换后的特征在均值和协方差上均与风格对齐。 着色的核心意义： 协方差匹配：通过数学变换强制白化特征的协方差与目标风格一致，实现风格统计特性的迁移（如笔触、纹理）。 均值对齐：添加风格均值向量 $m_s$，确保色彩分布与风格图像一致（如梵高画的蓝色基调）。 内容 风格分离与融合：白化剥离内容风格，着色注入目标风格，实现“内容结构 + 风格统计”的合成。 ## 多级风格迁移 从高层到低层（粗到细）：先在高语义层（如ReLU_5_1）匹配风格的整体模式，再逐步细化低层（如ReLU_1_1）的色彩和纹理细节。 反向操作（低层到高层）会导致底层细节被高层操作破坏，因此粗到细的顺序更有效。 ## 关键创新 无需风格训练：仅需预训练图像重建解码器，新风格通过 WCT 直接匹配特征统计量，无需微调网络。 高效性与通用性：单次前馈即可完成迁移，支持任意风格输入，且生成质量优于现有方法（如 Gatys 的优化方法或固定风格的前馈网络）。 用户控制：通过参数 α 调节风格化强度（公式 4），支持多尺度风格输入和空间掩码控制局部风格化。 ## 实现 无"},"/StyleTransfer/ref_and_notes/pytorch_modular.html":{"title":"PyTorch 模块化","content":" title: PyTorch 模块化 keywords: PyTorch desc: PyTorch 模块化 date: 2025 02 13 id: pytorch_modular Reference: [PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/) ## 前言 模块化后的 Python 文件： `data_setup.py`：用于准备和下载数据的文件。 `engine.py`：包含各种训练函数的文件。 `model_builder.py` 或 `model.py`：创建 PyTorch 模型的文件。 `train.py`：利用所有其他文件并训练目标 PyTorch 模型的文件 `utils.py`：专用于有用的实用程序功能的文件。 然后就可以使用类似于下面的命令调用脚本进行模型训练： ```bat python train.py model tinyvgg batch_size 32 lr 0.001 num_epochs 10 ``` 模块化后的目录： ``` root/ ├── data_setup.py ├── engine.py ├── model.py ├── train.py └── utils.py └── models/ │ ├── xxx.pth └── data/ ├── train/ │ └── xxx.jpg └── test/ └── xxx.jpg ``` ## data_setup.py：建立 Datasets 和 DataLoaders 获得数据后，将其转换为 PyTorch `Dataset` 和 `DataLoader` 。 将 `Dataset` 和 `DataLoader` 创建代码转换为一个名为 `create_dataloaders()` 的函数。 <details> <summary>data_setup.py</summary> ```python \"\"\" @file: data_setup.py @brief: 包含从图像文件夹创建 PyTorch dataloader 的函数。 @author: @date: 2025 02 13 \"\"\" import os from torchvision import datasets, transforms from torch.utils.data import DataLoader NUM_WORKERS 3 def create_dataloaders( train_dir: str, test_dir: str, transform: transforms.Compose, batch_size: int, num_workers: int NUM_WORKERS, ) > tuple[DataLoader, DataLoader, list[str]]: \"\"\" 从图像文件夹创建PyTorch dataloader。 参数: train_dir (str): 训练图像文件夹的路径。 test_dir (str): 测试图像文件夹的路径。 transform (transforms.Compose): 应用的图像变换。 batch_size (int): DataLoader 的批量大小。 num_workers (int): 用于加载数据的工作线程数。 返回: 包含训练 DataLoader、测试 DataLoader 和类名列表的元组。 \"\"\" train_data datasets.ImageFolder(train_dir, transform transform) test_data datasets.ImageFolder(test_dir, transform transform) class_names train_data.classes train_dataloader DataLoader( train_data, batch_size batch_size, shuffle True, num_workers num_workers, pin_memory True, ) test_dataloader DataLoader( test_data, batch_size batch_size, shuffle True, num_workers num_workers, pin_memory True, ) return train_dataloader, test_dataloader, class_names ``` </details> ## model.py: 构建模型 将模型放入其文件中使得可以一次又一次地重用它。 <details> <summary>data_setup.py</summary> ```python \"\"\" @file: model.py @brief: TinyVGG 定义 @author: @date: 2025 02 13 \"\"\" import torch import torch.nn as nn class TinyVGG(nn.Module): \"\"\" 一个简单的 VGG 类神经网络图像分类任务。 参数: input_channels (int): 输入通道数 (e.g., 3 for RGB images)。 hidden_units (int): 卷积层中隐藏单元的数量。 output_shape (int): 输出类的数量。 \"\"\" def __init__(self, input_channels: int, hidden_units: int, output_shape: int) > None: super().__init__() self.block_1 nn.Sequential( nn.Conv2d(in_channels input_channels, out_channels hidden_units, kernel_size 3, stride 1, padding 1), nn.ReLU(), nn.Conv2d(in_channels hidden_units, out_channels hidden_units, kernel_size 3, stride 1, padding 1), nn.ReLU(), nn.MaxPool2d(2) ) self.block_2 nn.Sequential( nn.Conv2d(hidden_units, hidden_units, kernel_size 3, padding 1), nn.ReLU(), nn.Conv2d(hidden_units, hidden_units, kernel_size 3, padding 1), nn.ReLU(), nn.MaxPool2d(2) ) self.classifier nn.Sequential( nn.Flatten(), nn.Linear(in_features hidden_units * 16 * 16, out_features output_shape) ) def forward(self, x: torch.Tensor) > torch.Tensor: \"\"\" 前向传播 参数: x (torch.Tensor): 输入张量，形状：(batch_size, input_channels, height, width)。 返回: torch.Tensor: 输出张量，形状：(batch_size, output_shape)。 \"\"\" return self.classifier(self.block_2(self.block_1(x))) ``` </details> ## engine.py：训练和测试模型 在前面编写了几个训练函数： `train_step()`：接受一个模型、一个 DataLoader、一个损失函数和一个优化器，并在 DataLoader 上训练一个 step。 `test_step`：接受一个模型、一个 DataLoader 和一个损失函数，并在 DataLoader 上测试模型。 `train()`：执行 `train_step()` 和 `test_step()`。并返回一个结果字典。 <details> <summary>engine.py</summary> *需要下载 tqdm 包。* ```python \"\"\" @file: engine.py @brief: PyTorch模型的训练和测试函数 @author: @date: 2025 02 13 \"\"\" import torch from tqdm.auto import tqdm from typing import Dict, List, Tuple def train_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, device: torch.device) > Tuple[float, float]: \"\"\" 模型逐步训练 参数: model: 要训练的模型 dataloader: 用于训练的 DataLoader loss_fn: 损失函数 optimizer: 优化器 device: 设备 (e.g. GPU or CPU) 返回值: 包含训练损失和正确率的元组 \"\"\" model.train() train_loss, train_acc 0, 0 for batch, (X, y) in enumerate(dataloader): X, y X.to(device), y.to(device) y_pred model(X) loss loss_fn(y_pred, y) train_loss + loss.item() optimizer.zero_grad() loss.backward() optimizer.step() y_pred_class torch.argmax(torch.softmax(y_pred, dim 1), dim 1) train_acc + (y_pred_class y).sum().item() / len(y_pred) train_loss train_loss / len(dataloader) train_acc train_acc / len(dataloader) return train_loss, train_acc def test_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, device: torch.device) > Tuple[float, float]: \"\"\" 模型逐步测试 参数: model: 要测试的模型 dataloader: 用于测试数据的 DataLoader loss_fn: 损失函数 device: 设备 (e.g. GPU or CPU) 返回值: 包含训练损失和正确率的元组 \"\"\" model.eval() test_loss, test_acc 0, 0 with torch.inference_mode(): for batch, (X, y) in enumerate(dataloader): X, y X.to(device), y.to(device) test_pred_logits model(X) loss loss_fn(test_pred_logits, y) test_loss + loss.item() test_pred_labels test_pred_logits.argmax(dim 1) test_acc + ((test_pred_labels y).sum().item() / len(test_pred_labels)) test_loss test_loss / len(dataloader) test_acc test_acc / len(dataloader) return test_loss, test_acc def train(model: torch.nn.Module, train_dataloader: torch.utils.data.DataLoader, test_dataloader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer, loss_fn: torch.nn.Module, epochs: int, device: torch.device) > Dict[str, List]: \"\"\" 训练模型多个 epoch 参数: model: 训练的模型 train_dataloader: 训练数据的 DataLoader test_dataloader: 测试数据的 DataLoader optimizer: 优化器 loss_fn: 损失函数 epochs: 训练轮数 device: 设备 (e.g. GPU or CPU) 返回值: 包含训练和测试损失和正确率的字典 \"\"\" results {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": [] } for epoch in tqdm(range(epochs)): train_loss, train_acc train_step(model model, dataloader train_dataloader, loss_fn loss_fn, optimizer optimizer, device device) test_loss, test_acc test_step(model model, dataloader test_dataloader, loss_fn loss_fn, device device) print( f\"\\nEpoch: {epoch+1} \" f\"train_loss: {train_loss:.4f} \" f\"train_acc: {train_acc:.4f} \" f\"test_loss: {test_loss:.4f} \" f\"test_acc: {test_acc:.4f}\" ) results[\"train_loss\"].append(train_loss) results[\"train_acc\"].append(train_acc) results[\"test_loss\"].append(test_loss) results[\"test_acc\"].append(test_acc) return results ``` </details> ## utils.py：实用函数集合 通常情况下，在训练期间或训练后需要保存模型。 将 helper 函数存储在名为 `utils.py` （utilities的缩写）的文件中。 <details> <summary>utils.py</summary> ```python \"\"\" @file: utils.py @brief: 包含实用函数 @author: @date: 2025 02 13 \"\"\" import torch from pathlib import Path from typing import Union def save_model(model: torch.nn.Module, target_dir: str, model_name: str): \"\"\" 保存模型 参数: model (torch.nn.Module): 要保存的模型。 target_dir (str): 保存模型的目标目录。 model_name (str): 保存的模型文件的名称。 异常: 断言错误: 如果 model_name 不以.pt或. pth结尾。 \"\"\" target_dir_path Path(target_dir) target_dir_path.mkdir(parents True, exist_ok True) assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with .pt or .pth\" model_save_path target_dir_path / model_name print(f\"[INFO] Saving model to: {model_save_path}\") torch.save(obj model.state_dict(), f model_save_path) ``` </details> ## train.py：训练、评估和保存模型 在其它项目里，经常会遇到将其所有功能组合在一个 `train.py` 文件中。 在这里的 `train.py` 文件中，将结合我们创建的其他 Python脚本的所有功能，并使用它来训练模型。 有以下步骤： 1. 从目录中导入 `data_setup`、`engine`、`model`、`utils` 等各种依赖项。 2. 设置各种超参数，如批量大小，epoch数，学习率和隐藏单元数（这些可以在将来通过 Python 的 argparse 设置）。 3. 设置训练和测试目录。 4. 设置使用设备。 5. 创建必要的数据转换。 6. 使用 `data_setup.py` 创建 DataLoader。 7. 使用 `model.py` 创建模型。 8. 设置损失函数和优化器。 9. 使用 `engine.py` 训练模型。 10. 使用 `utils.py` 保存模型。 <details> <summary>train.py</summary> ```python \"\"\" @file: train.py @brief: 训练模型 @author: @date: 2025 02 13 \"\"\" import os import torch import data_setup, engine, model, utils from torchvision import transforms # 训练超参数 NUM_EPOCHS 5 BATCH_SIZE 32 HIDDEN_UNITS 10 LEARNING_RATE 0.001 # 数据路径 train_dir \"../data/pizza_steak_sushi/train\" test_dir \"../data/pizza_steak_sushi/test\" # 训练设备 device \"cuda\" if torch.cuda.is_available() else \"cpu\" if __name__ \"__main__\": # 数据预处理 train_transform transforms.Compose([ transforms.Resize((64, 64)), transforms.ToTensor(), ]) # 创建数据加载器 train_dataloader, test_dataloader, class_names data_setup.create_dataloaders( train_dir train_dir, test_dir test_dir, transform train_transform, batch_size BATCH_SIZE, ) # 创建模型 model model.TinyVGG( input_channels 3, hidden_units HIDDEN_UNITS, output_shape len(class_names), ).to(device) # 设置损失函数和优化器 loss_fn torch.nn.CrossEntropyLoss() optimizer torch.optim.Adam(params model.parameters(), lr LEARNING_RATE) # 启动训练 engine.train(model model, train_dataloader train_dataloader, test_dataloader test_dataloader, optimizer optimizer, loss_fn loss_fn, epochs NUM_EPOCHS, device device) # 保存模型 utils.save_model(model model, target_dir \"models\", model_name \"tinyvgg_model.pth\") ``` </details> *可研究 `argparse` 的使用优化。* 测试调用： ```bat python .\\train.py 0% 0/5 [00:00<?, ?it/s] Epoch: 1 train_loss: 1.1113 train_acc: 0.3047 test_loss: 1.0953 test_acc: 0.3400 20%██████████████▌ 1/5 [00:18<01:14, 18.60s/it] Epoch: 2 train_loss: 1.1072 train_acc: 0.2969 test_loss: 1.0744 test_acc: 0.4233 40%█████████████████████████████▏ 2/5 [00:36<00:55, 18.45s/it] Epoch: 3 train_loss: 1.0931 train_acc: 0.4141 test_loss: 1.0908 test_acc: 0.3617 60%███████████████████████████████████████████▊ 3/5 [00:55<00:36, 18.45s/it] Epoch: 4 train_loss: 1.0891 train_acc: 0.4180 test_loss: 1.0931 test_acc: 0.3722 80%██████████████████████████████████████████████████████████▍ 4/5 [01:13<00:18, 18.46s/it] Epoch: 5 train_loss: 1.0622 train_acc: 0.4766 test_loss: 1.0636 test_acc: 0.4621 100%█████████████████████████████████████████████████████████████████████████ 5/5 [01:32<00:00, 18.43s/it] [INFO] Saving model to: models\\tinyvgg_model.pth ```"},"/StyleTransfer/ref_and_notes/pytorch_tensor.html":{"title":"PyTorch 张量","content":"Reference:[ZerotoMasteryLearnPyTorchforDeepLearning](https://www.learnpytorch.io/)*该页面由JupyterNotebook生成，原文件于[Github](https://github.com/Fingsinz/StyleTransfer/tree/main/src/02.pytorch_learning/pytorch_tensor.ipynb)*importtorchtorch.__version__##什么是张量张量用于表示数据，是机器学习的基本组成部分。 图片可以是三维张量，如`[height,width,channel]`，如经典的lena图片用张量表示：importnumpyasnpfromPILimportImage#使用pillow打开图片,转换为numpy矩阵,再转换为torch张量img torch.from_numpy(np.array(Image.open(\"imgs/lena.jpg\")))img.shape##创建张量Tensors说明文档：[https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html)1.Scalar，标量是一个单独的数字，用张量的术语来说是一个零维张量。scalar torch.tensor(3.0)#维度同样可以通过tensor.dim()获取print(f\"scalar为{scalar},维度为{scalar.ndim},常量通过item方法获取{scalar.item()}数字\")scalar为3.0, 维度为0, 常量通过item方法获取3.0数字2.Vector，向量是一个一维张量，类似于数组。vector torch.tensor([1.0,2.0,3.0])print(f\"vector为{vector},维度为{vector.ndim},通过shape属性获取形状{vector.shape}\")vector为tensor([1., 2., 3.]), 维度为1, 通过shape属性获取形状torch.Size([3])3.Matrix，矩阵是一个二维张量。matrix torch.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]])print(f\"{matrix},\\n维度为{matrix.ndim},通过shape属性获取形状{matrix.shape}\")tensor([[1., 2., 3.], [4., 5., 6.]]), 维度为2, 通过shape属性获取形状torch.Size([2, 3])总结：结构表示维度: :: :: :scalar一个数字0vector一组数字1matrix一个矩阵2tensor若干维度0维表示scalar，每一维表示一个vector###`torch.rand()`生成随机张量实际上在机器学习中很少会手动创建张量，更多是随机生成。#创建指定大小的随机张量random_tensor torch.rand(size (3,4))random_tensor,random_tensor.dtype###填充全零或全一张量zeros torch.zeros(size (3,4))ones torch.ones(size (3,4))zeros,zeros.dtypeones,ones.dtype###创建一个范围张量#创建一个从0到9的张量的两种方法#zero_to_ten1 torch.range(0,10)#将弃用zero_to_ten2 torch.arange(start 0,end 10,step 1)#zero_to_ten1,zero_to_ten1.dtypezero_to_ten2,zero_to_ten2.dtype#创建一个形状一样的向量same_shape torch.zeros_like(input zero_to_ten2)same_shape,same_shape.dtype##张量数据类型Tensor的DataTypes：[https://pytorch.org/docs/stable/tensors.html#data types](https://pytorch.org/docs/stable/tensors.html#data types)有些数据类型是特定于CPU，而有些更适合GPU。同时确保精度问题，可以选用不同精度的浮点数类型。float32_tensor torch.tensor([3.0,6.0,9.0],dtype None,#默认为None，即torch。Float32或传递的任何数据类型device None,#默认为None，使用默认的张量类型requires_grad False)#如果为True，则记录对张量执行的操作float32_tensor.shape,float32_tensor.dtype,float32_tensor.device可以修改张量的数据类型：float64_tensor float32_tensor.type(torch.float64)float64_tensor.dtype在进行带张量的操作时，除了张量的Shape要匹配之外，还需要注意张量的dtype和device。 `tensor.shape`：获取Shape。 `tensor.dtype`：获取dtype。 `tensor.device`：获取device。##张量的操作###张量的基础操作张量的加减乘操作如下：test_tensor torch.tensor([1,2,3])test_tensor+10,test_tensor*10,test_tensor 1,test_tensor#在不赋值的时候是不变的也可以通过函数实现：torch.add(test_tensor,10),torch.mul(test_tensor,10),torch.sub(test_tensor,1)注意，**矩阵乘法遵循其规则，与形状相关。**$$M_{m\\timesn} M_{m\\timesk}@M_{k\\timesn}$$*`@`在Python中是矩阵乘法*tensor torch.tensor([1,2,3])tensor*tensor,tensor@tensor,torch.matmul(tensor,tensor)#torch.matmul是矩阵乘法，且比@操作更快 $[1,2,3]*[1,2,3] [1*1,2*2,3*3] [1,4,9]$ $[1,2,3]@[1,2,3] 1*1+2*2+3*3 14$`torch.mm()`是`torch.matmul()`的缩写。另外提供一些操作进行矩阵变换： `torch.transpose(input,dim0,dim1)`，`input`是输入矩阵，`dim0`和`dim1`是要交换的维度。 `torch.T`：转置矩阵。###求最小值、最大值、平均值、总和等x torch.arange(0,100,10)print(x)print(f\"最小值:{x.min()}\")print(f\"最大值:{x.max()}\")#print(f\"Mean:{x.mean()}\")#会报错print(f\"均值:{x.type(torch.float32).mean()}\")#没有float数据类型将无法工作print(f\"总和:{x.sum()}\")tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90]) 最小值: 0 最大值: 90 均值: 45.0 总和: 450*一些方法，如torch.mean()，要求张量位于torch.float32（最常见）或其他特定数据类型中，否则操作将失败。*###求最小最大值的位置print(x)print(f\"Indexwheremaxvalueoccurs:{x.argmax()}\")print(f\"Indexwhereminvalueoccurs:{x.argmin()}\")tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90]) Index where max value occurs: 9 Index where min value occurs: 0###张量形状重塑、堆叠、挤压和扩展因为深度学习模型（神经网络）都是关于以某种方式操纵张量的。因为矩阵乘法的规则，如果有形状不匹配，就会遇到错误。这些方法帮助你确保你的张量的正确元素与其他张量的正确元素混合在一起。方法描述: :: :[torch.reshape(input,shape)](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape)或`torch.Tensor.reshape()`在兼容的情况下把`input`重塑成`shape`的形状[Tensor.view(shape)](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)以不同的形状返回原始张量的视图，但与原始张量共享相同的数据[torch.stack(tensors,dim 0)](https://pytorch.org/docs/1.9.1/generated/torch.stack.html)沿着一个新的维度`dim`连接一系列张量，所有张量必须是相同的大小[torch.squeeze(input)](https://pytorch.org/docs/stable/generated/torch.squeeze.html)挤压`input`，删除值为1的所有维度[torch.unsqueeze(input,dim)](https://pytorch.org/docs/1.9.1/generated/torch.unsqueeze.html)在`dim`处添加值为1的维度并返回[torch.permute(input,dims)](https://pytorch.org/docs/stable/generated/torch.permute.html)返回原始输入的视图，其维度重新排列`tensor.reshape()`：importtorchx torch.arange(1.,9.)x_reshaped x.reshape(1,8)#重塑print(f\"x.shape:{x.shape},x_reshaped.shape:{x_reshaped.shape}\")x.shape: torch.Size([8]), x_reshaped.shape: torch.Size([1, 8])`tensor.view()`：改变视图也会改变原来的张量。x_viewed x.view(2,4)#重塑print(f\"x.shape:{x.shape},x_viewed.shape:{x_viewed.shape}\")#修改x_viewd,x同步变化x_viewed[:,0] 5print(x)print(x_viewed)x.shape: torch.Size([8]), x_viewed.shape: torch.Size([2, 4]) tensor([5., 2., 3., 4., 5., 6., 7., 8.]) tensor([[5., 2., 3., 4.], [5., 6., 7., 8.]])用该函数改变一个张量的视图实际上只会创建同一个张量的新视图。如果想要将新张量在自身之上堆叠五次，可以使用`torch.stack()`来实现。x_stacked torch.stack([x,x,x,x],dim 0)x_stacked同时可以移除单维度：print(x_reshaped.shape)x_squzzed x_reshaped.squeeze()print(x_squzzed.shape)torch.Size([1, 8]) torch.Size([8])与`torch.squeeze()`相反，可以使用`torch.unsqueeze()`在特定索引处添加一个维度值1：print(x_squzzed.shape)x_unsquzzed x_squzzed.unsqueeze(dim 0)print(x_unsquzzed.shape)torch.Size([8]) torch.Size([1, 8])`torch.permute(input,dims)`重排张量的维度：img torch.rand(size (128,256,3))img_permuted img.permute(2,0,1)img.shape,img_permuted.shape###张量取下标importtorchx torch.arange(1,10).reshape(1,3,3)print(f\"{x},{x.shape}\")print(f\"x[0]:\\n{x[0]}\")print(f\"x[0][0]:{x[0][0]}\")print(f\"x[0][0][0]:{x[0][0][0]}\")tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]), torch.Size([1, 3, 3]) x[0]: tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) x[0][0]: tensor([1, 2, 3]) x[0][0][0]: 1*可以使用`:`来指定此维度中的所有值，使用逗号`,`来添加另一个维度。*##Pytorch张量和NumpyNumPy和PyTorch数据结构互转： NumpyArray >PyTorchTensor：`torch.from_numpy(ndarray)`。 PyTorchTensor >NumpyArray：`torch.Tensor.numpy()`。##Tensor随机值`torch.rand()`方法可以生成一个给定大小而值随机的张量，但是每次生成都会不一样。如果需要每次随机都一样，需要固定下随机数种子。importtorchimportrandomRANDOM_SEED 42torch.manual_seed(seed RANDOM_SEED)random_tensor_A torch.rand(3,4)torch.random.manual_seed(seed RANDOM_SEED)random_tensor_B torch.rand(3,4)print(f\"TensorA:\\n{random_tensor_A}\\n\")print(f\"TensorB:\\n{random_tensor_B}\\n\")print(f\"A B?\")print(random_tensor_A random_tensor_B)Tensor A: tensor([[0.8823, 0.9150, 0.3829, 0.9593], [0.3904, 0.6009, 0.2566, 0.7936], [0.9408, 0.1332, 0.9346, 0.5936]]) Tensor B: tensor([[0.8823, 0.9150, 0.3829, 0.9593], [0.3904, 0.6009, 0.2566, 0.7936], [0.9408, 0.1332, 0.9346, 0.5936]]) A B? tensor([[True, True, True, True], [True, True, True, True], [True, True, True, True]])##GPU下使用张量导入PyTorch：importtorchtorch.cuda.is_available()设置设备类型：#Setdevicetypedevice \"cuda\"iftorch.cuda.is_available()else\"cpu\"device检查设备数：torch.cuda.device_count()###张量在CPU和GPU间移动通过调用`to(device)`将张量（和模型）放在特定的设备上。GPU可以提供比CPU更快的数值计算，但有时候某些操作不支持在GPU中执行，所以需要将张量进行移动。张量移动到GPU侧：tensor torch.tensor([1,2,3])print(tensor,tensor.device)tensor_on_gpu tensor.to(device)print(tensor_on_gpu)tensor([1, 2, 3]) cpu tensor([1, 2, 3], device 'cuda:0')张量移动到CPU侧：通过使用`tensor.CPU()`tensor_back_on_cpu tensor_on_gpu.cpu()print(tensor_back_on_cpu)#上面的代码返回CPU内存中GPU张量的副本，原始张量仍然在GPU上。print(tensor_on_gpu)tensor([1, 2, 3]) tensor([1, 2, 3], device 'cuda:0')"},"/StyleTransfer/ref_and_notes/index.html":{"title":"参考文献及笔记","content":" title: 参考文献及笔记 keywords: desc: 参考文献阅读及其代码测试 date: 2025 01 16 class: heading_no_counter ### TODO 1. 编写基本的项目框架代码，实现一种方法（MetaNets），后面考虑扩展比较 AdaIN （[Arbitrary Style Transfer in Real time with Adaptive Instance Normalization](https://arxiv.org/abs/1703.06868)）和特征变换（[Universal Style Transfer via Feature Transforms](https://arxiv.org/abs/1705.08086)） 2. 学习评估指标 [风格转移指标（Style Transfer Indicator）Evaluation of Painting Artistic Style Transfer Based on Generative Adversarial Network](https://ieeexplore.ieee.org/document/10154714) 3. 学习 PyTorch 框架(2) \t [ ] 迁移学习 [ ] 实验跟踪 [ ] 论文复制 [ ] 模型部署 ### 涉及文献 1. [VGG——Very Deep Convolutional Networks for Large Scale Image Recognition](https://arxiv.org/abs/1409.1556) 2. [ResNet——Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) 3. [U Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) 4. [GAN——Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) 5. [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) 6. [PatchGAN 起源 —— Image to Image Translation with Conditional Adversarial Networks](https://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image To Image_Translation_With_CVPR_2017_paper.pdf) 7. [将PatchGAN扩展为多尺度（3级金字塔）—— High Resolution Image Synthesis and Semantic Manipulation with Conditional GANs](https://arxiv.org/abs/1711.11585) 8. [Unpaired Image to Image Translation using Cycle Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593) 9. [Meta Networks for Neural Style Transfer](https://arxiv.org/abs/1709.04111) ### 风格迁移实战代码参考 1. Image Style Transfer Using Convolutional Neural Networks：[https://github.com/b06b01073/style transfer/tree/main](https://github.com/b06b01073/style transfer/tree/main)"},"/StyleTransfer/ref_and_notes/resnet.html":{"title":"ResNet：残差网络","content":" title: ResNet：残差网络 keywords: ResNet desc: ResNet文献及笔记 date: 2025 03 02 id: ref_ResNet [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) *He K , Zhang X , Ren S ,et al.Deep Residual Learning for Image Recognition[J].IEEE, 2016.DOI:10.1109/CVPR.2016.90.* > Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers 8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR 10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. **摘要**：更深的神经网络会更难进行训练。我们提出一个残差学习框架，可以简化比以前使用的网络深度更深的网络的训练。我们明确地将层重新表述为参考层输入的学习残差函数，而不是学习未参考的函数。我们提供了全面的经验证据，表明这些残差网络更容易优化，并且可以从相当大的深度中获得精度。在 ImageNet 数据集上，我们评估了深度高达 152 层的残差网络——比 VGG 网络深 8 倍，但仍然具有较低的复杂性。这些残差网络的集合在 ImageNet测试集上的误差达到 3.57%。该结果在 ILSVRC 2015 分类任务中获得第一名。我们还对 100 层和 1000 层的 CIFAR 10 进行了分析。表征的深度对于许多视觉识别任务至关重要。仅仅由于我们的深度表示，我们在 COCO 对象检测数据集上获得了 28% 的相对改进。深度残差网络是我们提交 ILSVRC 和 COCO 2015 竞赛的基础，我们还在 ImageNet 检测，ImageNet 本地化，COCO 检测和 COCO 分割任务中获得了第一名。 ## 残差块模型 通过引入残差块（Residual Block）和快捷连接（Shortcut Connection），解决了深度神经网络训练中的退化问题（随着深度增加，训练误差不降反升）。 1. **残差块**：将网络层的映射目标从 **直接学习目标函数 $\\mathcal{H}(x)$** 变成 **学习残差函数 $\\mathcal{F}(x) \\mathcal{H}(x) x$**，最终输出为 $\\mathcal{F}(x)+x$。 若恒等映射（Identity Mapping）是最优解，则残差函数更容易趋近于零，简化了优化过程。 > 注：恒等映射是一个输入与输出相同的函数，即 $f(x) x$。在神经网络中，它表现为直接将输入传递到输出，不做任何变换。 2. **快捷连接**：在堆叠层之间添加跨层连接，直接将输入与输出相加，无需额外参数。 快捷连接直接将输入 $x$ 加到残差函数 $\\mathcal{F}(x)$ 的输出上，相当于强制网络仅学习输入与目标之间的 **残差**（差异），而非完整的映射。 整个网络仍然可以通过反向传播的 SGD 进行端到端训练。 ![残差块](../static/images/ResNet/fig1.png) ## 残差网络理论 论文表明： 极深残差网络很容易优化，但是当深度增加时，对应的“普通”网络（简单地堆叠层）会表现出更高的训练误差； 深度残差网络可以很容易地从显着增加的深度中获得准确度增益，产生的结果比以前的网络要好得多。 与先前工作的关键区别： 残差函数 vs 完整映射：ResNet显式学习残差（$\\mathcal{F}(x) \\mathcal{H}(x) x$），而传统网络直接学习 $\\mathcal{H}(x) $。 无门控 vs 门控：高速公路网络依赖参数化的门控函数，而ResNet通过恒等映射，无需参数，保持信息无损传递。 ### 残差学习 如果假设多个非线性层可以渐近逼近复杂函数 $\\mathcal{H}(x)$，则等价于假设这多个非线性层可以渐近逼近残差函数。残差学习将其重新定义为 $\\mathcal{F}(x) \\mathcal{H}(x) x$（假设输入和输出具有相同的维数）。 因此，不需要期望堆叠层来近似 $\\mathcal{H}(x)$，而实际学习的是残差函数，最终输出为 $y \\mathcal{F}(x)+x$。 若恒等映射 $\\mathcal{H}(x) x$ 是最优解，则残差函数 $\\mathcal{F}(x)$ 只需趋近于零，比直接学习完整映射更简单。 在实际场景中，恒等映射不太可能是最优的，但我们的重新表达可能有助于解决这个问题。 如果最优函数更接近于恒等映射而不是零映射，求解器应该更容易找到参考恒等映射的扰动，而不是学习函数作为新函数。 ### 快捷连接 输入 $x$ 直接跨过堆叠的层，与残差函数 $\\mathcal{F}(x)$ 的输出相加。 当输入输出维度相同时，快捷连接直接传递 $x$（无参数）；维度不同时，快捷连接执行线性投影 $W_s$ 以匹配维度： $$ \\begin{aligned} \\text{y} &\\mathcal{F}(x)+x,& 维度相同\\\\ \\text{y} &\\mathcal{F}(x)+W_s x,& 维度不同 \\end{aligned} $$ 残差块中，ReLU 激活函数位于残差函数 $\\mathcal{F}(x)$ 之后，即：$y \\sigma(\\mathcal{F}(x)+x)$，其中 $\\sigma$ 表示 ReLU。 并且，残差函数是灵活的，可以包含多个堆叠层，全连接层、卷积层等等。但只有一个层时，并没有优势。 ### 瓶颈结构 为了降低计算复杂度，适用于极深网络（如ResNet 50/101/152）。 组成：1 × 1 卷积（降维）→ 3 × 3 卷积 → 1 × 1 卷积（恢复维度），形成“瓶颈”结构（下图右边）。 ![瓶颈结构](../static/images/ResNet/fig2.png) ### 网络架构 ![ResNet](../static/images/ResNet/fig3.png) ResNet 9 适用于轻量级任务，如CIFAR 10。补充 ResNet 9 的大致架构： 层级 操作 : :: : conv1 3 × 3 卷积，64 通道，步长为 1 conv2_x 1 个基础残差块（2 层 3 × 3 卷积， 64 通道） conv3_x 1 个基础残差块（2 层 3 × 3 卷积， 128 通道，下采样） conv4_x 1 个基础残差块（2 层 3 × 3 卷积， 256 通道，下采样） 全局平均池化 输出尺寸 1 × 1 全连接层 若干个神经元输出 大致对比： 网络 残差块类型 阶段块数（conv2_x到conv5_x） : :: :: : ResNet 9 基础块 [1,1,1,0] ResNet 18 基础块 [2,2,2,2] ResNet 34 基础块 [3,4,6,3] ResNet 50 瓶颈块 [3,4,6,3] ResNet 101 瓶颈块 [3,4,23,3] ResNet 152 瓶颈块 [3,8,36,3] ## 论文实验总结 论文的第四部分 Experiments 通过大量实验验证了残差网络（ResNet）的性能优势，涵盖多个数据集和任务，并深入分析了不同设计选择的影响。 *DeepSeek 总结* ### ImageNet图像分类实验 实验设置： 数据集：ImageNet 2012（1.28M 训练图像，50K 验证图像）。 网络结构：测试了 18 层、34 层、50 层、101 层、152 层的普通网络（Plain Net）和残差网络（ResNet）。 训练细节：SGD 优化器，初始学习率 0.1，分阶段衰减；数据增强包括随机裁剪、水平翻转、颜色扰动；使用批量归一化（BN）但无 Dropout。 关键结果： 退化问题验证：34 层普通网络的训练误差高于 18 层普通网络，而 34 层 ResNet 显著优于 18 层 ResNet，证明残差学习解决了退化问题。 极深网络表现：ResNet 152 在 ImageNet 上实现单模型 3.57% top 5 错误率（集成模型），赢得 ILSVRC 2015 分类任务冠军。 对比 VGG 和 GoogLeNet：ResNet 152 的复杂度（11.3 亿 FLOPs ）低于 VGG 19（19.6 亿 FLOPs），但性能显著更优。 ### CIFAR 10分析实验 实验目的：验证残差学习在更小数据集上的泛化能力。 网络设计：堆叠简单残差块（每个残差块包含两个3×3卷积层），测试20层到1202层的网络。 关键发现： 极深网络的可行性：ResNet 110（170 万参数）达到 6.43% 测试错误率，优于当时的先进方法（如高速公路网络）。 1202层网络训练：虽然训练误差趋近于零，但测试误差因过拟合略升至 7.93%，表明需要更强正则化。 ### 快捷连接的消融实验 对比选项： 选项 A：维度不匹配时用零填充（无参数）。 选项 B：维度不匹配时用 1 × 1 卷积调整（含参数）。 选项 C：所有快捷连接均为 1 × 1 卷积（大量参数）。 结论： 选项 B 略优于 A：因零填充无法学习残差。 选项 C 提升有限：参数过多但收益不高，最终选择选项 B 用于极深网络（如 ResNet 50/101/152）。 ### 目标检测与分割实验 任务与框架： 目标检测：基于 Faster R CNN，将 VGG 16 替换为 ResNet 101。 数据集：PASCAL VOC 2007/2012、MS COCO。 关键结果： COCO 检测：ResNet 101 相比 VGG 16，mAP@[.5, .95] 提升 6%（相对提升28%），验证了深层特征的泛化能力。 竞赛表现：在 ILSVRC 2015 中，ResNet 赢得检测、定位、分割任务冠军。 ### 极深网络的可视化与分析 残差响应分析： ResNet的残差函数输出的响应值普遍较小，表明网络更倾向于学习接近恒等映射的微调。 网络越深，单个残差块的调整幅度越小，验证了残差学习的“微扰动”假设。 ### 对比其他先进模型 与高速公路网络对比： ResNet在超过100层时仍能提升性能，而高速公路网络（Highway Networks）在极深时表现下降。 ResNet的快捷连接无参数，计算更高效。 ## 代码实现 基于残差网络的 CIFAR 10 分类实验代码在 [Github](https://github.com/Fingsinz/StyleTransfer/blob/main/src/01.ref_and_note/03.ResNet.py)。 实验记录如下： <table> <tr> <td><img src \"../static/images/ResNet/fig4.png\" /></td> <td><img src \"../static/images/ResNet/fig5.png\" /></td> </tr> </table> 部分代码如下： <details> <summary>实现简单残差块</summary> ```python class BasicBlock(nn.Module): \"\"\"残差块\"\"\" expansion 1 def __init__(self, in_channels, out_channels, stride 1, downsample None): super(BasicBlock, self).__init__() # 第一层卷积 → 批量归一化 → ReLU → 第二层卷积 → 批量归一化 self.conv1 nn.Conv2d( in_channels, out_channels, kernel_size 3, stride stride, padding 1, bias False ) self.bn1 nn.BatchNorm2d(out_channels) self.relu nn.ReLU(inplace True) self.conv2 nn.Conv2d( out_channels, out_channels, kernel_size 3, stride 1, padding 1, bias False ) self.bn2 nn.BatchNorm2d(out_channels) self.downsample downsample def forward(self, x): identity x # 恒等映射 # 第一层卷积 → 批量归一化 → ReLU → 第二层卷积 → 批量归一化 out self.conv1(x) out self.bn1(out) out self.relu(out) out self.conv2(out) out self.bn2(out) if self.downsample is not None: identity self.downsample(x) out + identity # 残差连接，F(x) + x out self.relu(out) # ReLU return out ``` </details> <details> <summary>基于残差块构建 ResNet 9</summary> ```python class ResNet9(nn.Module): \"\"\"ResNet9 model\"\"\" def __init__(self, num_classes): super(ResNet9, self).__init__() self.in_channels 64 # 初始卷积层 self.conv1 nn.Conv2d(3, 64, kernel_size 7, stride 2, padding 3, bias False) self.bn1 nn.BatchNorm2d(64) self.relu nn.ReLU(inplace True) self.maxpool nn.MaxPool2d(kernel_size 3, stride 2, padding 1) # 残差块 self.layer1 self._make_layer(BasicBlock, 64, 1, stride 1) self.layer2 self._make_layer(BasicBlock, 128, 1, stride 2) self.layer3 self._make_layer(BasicBlock, 256, 1, stride 2) # 全局平均池化和全连接层 self.avgpool nn.AdaptiveAvgPool2d((1, 1)) self.fc nn.Linear(256 * BasicBlock.expansion, num_classes) def _make_layer(self, block, out_channels, blocks, stride): downsample None # 检查是否需要下采样：如果步长不为 1 或输入输出通道数不匹配，则创建下采样层。 if stride ! 1 or self.in_channels ! out_channels * block.expansion: downsample nn.Sequential( nn.Conv2d( self.in_channels, out_channels * block.expansion, kernel_size 1, stride stride, bias False ), nn.BatchNorm2d(out_channels * block.expansion), ) layers [] layers.append(block(self.in_channels, out_channels, stride, downsample)) self.in_channels out_channels * block.expansion for _ in range(1, blocks): layers.append(block(self.in_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): x self.conv1(x) x self.bn1(x) x self.relu(x) x self.maxpool(x) x self.layer1(x) x self.layer2(x) x self.layer3(x) x self.avgpool(x) x torch.flatten(x, 1) x self.fc(x) return x ``` </details> <details> <summary>基于残差块构建 ResNet 9</summary> ```python class ResNet18(nn.Module): \"\"\"ResNet18 model\"\"\" def __init__(self, num_classes 1000): super(ResNet18, self).__init__() self.in_channels 64 # 初始卷积层 self.conv1 nn.Conv2d(3, 64, kernel_size 7, stride 2, padding 3, bias False) self.bn1 nn.BatchNorm2d(64) self.relu nn.ReLU(inplace True) self.maxpool nn.MaxPool2d(kernel_size 3, stride 2, padding 1) # 残差块 self.layer1 self._make_layer(BasicBlock, 64, 2, stride 1) self.layer2 self._make_layer(BasicBlock, 128, 2, stride 2) self.layer3 self._make_layer(BasicBlock, 256, 2, stride 2) self.layer4 self._make_layer(BasicBlock, 512, 2, stride 2) # 全局平均池化和全连接层 self.avgpool nn.AdaptiveAvgPool2d((1, 1)) self.fc nn.Linear(512 * BasicBlock.expansion, num_classes) def _make_layer(self, block, out_channels, blocks, stride): downsample None if stride ! 1 or self.in_channels ! out_channels * block.expansion: downsample nn.Sequential( nn.Conv2d( self.in_channels, out_channels * block.expansion, kernel_size 1, stride stride, bias False ), nn.BatchNorm2d(out_channels * block.expansion), ) layers [] layers.append(block(self.in_channels, out_channels, stride, downsample)) self.in_channels out_channels * block.expansion for _ in range(1, blocks): layers.append(block(self.in_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): x self.conv1(x) x self.bn1(x) x self.relu(x) x self.maxpool(x) x self.layer1(x) x self.layer2(x) x self.layer3(x) x self.layer4(x) x self.avgpool(x) x torch.flatten(x, 1) x self.fc(x) return x ``` </details>"},"/StyleTransfer/ref_and_notes/metanet.html":{"title":"Meta Networks","content":" title: Meta Networks keywords: Meta Networks desc: Meta Networks date: 2025 03 19 id: MetaNets [Meta Networks for Neural Style Transfer](https://arxiv.org/abs/1709.04111) *Shen F , Yan S , Zeng G .Meta Networks for Neural Style Transfer[J]. 2017.DOI:10.48550/arXiv.1709.04111.* > In this paper we propose a new method to get the specified network parameters through one time feed forward propagation of the meta networks and explore the application to neural style transfer. Recent works on style transfer typically need to train image transformation networks for every new style, and the style is encoded in the network parameters by enormous iterations of stochastic gradient descent. To tackle these issues, we build a meta network which takes in the style image and produces a corresponding image transformations network directly. Compared with optimization based methods for every style, our meta networks can handle an arbitrary new style within 19 ms seconds on one modern GPU card. The fast image transformation network generated by our meta network is only 449KB, which is capable of real time executing on a mobile device. We also investigate the manifold of the style transfer networks by operating the hidden features from meta networks. Experiments have well validated the effectiveness of our method. Code and trained models has been released https://github.com/FalongShen/styletransfer. **摘要**：在本文中，我们提出了一种通过元网络的一次性前馈传播获得指定网络参数的新方法，并探讨了该方法在神经风格迁移中的应用。目前的风格迁移研究通常需要针对每一种新风格训练图像变换网络，并且通过大量的随机梯度下降迭代将风格编码到网络参数中。为了解决这些问题，我们**构建了一个元网络，该网络接收风格图像并直接产生相应的图像转换网络**。与针对每种风格的基于优化的方法相比，我们的元网络可以在一张现代 GPU 卡上花费 19 毫秒内处理任意新风格。我们的元网络生成的快速图像变换网络只有 449KB，能够在移动设备上实时执行。我们还通过操作元网络中的隐藏特征来研究风格转换网络的多样性。实验验证了该方法的有效性。代码和训练过的模型发布在：https://github.com/FalongShen/styletransfer。 文章贡献如下： 提供一个元网络来生成特定的网络，从而解决网络生成任务。**元网络在一次前馈传播中接受新风格图像并生成对应图像的图像变换网络**。 为风格迁移提供更显式的表示，使得纹理合成和纹理生成更自然。 与基于 SGD 的方法相比，元网络生成的网络在性能相似的情况下，速度提高了几个数量级。 为神经风格迁移算法提供了一个新的视角，这表明卷积神经网络可以应用于优化问题。 ## 超网络和元网络 超网络：一种为大型网络生成权值的小网络，生成过程相对静态。 HyperNEAT（Stanley， D’Ambrosio, and Gauci 2009）采用一组虚拟坐标来产生权重。 Ha 等人（Ha, Dai, and Le 2016）提出使用静态超网络为卷积神经网络生成权重，并使用动态超网络为循环网络生成权重，他们将超网络作为宽松的权重共享形式。 元网络：一种利用神经网络来生成其他网络参数的技术，生成目标网络方面更为全面和灵活。 Munkhdalai等人（Munkhdalai and Yu 2017）提出了一种通过快速参数化实现一次性分类的元网络，用于快速泛化。 元网络的工作（Mitchell, Thrun, and others 1993; Vilalta and Drissi 2002）采用了两级学习，分别是 跨任务执行的元级（meta level）模型的缓慢学习 和 在每个任务内执行的基本级（base level）模型的快速学习。 论文通过三个逐步递进的 Situation 构建元网络的数学基础，核心目标是从传统优化问题过渡到动态网络生成。令 $f(x)$ 和 $h(x)$ 为固定可微函数，$\\vert\\vert \\cdot \\vert\\vert$ 为正则化。考虑优化问题： $$ \\vert\\vert f(x) f(a) \\vert\\vert + \\lambda\\vert\\vert h(x) h(b) \\vert\\vert\\tag{1} $$ 有三种情况，取决于 $a$ 和 $b$ 是固定的还是可变的。 ### Situation 1: 固定 $a$ 和 $b$ 如果 $f(x)$ 和 $h(x)$ 是凸函数，式 (1) 是一个典型的关于 $x$ 的凸优化问题，我们可以直接用梯度下降法求出最优的 $x$。 $$ \\arg\\min_x \\vert\\vert f(x) f(a_0) \\vert\\vert + \\lambda\\vert\\vert h(x) h(b_0) \\vert\\vert\\tag{2} $$ 该情况下，定义为输入固定的内容 $a a_0$ 和固定的风格图像 $b b_0$。通过优化生成图像 $x$。上式中，$f(x)$ 表示内容感知函数，$h(x)$ 表示风格感知函数。 解决办法是通过梯度下降直接优化生成图像 $x$。 ### Situation 2: $a$ 可变，$b$ 固定 对于任意给定的 $a$，根据情况 1，存在一个对应的 $x$ 满足这个函数。也就是说，总是存在一个映射函数： $$ \\mathcal{N}: a \\vert\\rightarrow x\\tag{3} $$ 将映射函数表示为深度神经网络 $\\mathcal{N}(a;w)$，由 $w$ 参数化。现在考虑经验风险最小化（ERM）问题来寻找最优映射函数： $$ \\arg\\min_w \\sum_a \\vert\\vert f(x) f(a) \\vert\\vert + \\lambda\\vert\\vert h(x) h(b_0) \\vert\\vert\\tag{4} $$ 其中，$x \\mathcal{N}(a;w)$。然而，这种情况下难以优化 $\\mathcal{N}(\\cdot;w)$。函数 $f(\\cdot)$ 对于决定 SGD 是否可以逼近最优映射函数也很重要。 该情况下，定义为输入可变的内容 $a$ 和固定的风格图像 $b b_0$。希望找到一个映射函数 $\\mathcal{N}(a;w)$，直接生成风格化图像 $x \\mathcal{N}(a;w)$，最小化所有内容图像的期望损失。 解决办法是训练一个 CNN（$\\mathcal{N}(a;w)$），参数 $w$ 通过 SGD 优化，学习内容图像到风格化图像的映射。 将优化变量从图像 $x$ 转换为网络参数 $w$；网络 $\\mathcal{N}$ 隐含编码了风格信息，但仅适用于单一固定风格。 ### Situation 3: $a$ 和 $b$ 都可变 对于任意给定的 $b$，根据情况 2，存在一个映射函数，使得在给定 $a$ 的情况下找到接近最优的 $x$。假设这个最优映射函数由 $\\mathcal{N}(\\cdot;w)$ 中的 $w$ 参数化，可以通过 SGD 处理。在这种情况下，映射函数： $$ meta\\mathcal{N}: b\\vert\\rightarrow \\mathcal{N}(\\cdot;w)\\tag{5} $$ 与情况 2 类似，再次将此映射函数表示为由 $\\theta$ 参数化的深度神经网络 $meta\\mathcal{N}(b;\\theta)$。该网络将 $b$ 作为输入，产生一个近似式 (2) 的最优网络。与 SGD 的迭代求解不同，元网络只需要一个前馈传播来找到接近最优的网络。 为了优化网络中的 $\\theta$，考虑以下经验风险最小化（ERM）问题： $$ \\arg\\min_\\theta \\sum_b\\sum_a \\vert\\vert f(x) f(a) \\vert\\vert + \\lambda\\vert\\vert h(x) h(b) \\vert\\vert\\tag{6} $$ 其中，$x \\mathcal{N}(a;w)$，$w meta\\mathcal{N}(b;\\theta)$。对于每一个给定的 $b$，都存在一个最优的 $w$，因此训练阶段需要 SGD 的迭代来更新元网络参数 $\\theta$，以此为每个给定的 $b$ 产生一个合适的 $w$。 该情况下，定义为输入可变的内容 $a$ 和可变的风格图像 $b$。构建元网络 $meta\\mathcal{N}(b;\\theta)$，输入风格图像 $b$，生成对应的图像转换网络参数 $w$。 解决办法是通过预训练模型提取风格图像 $b$ 的特征，再使用全连接层将风格特征映射为图像转换网络的参数 $w$。训练上使用大量风格和内容图像联合优化元网络的参数 $\\theta$，使其泛化能力强。 将优化变量从网络参数 $w$ 转换为元网络参数 $\\theta$，实现从风格到网络参数的端到端映射。本质上，元网络 $Meta\\mathcal{N}$ 是一个高阶函数，学习如何生成适应不同风格的低阶函数（图像转换网络）。 ## 风格迁移 ### 固定内容图像的固定风格迁移 对于给定图像对 $I_s$ 和 $I_c$，目标是找到一个将 $I_s$ 的风格和 $I_c$ 的内容结合后感知损失函数最小的最优图像 $I$： $$ \\min_I\\left( \\lambda_c \\vert\\vert \\mathbf{CP}(I;w_f) \\mathbf{CP}(I_c;w_f) \\vert\\vert _2^2 + \\lambda_s \\vert\\vert \\mathbf{SP}(I;w_f) \\mathbf{SP}(I_s;w_f) \\vert\\vert _2^2 \\tag{7} \\right) $$ 其中， $\\mathbf{SP}(\\cdot;w_f)$ 和 $\\mathbf{CP}(\\cdot;w_f)$ 是基于预训练深度神经网络的感知函数，由固定权重 $w_f$ 参数化，分别代表风格感知器和内容感知器。 $\\lambda_s$ 和 $\\lambda_c$ 控制风格和内容的权重。 根据情况 1，将梯度下降应用于整个网络，并利用反向传播的梯度信息合成图像，最小化损失函数。这种方法为任何给定的风格图像生成高质量的结果，但需要数百次优化迭代来获得每个样本的收敛结果，这带来了很大的计算负担。 ### 任意内容图像的固定风格迁移 这个阶段的图像变换网络基于大型自然图像数据集，通过反向传播利用参数的梯度上进行优化: $$ \\min_w\\sum_{I_c}\\left( \\lambda_c \\vert\\vert \\mathbf{CP}(I_w;w_f) \\mathbf{CP}(I_c;w_f) \\vert\\vert _2^2 + \\lambda_s \\vert\\vert \\mathbf{SP}(I_w;w_f) \\mathbf{SP}(I_s;w_f) \\vert\\vert _2^2 \\tag{8} \\right) $$ 其中，$I_w \\mathcal{N}(I_c;w)$，$\\mathcal{N}$ 是由 $w$ 参数化的图像变换网络。 $I_s$ 的风格编码在 $w$ 中。对于一个新的内容图像，只需要通过变换网络进行前向传播来生成对应的图像。 ### 固定内容图像的任意风格迁移 如前所述情况 2，对于某些特定条件，存在一个由 CNN 参数化的直接映射来逼近损失的近最优解。 考虑对称性问题，对于固定内容图像，尝试找到每个样式图像的转移图像： $$ \\min_w\\sum_{I_s}\\left( \\lambda_c \\vert\\vert \\mathbf{CP}(I_w;w_f) \\mathbf{CP}(I_c;w_f) \\vert\\vert _2^2 + \\lambda_s \\vert\\vert \\mathbf{SP}(I_w;w_f) \\mathbf{SP}(I_s;w_f) \\vert\\vert _2^2 \\tag{9} \\right) $$ 其中，$I_w \\mathcal{N}(I_c;w)$。 但是，已经发现它无法找到正确的映射。图像变换网络只给出样式图像作为传输图像，这表明直接映射来接近梯度下降解并不是那么简单。 ### 任意内容图像的任意风格迁移 如情况 3 所示，通过下式得到转换网络 $\\mathcal{N}(\\cdot;w)$： $$ Meta\\mathcal{N}: I_s\\vert\\rightarrow \\mathcal{N}(I_s;w)\\tag{10} $$ 元网络由 $\\theta$ 参数化，且通过内容图像数据集和风格图像数据集进行优化： $$ \\min_{\\theta}\\sum_{I_c,I_s}\\left( \\lambda_c \\vert\\vert \\mathbf{CP}(I_{w_\\theta};w_f) \\mathbf{CP}(I_c;w_f) \\vert\\vert ^2_2 + \\lambda_s \\vert\\vert \\mathbf{SP}(I_{w_\\theta};w_f) \\mathbf{SP}(I_s;w_f) \\vert\\vert ^2_2 \\right)\\tag{11} $$ 其中，$I_{w_{\\theta}} \\mathcal{N}(I_c;w_{\\theta})$，$w_{\\theta} Meta\\mathcal{N}(I_s;\\theta)$。 风格图像 $I_s$ 作为损失函数中的监督目标，同时作为元网络的输入特征。 即元网络以风格图像为输入，生成一个能够将内容图像向风格图像传递的网络。 ## 网络结构 ### 图像转换网络结构 论文中的表 2： 网络层 输出形状 备注 : :: :: : 输入层 3 × 256 × 256 反射填充层 (40 × 40) 3 × 336 × 336 8 × 9 × 9 conv，stride 1 8 × 336 × 336 与 MetaNet 一同训练，在推理阶段是固定的 16 × 3 × 3 conv，stride 2 16 × 168 × 168 在推理阶段由元网络生成 32 × 3 × 3 conv，stride 2 32 × 84 × 84 在推理阶段由元网络生成 残差块，32 个卷积核 32 × 80 × 80 在推理阶段由元网络生成 残差块，32 个卷积核 32 × 76 × 76 在推理阶段由元网络生成 残差块，32 个卷积核 32 × 72 × 72 在推理阶段由元网络生成 残差块，32 个卷积核 32 × 68 × 68 在推理阶段由元网络生成 残差块，32 个卷积核 32 × 64 × 64 在推理阶段由元网络生成 16 × 3 × 3 conv，stride 2 16 × 128 × 128 在推理阶段由元网络生成 8 × 3 × 3 conv，stride 2 8 × 256 × 256 在推理阶段由元网络生成 3 × 9 × 9 conv，stride 1 3 × 256 × 256 与 MetaNet 一同训练，在推理阶段是固定的 *除了第一个和最后一个之外，每个 conv 层后面都有一个实例 batchnorm 层和一个 relu 层，为清楚起见，表中省略了这些层。* ### 整体模型结构 网络结构如下： ![](../static/images/Meta/fig1.png) #### 特征提取与损失计算 风格图像输入到预训练的 VGG16 模型中，提取出高层次风格特征，计算均值和标准差，拼接成风格特征向量。 对于内容损失计算：输入内容图像，取 VGG16 的 `relu3_3` 层的特征图作为内容特征，计算生成图像与内容图像在该层特征图的距离。 对于风格损失计算：输入风格图像，取 VGG16 的 `relu1_2`、`relu2_2`、`relu3_3`、`relu4_3` 层的输出作为风格特征，计算风格损失。 #### 参数生成（全连接层） 1. 风格特征通过两个全连接层，将高维的风格特征映射到生成图像转换网络（TransformNet）所需的各个卷积层滤波器参数上。 2. 每个卷积层的参数由独立的 128 维子向量生成，按照上述图像转换网络，生成的卷积层有 14 层，故总隐藏向量维度为 $14 \\times 128 1792$。 详见 [特征提取与 MetaNet 网络计算](#特征提取与 MetaNet 网络计算)。 #### 图像转换网络 依次为：反射填充、下采样卷积、残差块、上采样转置卷积。详见 [图像转换网络](#图像转换网络 TransformNet)。 ### 训练策略 神经风格迁移的元网络小批量随机梯度下降训练。在实验中使用 $k 20$ 和 $m 8$。 for epoch of epochs do 取样风格图像 $I_s$。 for k steps do 对元网络进行前馈传播，得到变换网络：$w \\leftarrow meta\\mathcal{N}(I_s;\\theta)$。 小批次取样 $m$ 个内容输入图像 $\\{I_c^{(1)}, ..., I_c^{(m)}\\}$。 对变换网络进行前馈传播，得到变换后的图像。 按式 (12) 计算内容损失和样式损失以及更新 $\\theta$ end for end for $$ \\nabla_{\\theta}\\sum_{I_c}\\left( \\lambda_c \\vert\\vert \\mathbf{CP}(I) \\mathbf{CP}(I_c) \\vert\\vert ^2_2 + \\lambda_s \\vert\\vert \\mathbf{SP}(I) \\mathbf{SP}(I_c) \\vert\\vert ^2_2 \\right)\\tag{12} $$ ## 结合代码分析网络结构 Reference: https://ypw.io/style transfer/ ### 卷积块构建 ConvLayer 构建包含反射填充、卷积、实例归一化和 ReLU 的卷积块。 **用于区分权值是可训练的参数还是由 MetaNet 生成的参数。** ```python def ConvLayer(in_channels, out_channels, kernel_size 3, stride 1, upsample None, instance_norm True, relu True, trainable False): layers [] if upsample: layers.append(nn.Upsample(mode 'nearest', scale_factor upsample)) layers.append(nn.ReflectionPad2d(kernel_size // 2)) # 填充以保持空间维度 if trainable: layers.append(nn.Conv2d( in_channels, out_channels, kernel_size, stride)) else: layers.append(MyConv2D( in_channels, out_channels, kernel_size, stride)) if instance_norm: layers.append(nn.InstanceNorm2d(out_channels)) if relu: layers.append(nn.ReLU(inplace True)) return layers ``` `trainable` 参数决定使用 `nn.Conv2d`（可训练）或 `MyConv2D`（参数由 MetaNet 生成）。 上采样采用最近邻插值。 ### 自定义卷积层 MyConv2D **将传统卷积层的权重替换为外部生成的参数，实现动态风格适配。** 权重和偏置初始化为零，固定不可训练。用于 TransformNet 中，其参数由 MetaNet 动态生成。 ```python class MyConv2D(nn.Module): def __init__(self, in_channels, out_channels, kernel_size 3, stride 1): super(MyConv2D, self).__init__() self.weight torch.zeros( out_channels, in_channels, kernel_size, kernel_size ).to(config[\"device\"]) self.bias torch.zeros(out_channels).to(config[\"device\"]) self.in_channels in_channels self.out_channels out_channels self.kernel_size (kernel_size, kernel_size) self.stride (stride, stride) def forward(self, x): return F.conv2d(x, self.weight, self.bias, self.stride) def extra_repr(self): s ('{in_channels}, {out_channels}, kernel_size {kernel_size}' ', stride {stride}') return s.format(**self.__dict__) ``` 当设置 `trainable False` 时，使用 `MyConv2D`。 ### 图像转换网络 TransformNet ```python class TransformNet(nn.Module): def __init__(self, base 8): super(TransformNet, self).__init__() self.base base self.weights [] self.downsampling nn.Sequential( *ConvLayer(3, base, kernel_size 9, trainable True), *ConvLayer(base, base * 2, kernel_size 3, stride 2), *ConvLayer(base * 2, base * 4, kernel_size 3, stride 2) ) self.residuals nn.Sequential(*[ResidualBlock(base*4) for _ in range(5)]) self.upsampling nn.Sequential( *ConvLayer(base * 4, base * 2, kernel_size 3, upsample 2), *ConvLayer(base * 2, base, kernel_size 3, upsample 2), *ConvLayer(base, 3, kernel_size 9, instance_norm False, relu False, trainable True) ) self.get_param_dict() def forward(self, x): y self.downsampling(x) y self.residuals(y) y self.upsampling(y) return y ``` 结构：`下采样 → 残差块 → 上采样` ```python def get_param_dict(self): param_dict defaultdict(int) def dfs(module, name): for _name, layer in module.named_children(): dfs(layer, '%s.%s' % (name, _name) if name ! '' else _name) if isinstance(module, MyConv2D): param_dict[name] + int(np.prod(module.weight.shape)) param_dict[name] + int(np.prod(module.bias.shape)) dfs(self, '') return param_dict ``` 深度优先搜索遍历网络，递归访问所有子模块：通过 `module.named_children()` 遍历每一层。 构建层级名称：例如，`downsampling.0` 表示 `self.downsampling` 中的第一个子模块。 如果是需要生成的参数的层，计算权值数量，累加到字典中。 ```python def set_my_attr(self, name, value): target self for x in name.split('.'): if x.isnumeric(): target target.__getitem__(int(x)) else: target getattr(target, x) n_weight np.prod(target.weight.shape) target.weight value[:n_weight].view(target.weight.shape) target.bias value[n_weight:].view(target.bias.shape) ``` 解析层级名称，如 `residuals.0.conv1`，表示第 1 个 残差块中的 `conv1` 层。 `value` 表示元网络生成的参数向量。 根据 `target.weight.shape` 累乘计算权重参数数量，截取前 `n_weight` 部分为权重，剩余部分为偏置。 ```python def set_weights(self, weights, i 0): for name, param in weights.items(): self.set_my_attr(name, weights[name][i]) ``` 输入 `weights`：由元网络生成的参数字典，键为层名（如 `downsampling.0`），值为参数张量。 遍历字典，对每个层名调用 `set_my_attr`，将参数填充到层。 `i 0` 表示支持批量处理时的索引（默认取第 0 个样本的参数）。 对该网络进行 `torchsummary` 的 `summary()` 分析总结： <table> <th>结构</th> <th>层</th> <th>Shape</th> <tr> <td rowspan \"4\">下采样 1</td> <td>ReflectionPad2d 1</td> <td>[ 1, 3, 264, 264]</td> </tr> <tr> <td>Conv2d 2</td> <td>[ 1, 32, 256, 256]</td> </tr> <tr> <td>InstanceNorm2d 3</td> <td>[ 1, 32, 256, 256]</td> </tr> <tr> <td>ReLU 4</td> <td>[ 1, 32, 256, 256]</td> </tr> <tr> <td colspan \"3\" align \"center\">......</td> </tr> <tr> <td rowspan \"7\">残差块 1</td> <td>ReflectionPad2d 13</td> <td>[ 1, 128, 66, 66]</td> </tr> <tr> <td>MyConv2D 14</td> <td>[ 1, 128, 64, 64]</td> </tr> <tr> <td>InstanceNorm2d 15</td> <td>[ 1, 128, 64, 64]</td> </tr> <tr> <td>ReLU 16</td> <td>[ 1, 128, 64, 64]</td> </tr> <tr> <td>ReflectionPad2d 17</td> <td>[ 1, 128, 66, 66]</td> </tr> <tr> <td>MyConv2D 18</td> <td>[ 1, 128, 64, 64]</td> </tr> <tr> <td>InstanceNorm2d 19</td> <td>[ 1, 128, 64, 64]</td> </tr> <tr> <td colspan \"3\" align \"center\">......</td> </tr> <tr> <td rowspan \"5\">上采样 1</td> <td>Upsample 53</td> <td>[ 1, 128, 128, 128]</td> </tr> <tr> <td>ReflectionPad2d 54</td> <td>[ 1, 128, 130, 130]</td> </tr> <tr> <td>MyConv2D 55</td> <td>[ 1, 64, 128, 128]</td> </tr> <tr> <td>InstanceNorm2d 56</td> <td>[ 1, 64, 128, 128]</td> </tr> <tr> <td>ReLU 57</td> <td>[ 1, 64, 128, 128]</td> </tr> <tr> <td colspan \"3\" align \"center\">......</td> </tr> </table> <details> <summary>原输出</summary> ``` Layer (type) Output Shape Param # ReflectionPad2d 1 [ 1, 3, 264, 264] 0 Conv2d 2 [ 1, 32, 256, 256] 7,808 InstanceNorm2d 3 [ 1, 32, 256, 256] 0 ReLU 4 [ 1, 32, 256, 256] 0 ReflectionPad2d 5 [ 1, 32, 258, 258] 0 MyConv2D 6 [ 1, 64, 128, 128] 18,496 InstanceNorm2d 7 [ 1, 64, 128, 128] 0 ReLU 8 [ 1, 64, 128, 128] 0 ReflectionPad2d 9 [ 1, 64, 130, 130] 0 MyConv2D 10 [ 1, 128, 64, 64] 73,856 InstanceNorm2d 11 [ 1, 128, 64, 64] 0 ReLU 12 [ 1, 128, 64, 64] 0 ReflectionPad2d 13 [ 1, 128, 66, 66] 0 MyConv2D 14 [ 1, 128, 64, 64] 147,584 InstanceNorm2d 15 [ 1, 128, 64, 64] 0 ReLU 16 [ 1, 128, 64, 64] 0 ReflectionPad2d 17 [ 1, 128, 66, 66] 0 MyConv2D 18 [ 1, 128, 64, 64] 147,584 InstanceNorm2d 19 [ 1, 128, 64, 64] 0 ResidualBlock 20 [ 1, 128, 64, 64] 0 ReflectionPad2d 21 [ 1, 128, 66, 66] 0 MyConv2D 22 [ 1, 128, 64, 64] 147,584 InstanceNorm2d 23 [ 1, 128, 64, 64] 0 ReLU 24 [ 1, 128, 64, 64] 0 ReflectionPad2d 25 [ 1, 128, 66, 66] 0 MyConv2D 26 [ 1, 128, 64, 64] 147,584 InstanceNorm2d 27 [ 1, 128, 64, 64] 0 ResidualBlock 28 [ 1, 128, 64, 64] 0 ReflectionPad2d 29 [ 1, 128, 66, 66] 0 MyConv2D 30 [ 1, 128, 64, 64] 147,584 InstanceNorm2d 31 [ 1, 128, 64, 64] 0 ReLU 32 [ 1, 128, 64, 64] 0 ReflectionPad2d 33 [ 1, 128, 66, 66] 0 MyConv2D 34 [ 1, 128, 64, 64] 147,584 InstanceNorm2d 35 [ 1, 128, 64, 64] 0 ResidualBlock 36 [ 1, 128, 64, 64] 0 ReflectionPad2d 37 [ 1, 128, 66, 66] 0 MyConv2D 38 [ 1, 128, 64, 64] 147,584 InstanceNorm2d 39 [ 1, 128, 64, 64] 0 ReLU 40 [ 1, 128, 64, 64] 0 ReflectionPad2d 41 [ 1, 128, 66, 66] 0 MyConv2D 42 [ 1, 128, 64, 64] 147,584 InstanceNorm2d 43 [ 1, 128, 64, 64] 0 ResidualBlock 44 [ 1, 128, 64, 64] 0 ReflectionPad2d 45 [ 1, 128, 66, 66] 0 MyConv2D 46 [ 1, 128, 64, 64] 147,584 InstanceNorm2d 47 [ 1, 128, 64, 64] 0 ReLU 48 [ 1, 128, 64, 64] 0 ReflectionPad2d 49 [ 1, 128, 66, 66] 0 MyConv2D 50 [ 1, 128, 64, 64] 147,584 InstanceNorm2d 51 [ 1, 128, 64, 64] 0 ResidualBlock 52 [ 1, 128, 64, 64] 0 Upsample 53 [ 1, 128, 128, 128] 0 ReflectionPad2d 54 [ 1, 128, 130, 130] 0 MyConv2D 55 [ 1, 64, 128, 128] 73,792 InstanceNorm2d 56 [ 1, 64, 128, 128] 0 ReLU 57 [ 1, 64, 128, 128] 0 Upsample 58 [ 1, 64, 256, 256] 0 ReflectionPad2d 59 [ 1, 64, 258, 258] 0 MyConv2D 60 [ 1, 32, 256, 256] 18,464 InstanceNorm2d 61 [ 1, 32, 256, 256] 0 ReLU 62 [ 1, 32, 256, 256] 0 ReflectionPad2d 63 [ 1, 32, 264, 264] 0 Conv2d 64 [ 1, 3, 256, 256] 7,779 Total params: 1,676,035 Trainable params: 15,587 Non trainable params: 1,660,448 Input size (MB): 0.75 Forward/backward pass size (MB): 460.16 Params size (MB): 6.39 Estimated Total Size (MB): 467.30 ``` </details> ### 特征提取与 MetaNet 网络计算 风格图像经过 VGG16 输出的 `relu1_2`、`relu2_2`、`relu3_3`、`relu4_3` 四层特征尺寸很大。 假设图像为 `(256, 256)`，那么 VGG16 输出的尺寸分别为 `(64, 256, 256)`、`(128, 128, 128)`、`(256, 64, 64)`、`(512, 32, 32)`。取其 Gram 矩阵，即 `(64, 64)`、`(128, 128)`、`(256, 256)`、`(512, 512)` 也是很大的。 假如使用 512 × 512 个特征去生成 147584 个权值（一个残差层），那么这层全连接层的权值就是 $512\\times 512\\times 147584 386888260096$，再假设权重类型为 `float32`，则仅是权值大小为 144GB，非常不现实。 考虑只计算每一个卷积核输出的内容均值和标准差。 这种情况下，特征变成 $(64 + 128 + 256 + 512) \\times 2 1920$ 维，但是，通过计算将图像转换网络的所有权重求和再乘上特征 $1920\\times (18496+73856+147584*10+73792+18464) 3188060160$，计算后大小也有 11.8GB，还是不现实。 论文中提到，计算风格图像和转移图像两个特征图的均值和标准差作为风格特征。 > The dimension of hidden vector is 1792 without specification. The hidden features are connected with the filters of each conv layer of the network in a group manner to decrease the parameter size, which means a 128 dimensional hidden vector for each conv layer. *隐向量的维数为 1792。将隐藏特征分组连接到网络各 conv 层的滤波器中，减小参数大小，即每个 conv 层有一个 128 维的隐藏向量。* 意思是全连接层使用 $14\\times 128 1792$ 个神经元，分别对应 2 层下采样层、10 层残差层和 2 层上采样层。 ```python def mean_std(features): mean_std_features [] for x in features: batch, C, H, W x.shape x_flat x.view(batch, C, 1) mean x_flat.mean(dim 1) std torch.sqrt(x_flat.var(dim 1) + 1e 5) feature torch.cat([mean, std], dim 1) mean_std_features.append(feature) return torch.cat(mean_std_features, dim 1) ``` 输入VGG16提取的多层特征，计算每层特征的均值和标准差，拼接为 1920 维向量。 捕捉风格图像的统计特征（类似 Gram 矩阵），用于风格损失计算。 MetaNet 网络代码： ```python class MetaNet(nn.Module): def __init__(self, param_dict): super(MetaNet, self).__init__() self.param_num len(param_dict) self.hidden nn.Linear(1920, 128 * self.param_num) self.fc_dict {} for i, (name, params) in enumerate(param_dict.items()): self.fc_dict[name] i setattr(self, 'fc{}'.format(i + 1), nn.Linear(128, params)) ``` `self.hidden nn.Linear(1920, 128 * self.param_num)`：对应风格图像通过 VGG 16 提取的多层特征均值和标准差拼接后的维度，为每个参数层分配一个 128 维的隐藏向量。 `self.fc_dict[name] i`：将参数层名称（如 \"`downsampling.0`\"）映射到索引 `i`。 `setattr(...)`：动态为每个参数层创建一个独立的线性层（`fc1`, `fc2`, ...），输入维度为 128，输出维度为该层所需的参数数量 `params`（如某卷积层的权重+偏置总数）。 ```python def forward(self, mean_std_features): hidden F.relu(self.hidden(mean_std_features)) filters {} for name, i in self.fc_dict.items(): fc getattr(self, 'fc{}'.format(i + 1)) filters[name] fc(hidden[:, i * 128 : (i + 1) * 128]) return filters ``` `hidden F.relu(self.hidden(mean_std_features))`：根据风格图像的均值和标准差，计算隐藏向量，维度为 `[batch_size, 1920]`，并引入激活函数。 `for` 循环遍历 `fc_dict` 生成参数。首先根据索引 `i` 获取对应的 `fc`。然后从 `hidden` 输出中切分出该层对应的 128 维片段，接着将这 128 维向量输入线性层生成该层所需的参数，最后将参数按层名存入字典。 对该网络进行 `print` 分析总结： <table> <th>结构</th> <th>层</th> <th>[in, out]</th> <tr> <td>隐藏层</td> <td>hidden</td> <td>[1920, 1792]</td> </tr> <tr> <td rowspan \"2\">映射至下采样层参数</td> <td>fc1</td> <td>[128, 18496]</td> </tr> <tr> <td>fc2</td> <td>[128, 73856]</td> </tr> <tr> <td rowspan \"10\">映射至残差块参数</td> <td>fc3</td> <td>[128, 147584]</td> </tr> <tr> <td>fc4</td> <td>[128, 147584]</td> </tr> <tr> <td>fc5</td> <td>[128, 147584]</td> </tr> <tr> <td>fc6</td> <td>[128, 147584]</td> </tr> <tr> <td>fc7</td> <td>[128, 147584]</td> </tr> <tr> <td>fc8</td> <td>[128, 147584]</td> </tr> <tr> <td>fc9</td> <td>[128, 147584]</td> </tr> <tr> <td>fc10</td> <td>[128, 147584]</td> </tr> <tr> <td>fc11</td> <td>[128, 147584]</td> </tr> <tr> <td>fc12</td> <td>[128, 147584]</td> </tr> <tr> <td rowspan \"2\">映射至上采样层参数</td> <td>fc13</td> <td>[128, 73792]</td> </tr> <tr> <td>fc14</td> <td>[128, 18464]</td> </tr> </table> <details> <summary>原输出</summary> `print(metanet)`： ``` MetaNet( (hidden): Linear(in_features 1920, out_features 1792, bias True) (fc1): Linear(in_features 128, out_features 18496, bias True) (fc2): Linear(in_features 128, out_features 73856, bias True) (fc3): Linear(in_features 128, out_features 147584, bias True) (fc4): Linear(in_features 128, out_features 147584, bias True) (fc5): Linear(in_features 128, out_features 147584, bias True) (fc6): Linear(in_features 128, out_features 147584, bias True) (fc7): Linear(in_features 128, out_features 147584, bias True) (fc8): Linear(in_features 128, out_features 147584, bias True) (fc9): Linear(in_features 128, out_features 147584, bias True) (fc10): Linear(in_features 128, out_features 147584, bias True) (fc11): Linear(in_features 128, out_features 147584, bias True) (fc12): Linear(in_features 128, out_features 147584, bias True) (fc13): Linear(in_features 128, out_features 73792, bias True) (fc14): Linear(in_features 128, out_features 18464, bias True) ) ``` `torchsummary.summary(metanet, (1920,))`: ``` Layer (type) Output Shape Param # Linear 1 [ 1, 1792] 3,442,432 Linear 2 [ 1, 18496] 2,385,984 Linear 3 [ 1, 73856] 9,527,424 Linear 4 [ 1, 147584] 19,038,336 Linear 5 [ 1, 147584] 19,038,336 Linear 6 [ 1, 147584] 19,038,336 Linear 7 [ 1, 147584] 19,038,336 Linear 8 [ 1, 147584] 19,038,336 Linear 9 [ 1, 147584] 19,038,336 Linear 10 [ 1, 147584] 19,038,336 Linear 11 [ 1, 147584] 19,038,336 Linear 12 [ 1, 147584] 19,038,336 Linear 13 [ 1, 147584] 19,038,336 Linear 14 [ 1, 73792] 9,519,168 Linear 15 [ 1, 18464] 2,381,856 Total params: 217,640,224 Trainable params: 217,640,224 Non trainable params: 0 Input size (MB): 0.01 Forward/backward pass size (MB): 12.68 Params size (MB): 830.23 Estimated Total Size (MB): 842.92 ``` </details> ## 论文实验结果 文献中主要对比了以下四类神经风格迁移方法： 1. Gatys 等人的梯度下降优化方法（2015） 2. Johnson 等人的逐风格训练方法（2016） 3. AdaIN（自适应实例归一化）方法（Huang & Belongie, 2017） 4. 本文提出的元网络方法 对比指标与结果： 方法 编码时间 图像转换时间 模型大小 支持风格数 核心优势与限制 : :: :: :: :: :: : Gatys (梯度下降) N/A 9.52s N/A ∞ (任意风格) 灵活但速度极慢，无法实时应用 Johnson (逐风格训练) 4小时/风格 15ms 7MB/风格 1 单风格快速生成，但需逐风格训练 AdaIN (统计量对齐) 27ms 18ms 25MB ∞ 支持多风格，但依赖VGG编码器，模型较大 元网络 19ms 15ms 449KB ∞ 快速生成任意风格，模型轻量化，无需重训练 ## 代码 Demo 结果 复现代码：[Github/01.ref_and_note/06.MetaNets](https://github.com/Fingsinz/StyleTransfer/blob/main/src/01.ref_and_note/06.MetaNets.py) <table> <th>示例 1</th> <th>示例 2</th> <tr> <td><img src \"../static/images/Meta/fig2.png\" /></td> <td><img src \"../static/images/Meta/fig3.png\" /></td> </tr> </table>"},"/StyleTransfer/ref_and_notes/msgnet.html":{"title":"Multi-style Generative Network for Real-time Transfer","content":" title: Multi style Generative Network for Real time Transfer keywords: msgnet desc: Multi style Generative Network for Real time Transfer date: 2025 04 09 id: msgnet [Multi style Generative Network for Real time Transfer](https://arxiv.org/abs/1703.06953) *ZHANG H, DANA K. Multi style Generative Network for Real time Transfer[M/OL]//Lecture Notes in Computer Science,Computer Vision – ECCV 2018 Workshops. 2019: 349 365. http://dx.doi.org/10.1007/978 3 030 11018 5_32. DOI:10.1007/978 3 030 11018 5_32.* > Despite the rapid progress in style transfer, existing approaches using feed forward generative network for multi style or arbitrary style transfer are usually compromised of image quality and model flexibility. We find it is fundamentally difficult to achieve comprehensive style modeling using 1 dimensional style embedding. Motivated by this, we introduce CoMatch Layer that learns to match the second order feature statistics with the target styles. With the CoMatch Layer, we build a Multi style Generative Network (MSG Net), which achieves real time performance. We also employ an specific strategy of upsampled convolution which avoids checkerboard artifacts caused by fractionally strided convolution. Our method has achieved superior image quality comparing to state of the art approaches. The proposed MSG Net as a general approach for real time style transfer is compatible with most existing techniques including content style interpolation, color preserving, spatial control and brush stroke size control. MSG Net is the first to achieve real time brush size control in a purely feed forward manner for style transfer. Our implementations and pre trained models for Torch, PyTorch and MXNet frameworks will be publicly available. **摘要**：尽管风格迁移的研究进展迅速，但现有的使用前馈生成网络进行多风格或任意风格迁移的方法通常会损害图像质量和模型灵活性。我们发现使用一维样式嵌入实现全面的样式建模从根本上是困难的。受此启发，我们引入了 CoMatch Layer，该层学习将二阶特征统计与目标风格进行匹配。利用 CoMatch 层构建了多风格生成网络（MSG Net），实现了实时性能。我们还采用了一种特殊的上采样卷积策略，避免了由分数阶卷积引起的棋盘伪影。与最先进的方法相比，我们的方法取得了卓越的图像质量。所提出的 MSG Net 作为实时风格转换的通用方法，与大多数现有技术兼容，包括内容风格插值、色彩保持、空间控制和笔触大小控制。MSG Net 是第一个以纯粹的前馈方式实现实时笔刷大小控制的风格转移。我们对 Torch、PyTorch 和 MXNet 框架的实现和预训练模型将公开可用。 ## 主要内容 ### CoMatch 层 通过匹配目标风格的二阶特征统计量（ Gram 矩阵），取代传统的一维风格嵌入（均值和方差），更全面地捕捉风格特征。 解决了因一维嵌入导致的风格表达能力不足的问题，显著提升了生成图像的质量。 ### MSG Net架构 上采样卷积：采用整数步长卷积结合上采样操作，替代传统的反卷积（分数步长卷积），避免了棋盘格伪影。 上采样残差块：扩展残差网络结构，支持恒等映射，加速收敛并提升深层网络的稳定性。 实时笔触控制：通过动态调整风格图像的分辨率（训练时随机采样尺寸，推理时用户自定义），实现笔触大小的实时调整。 ## 关键技术细节 ### 训练策略 1. 使用预训练的 VGG 网络作为损失网络，通过联合优化内容损失（特征匹配）和风格损失（Gram 矩阵匹配）。 2. 风格图像在训练时动态调整尺寸（256、512、768），使模型适应不同笔触大小。 ### 网络结构 编码器 解码器框架：Siamese 网络提取风格特征，生成网络通过 CoMatch 层融合内容与风格信息。 反射填充：减少边界伪影。 实例归一化：提升生成图像对比度鲁棒性。 ## 实现 PyTorch Github：https://github.com/zhanghang1989/PyTorch Multi Style Transfer"},"/StyleTransfer/ref_and_notes/patchgan.html":{"title":"PatchGAN 到 Multi-Scale PatchGAN","content":" title: PatchGAN 到 Multi Scale PatchGAN keywords: PatchGAN desc: PatchGAN 到 Multi Scale PatchGAN date: 2025 03 11 id: PatchGAN [Image to Image Translation with Conditional Adversarial Networks](https://ieeexplore.ieee.org/document/8100115) *Isola P , Zhu J Y , Zhou T ,et al.Image to Image Translation with Conditional Adversarial Networks[C]//IEEE Conference on Computer Vision & Pattern Recognition.IEEE, 2016.DOI:10.1109/CVPR.2017.632.* [High Resolution Image Synthesis and Semantic Manipulation with Conditional GANs](https://arxiv.org/abs/1711.11585) *Wang T C , Liu M Y , Zhu J Y ,et al.High Resolution Image Synthesis and Semantic Manipulation with Conditional GANs[J]. 2017.DOI:10.48550/arXiv.1711.11585.* ## 基础 PatchGAN *来自《Image to Image Translation with Conditional Adversarial Networks》（Phillip Isola 等，CVPR 2017，即 pix2pix）* ### PatchGAN 原理 PatchGAN 的判别器不再对整张图像进行全局真伪判断，而是**将输入图像分割为多个局部小块（Patch），每个 Patch 独立判断真伪，最终取所有 Patch 的平均结果作为整体判别输出**。 判别器通过多层卷积操作，逐步下采样输入图像，最终输出一个 **N×N 的特征图**。 每个特征图上的点对应输入图像的一个局部块，用于判断该局部区域是否真实。 ### 卷积参数 卷积参数： 卷积核大小：4 × 4，步长（stride） 2，填充（padding） 1。 归一化层：使用 `BatchNorm2d` 或 `InstanceNorm2d`（根据输入条件）。 激活函数：LeakyReLU（负斜率为 0.2）。 输出层：最后一层卷积输出 1 通道的特征图，通过 Sigmoid 函数得到每个局部块的真假概率。 ### PatchGAN 感受野 每层卷积的感受野大小可通过公式递推计算： $$ R_{n} R_{n 1} + (k_n 1) \\times \\prod_{i 1}^{n 1} s_i $$ 其中 $k_n$ 为第 $n$ 层卷积核大小，$s_i$ 为第 $i$ 层步长。 以一个简单的三层卷积网络为例，来展示如何计算每个输出单元对应的输入图像的局部感受野大小。假设网络参数如下： > 输入的单个像素的感受野 $R_0 1$。 > > 第一层卷积：卷积核大小 $k_1 4$，步长 $s_1 2$。 > $R_1 R_0 + (k_1 1) \\times \\prod_{i 1}^{0} s_i 1+(4 1)\\times 1 1 + 3 4$。 > 这表示第一层中，每个神经元对应输入图像上的 $4 \\times 4$ 区域。 > > 第二层卷积：卷积核大小 $k_2 4 $，步长 $s_2 2$，前一层步长的累计乘积为 $s_1 2$。 > $R_2 R_1 + (k_2 1) \\times s_1 4 + 3 \\times 2 4 + 6 10$ > 说明第二层的每个神经元对应输入图像上的 $10 \\times 10$ 区域。 > > 第三层卷积：卷积核大小 $k_3 4$，步长 $s_3 1$，前两层步长的累计乘积为 $s_1 \\times s_2 2 \\times 2 4$。 > $R_3 R_2 + (k_3 1) \\times (s_1 \\times s_2) 10 + 3 \\times 4 10 + 12 22$ > 说明第三层中，每个神经元的感受野大小为 $22 \\times 22$ 像素。 这个例子说明，通过逐层计算，最终网络的最后一层中的每个判别器输出单元实际上“看到了”输入图像上的一个 $22 \\times 22$ 的局部区域。PatchGAN 的设计思想就是利用这样的局部感受野，使得判别器能够专注于图像中细小局部区域的真实感，从而更有效地区分真实图像和生成图像。 ### PatchGAN 感受野计算原理 局部连接与覆盖范围： 每个卷积核具有固定大小 $k$，它在输入图像上滑动时，每次都会覆盖 $k$ 个像素（在一维情况中，二维类似）。 第一个卷积层中，每个神经元直接看到大小为 $k$ 的区域。如果我们把输入的单个像素的感受野记为 $R_0 1$，那么第一层每个神经元的感受野就是：$R_1 1 + (k_1 1)$。这里的 $(k_1 1)$ 代表了额外扩展的部分（因为中心像素已经在 $R_0$ 内）。 步长的影响： 当卷积层使用步长 $s$ 时，输出特征图的每个神经元在输入上移动的间隔不再是 1，而是 $s$。这意味着，后续层的卷积操作相当于在上一层感受野的基础上，以 $s$ 的“倍率”去扩展。 例如，第二层卷积时，假设核大小为 $k_2$ 和步长 $s_2$，它增加的感受野大小不是直接的 $k_2 1$，而是乘上了上一层的步长 $s_1$（因为上一层的每个步长移动在原始输入上代表了 $s_1$ 个像素）：$R_2 R_1 + (k_2 1) \\times s_1$ 层层叠加： 当网络层数增多时，每一层的卷积操作都在前一层的基础上进一步扩展感受野，而且这种扩展会受前面所有层步长的累计影响。 所以对于第 $l$ 层，用前面所有层的步长乘积来缩放当前层的扩展量：$R_l R_{l 1} + (k_l 1) \\times \\prod_{i 1}^{l 1} s_i$，其中： $R_{l 1}$ 是上一层的感受野； $(k_l 1)$ 是当前层除中心外能“看到”的额外像素数量； $\\prod_{i 1}^{l 1} s_i$ 则是前面所有层步长的累乘，表明每一层的位移在原始输入上对应的像素数。 直观地说，每一层卷积操作会在原有感受野的基础上“增加”一圈额外的像素，这一圈的宽度为 $k 1$（不考虑步长时），而实际增加的像素数量会因为前面层的步长而被放大。于是就得到了上述递归公式，用来精确计算任意层的感受野。 ### 论文中的 PatchGAN [@NLayerDiscriminator](https://github.com/junyanz/pytorch CycleGAN and pix2pix/blob/master/models/networks.py#L539) ```python class NLayerDiscriminator(nn.Module): \"\"\"Defines a PatchGAN discriminator\"\"\" def __init__(self, input_nc, ndf 64, n_layers 3, norm_layer nn.BatchNorm2d): \"\"\" Parameters: input_nc (int) 输入图像的通道数 ndf (int) 最后一个转换层中卷积核的数量 n_layers (int) 判别器中的转换层数 norm_layer 归一化层 \"\"\" super(NLayerDiscriminator, self).__init__() if type(norm_layer) functools.partial: # 不需要使用偏差，因为BatchNorm2d有仿射参数 use_bias norm_layer.func nn.InstanceNorm2d else: use_bias norm_layer nn.InstanceNorm2d kw 4 padw 1 sequence [nn.Conv2d(input_nc, ndf, kernel_size kw, stride 2, padding padw), nn.LeakyReLU(0.2, True)] nf_mult 1 nf_mult_prev 1 for n in range(1, n_layers): # 逐渐增加过滤器的数量 nf_mult_prev nf_mult nf_mult min(2 ** n, 8) sequence + [ nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size kw, stride 2, padding padw, bias use_bias), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True) ] nf_mult_prev nf_mult nf_mult min(2 ** n_layers, 8) sequence + [ nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size kw, stride 1, padding padw, bias use_bias), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True) ] sequence + [nn.Conv2d(ndf * nf_mult, 1, kernel_size kw, stride 1, padding padw)] # 输出 1 通道预测图 self.model nn.Sequential(*sequence) def forward(self, input): \"\"\"Standard forward.\"\"\" return self.model(input) ``` 这个 PatchGAN 判别器总共包含 5 个卷积层，其参数分别为： *padding 1* 层级 卷积核大小 步长 通道 效果 : :: :: :: :: : 第一层 4 2 input_nc → ndf 下采样 第二层（第一次迭代） 4 2 ndf → 2 * ndf 下采样 第三层（第二次迭代） 4 2 2 * ndf → 4 * ndf 下采样 第四层（循环结束后） 4 1 4 * ndf → 8 * ndf 尺寸不变 第五层 4 1 8 * ndf → 1 每个位置对应一个局部区域的真实性概率 假设输入图像为 256×256，那么按照代码中的卷积参数（核尺寸 4，padding 1）以及各层的步幅，特征图在每一层的尺寸变化如下： > 1. 第一层卷积（Conv2d(input_nc, 64, kernel_size 4, stride 2, padding 1)），输入尺寸 256×256，输出特征图大小为 128×128，通道数为 64。 > > $$ > \\text{output size} \\left\\lfloor\\frac{256+2\\times1 4}{2}\\right\\rfloor + 1 128 > $$ > > 2. 第二层卷积（第一个 for 循环迭代 n 1，对应 Conv2d(64, 128, kernel_size 4, stride 2, padding 1)），输入尺寸 128×128，输出大小 64×64，通道数由 64 增加到 128。 > > $$ > \\text{output size} \\left\\lfloor\\frac{128+2 4}{2}\\right\\rfloor + 1 64 > $$ > > 3. 第三层卷积（for 循环中 n 2，对应 Conv2d(128, 256, kernel_size 4, stride 2, padding 1)），输入尺寸 64×64，输出大小 32×32，通道数由 128 增加到 256。 > > $$ > \\text{output size} \\left\\lfloor\\frac{64+2 4}{2}\\right\\rfloor + 1 32 > $$ > > 4. 第四层卷积（循环外的第一层卷积，Conv2d(256, 512, kernel_size 4, stride 1, padding 1)），此层不再下采样，输入尺寸 32×32，输出大小为 31×31，通道数从 256 变为 512。 > > $$ > \\text{output size} \\left\\lfloor\\frac{32+2 4}{1}\\right\\rfloor + 1 31 > $$ > > 5. 第五层卷积（最后一层，Conv2d(512, 1, kernel_size 4, stride 1, padding 1)），输入尺寸 31×31，最终输出特征图大小为 30×30，且通道数为 1，对应每个位置的 Patch 判别结果。 > > $$ > \\text{output size} \\left\\lfloor\\frac{31+2 4}{1}\\right\\rfloor + 1 30 > $$ 示意图如下（输入 `[256, 256, 3]`）： ![](../static/images/PatchGAN/fig1.png) 感受野逐层计算如下： $$ R_{n} R_{n 1} + (k_n 1) \\times \\prod_{i 1}^{n 1} s_i $$ 层级 参数 跳跃（前面各层步长的乘积） 感受野 : :: :: :: : 1 $k_1 4$，$s_1 2$ $j_0 1$ $R_1 1 + (4 1) \\times 1 1 + 3 4$ 2 $k_2 4$，$s_2 2$ $j_1 j_0 \\times s_1$ $ 1 \\times 2 2$ $R_2 R_1 + (4 1) \\times j_1$ $ 4 + 3 \\times 2 4 + 6 10$ 3 $k_3 4$，$s_3 2$ $j_2 j_1 \\times s_2$ $ 2 \\times 2 4$ $R_3 R_2 + (4 1) \\times j_2$ $ 10 + 3 \\times 4 10 + 12 22$ 4 $k_4 4$，$s_4 1$ $j_3 j_2 \\times s_3$ $ 4 \\times 2 8$ $R_4 R_3 + (4 1) \\times j_3$ $ 22 + 3 \\times 8 22 + 24 46$ 5 $k_5 4$，$s_5 1$ $j_4 j_3 \\times s_4$ $ 8 \\times 1 8$ $R_5 R_4 + (4 1) \\times j_4$ $ 46 + 3 \\times 8 46 + 24 70$ 因此，最终判别器输出的每个单元的感受野为 70×70 像素。 ## 3 级金字塔判别器（Multi Scale PatchGAN） *来自《High Resolution Image Synthesis and Semantic Manipulation with Conditional GANs》（Ting Chun Wang 等，CVPR 2018，即 pix2pixHD）* 解决了两个主要问题： 1. 使用 GAN 生成高分辨率图像的困难； 2. 先前高分辨率结果中缺乏细节和逼真的纹理。 ### Coarse to Fine Generator *《High Resolution Image Synthesis and Semantic Manipulation with cGANs》生成器部分* 将生成器分解为两个子网：$G_1$ 和 $G_2$。生成器 $G \\{G_1, G_2\\}$。 $G_1$ 为全局生成器网络； $G_2$ 为局部增强器网络。 全局生成器网络以 1024 × 512 的分辨率运行，局部增强器网络输出的图像分辨率是前一个图像输出大小的 4 倍（每个图像维度，长、宽为 2 倍）。 例如，生成器 $G \\{G_1, G_2\\}$ 的输出图像分辨率为 2048 × 1024，$G \\{G_1, G_2, G_3\\}$ 的输出图像分辨率为 4096 × 2048。 ![](../static/images/PatchGAN/fig2.png) 全局生成器由 **一个前端卷积 $G_1^{(F)}$**、**一组残差块 $G_1^{(R)}$** 和 **一个后端转置卷积 $G_1^{(B)}$** 组成。 分辨率为 1024 × 512 的语义标签图依次通过上面 3 个组件，以输出分辨率为 1024 × 512 的图像。 局部增强器由 **一个前端卷积 $G_2^{(F)}$**、**一组残差块 $G_2^{(R)}$** 和 **一个后端转置卷积 $G_2^{(B)}$** 组成。 $G_2$ 的输入标签映射的分辨率为 2048 × 1024。 残差块 $G_2^{(R)}$ 的输入是 $G_2^{(F)}$ 的输出特征图和全局生成器网络 $G_1^{(B)}$ 后端的最后一个特征图的总和。这有助于将全局信息从 $G_1$ 集成到 $G_2$ 中。 在训练过程中，首先训练全局生成器，接着按照它们的分辨率顺序训练局部增强器，然后一起微调所有的网络。 #### 全局生成器代码 [pix2pixHD/models/networks.py](https://github.com/NVIDIA/pix2pixHD/blob/master/models/networks.py#L183) ```python class GlobalGenerator(nn.Module): def __init__(self, input_nc, output_nc, ngf 64, n_downsampling 3, n_blocks 9, norm_layer nn.BatchNorm2d, padding_type 'reflect'): ``` > `input_nc`：输入通道数（如语义标签图的通道数）。 > `output_nc`：输出通道数（如生成图像的 RGB 通道数 3）。 > `ngf`：生成器的基础特征通道数（默认 64）。 > `n_downsampling`：下采样次数（默认 3 次，特征图尺寸缩小为输入的 1/8）。 > `n_blocks`：残差块数量（默认 9 个）。 > `norm_layer`：归一化层（默认批量归一化 BatchNorm2d）。 > `padding_type`：填充类型（默认反射填充 ReflectionPad2d，减少边缘伪影）。 ```python activation nn.ReLU(True) model [ nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size 7, padding 0), norm_layer(ngf), activation ] ``` > 反射填充和卷积层提取初始特征：`(input_nc, H, W) > (ngf, H, W)`。 ```python for i in range(n_downsampling): mult 2**i model + [ nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size 3, stride 2, padding 1), norm_layer(ngf * mult * 2), activation ] ``` > 下采样：每次通道数翻倍，特征图尺寸缩小为 1/2。 ```python mult 2**n_downsampling for i in range(n_blocks): model + [ ResnetBlock(ngf * mult, padding_type padding_type, activation activation, norm_layer norm_layer) ] ``` > 残差块，保持特征图尺寸不变，通过跳跃连接缓解梯度消失。 > 通道数固定为 `ngf * 2^n_downsampling`。 ```python for i in range(n_downsampling): mult 2**(n_downsampling i) model + [ nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size 3, stride 2, padding 1, output_padding 1), norm_layer(int(ngf * mult / 2)), activation ] ``` > 上采样：逐步恢复空间分辨率并减少通道数。 > 每次通道数减半，特征图尺寸增加为 2 倍 ```python model + [ nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size 7, padding 0), nn.Tanh() ] self.model nn.Sequential(*model) ``` > 反射填充和卷积层将通道数映射到 output_nc，并使用 Tanh 激活 #### 局部增强器 [pix2pixHD/models/networks.py](https://github.com/NVIDIA/pix2pixHD/blob/master/models/networks.py#L129) ```python class LocalEnhancer(nn.Module): def __init__(self, input_nc, output_nc, ngf 32, n_downsample_global 3, n_blocks_global 9, n_local_enhancers 1, n_blocks_local 3, norm_layer nn.BatchNorm2d, padding_type 'reflect'): super(LocalEnhancer, self).__init__() self.n_local_enhancers n_local_enhancers ``` > `input_nc`：输入通道数（如语义标签图的通道数）。 > `output_nc`：输出通道数（如RGB图像的3通道）。 > `ngf`：基础特征通道数（默认 32）。 > `n_downsample_global`：全局生成器的下采样次数（默认 3 次）。 > `n_blocks_global`：全局生成器的残差块数量（默认 9 个）。 > `n_local_enhancers`：局部增强器的数量（默认 1 个）。 > `n_blocks_local`：每个局部增强器的残差块数量（默认 3 个）。 > `norm_layer`：归一化层（默认批量归一化 BatchNorm2d）。 > `padding_type`：填充类型（默认反射填充 ReflectionPad2d）。 ```python # 全局生成器模型 ngf_global ngf * (2**n_local_enhancers) model_global GlobalGenerator(input_nc, output_nc, ngf_global, n_downsample_global, n_blocks_global, norm_layer).model # 去掉最后的卷积层 model_global [model_global[i] for i in range(len(model_global) 3)] self.model nn.Sequential(*model_global) ``` > 生成低分辨率基础图象（如 1024 * 512）。 > 截断全局生成器的最后 3 层（如最终卷积层），保留编码器和残差块部分。 > 特征通道：`ngf_global ngf * 2^n_local_enhancers`（如 `n_local_enhancers 1` 时，`ngf_global 64`）。 ```python # 局部增强层 for n in range(1, n_local_enhancers+1): # 下采样分支 ngf_global ngf * (2**(n_local_enhancers n)) model_downsample [ # 1 nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf_global, kernel_size 7, padding 0), norm_layer(ngf_global), nn.ReLU(True), # 2 nn.Conv2d(ngf_global, ngf_global * 2, kernel_size 3, stride 2, padding 1), norm_layer(ngf_global * 2), nn.ReLU(True) ] ``` > 1. 反射填充 → 卷积（7×7，步长 1） → 归一化 → ReLU。 > 2. 卷积（3×3，步长 2） → 归一化 → ReLU（通道数翻倍）。 ```python # 残差块 model_upsample [] for i in range(n_blocks_local): model_upsample + [ ResnetBlock(ngf_global * 2, padding_type padding_type, norm_layer norm_layer) ] ``` > `n_blocks_local` 个残差块（默认3个），保持特征图尺寸不变。 ```python # 上采样分支 model_upsample + [ nn.ConvTranspose2d(ngf_global * 2, ngf_global, kernel_size 3, stride 2, padding 1, output_padding 1), norm_layer(ngf_global), nn.ReLU(True) ] ``` > 转置卷积（3×3，步长2） → 归一化 → ReLU（通道数减半）。 ```python # 最终输出层 if n n_local_enhancers: model_upsample + [ nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size 7, padding 0), nn.Tanh() ] ``` > 反射填充 → 卷积（7×7） → Tanh 激活函数。 ```python setattr(self, 'model'+str(n)+'_1', nn.Sequential(*model_downsample)) setattr(self, 'model'+str(n)+'_2', nn.Sequential(*model_upsample)) self.downsample nn.AvgPool2d(3, stride 2, padding [1, 1], count_include_pad False) ``` > 使用平均池化（AvgPool2d）逐步下采样输入，生成多尺度输入金字塔。 > 例如，若 `n_local_enhancers 2`，则输入金字塔包含原始输入、1/2 分辨率、1/4 分辨率。 ```python def forward(self, input): # 构建输入金字塔 input_downsampled [input] for i in range(self.n_local_enhancers): input_downsampled.append(self.downsample(input_downsampled[ 1])) ``` > 原始输入逐步下采样，生成 `n_local_enhancers+1` 个不同尺度的输入。 ```python # output at coarest level output_prev self.model(input_downsampled[ 1]) ``` > 将输入金字塔中最低分辨率的图像输入全局生成器，得到基础特征。 ```python for n_local_enhancers in range(1, self.n_local_enhancers+1): model_downsample getattr(self, 'model'+str(n_local_enhancers)+'_1') model_upsample getattr(self, 'model'+str(n_local_enhancers)+'_2') input_i input_downsampled[self.n_local_enhancers n_local_enhancers] output_prev model_upsample(model_downsample(input_i) + output_prev) return output_prev ``` > 从最粗糙尺度开始，依次通过下采样分支提取局部特征。 > 将局部特征与上一层的输出相加（特征融合），再通过残差块和上采样。 > 最后一层输出最终高分辨率图像（如 2048×1024）。 ### Multi Scale PatchGAN *《High Resolution Image Synthesis and Semantic Manipulation with cGANs》判别器部分* 为了区分高分辨率的真实图像和合成图像，鉴别器需要有一个大的接受野。 通过图像金字塔构建多尺度判别器，包含 **3 个不同分辨率的判别器**，它们具有相同的网络结构： 1. 原始分辨率图像（如 1024×1024）。 2. 下采样 1/2 的图像（如 512×512）。 3. 下采样 1/4 的图像（如 256×256）。 每个判别器均为 PatchGAN 结构，但感受野随分辨率变化，分别捕捉 **局部细节**（高分辨率）、**中等结构**（中分辨率）和 **全局布局**（低分辨率）。 最粗糙的尺度上工作的鉴别器具有最大的接受域。它具有更全局的图像视图，可以引导生成器生成全局一致的图像。 最精细的尺度上的鉴别器鼓励生成器产生更精细的细节。 这也使得训练从粗到精的生成器更容易，因为将低分辨率模型扩展到更高分辨率只需要在最精细的级别添加鉴别器，而不是从头开始重新训练。在没有多尺度鉴别器的情况下，我们观察到生成的图像中经常出现许多重复的图案。 因此，原来的最大最小博弈变成： $$ \\min_G \\max_{D_1,D_2,D_3}\\sum_{k 1,2,3}\\mathcal{L}_{GAN}(G, D_k) $$ #### 判别器代码 ```python class MultiscaleDiscriminator(nn.Module): def __init__(self, input_nc, ndf 64, n_layers 3, norm_layer nn.BatchNorm2d, use_sigmoid False, num_D 3, getIntermFeat False): super(MultiscaleDiscriminator, self).__init__() self.num_D num_D self.n_layers n_layers self.getIntermFeat getIntermFeat ``` > `input_nc`：输入图像的通道数（如 RGB 图像为 3）。 > `ndf`：基础特征通道数（默认 64）。 > `n_layers`：每个判别器的卷积层数（默认 3）。 > `norm_layer`：归一化层（默认批量归一化 BatchNorm2d）。 > `use_sigmoid`：是否在输出层使用 Sigmoid 激活函数（默认否）。 > `num_D`：判别器的数量（默认 3 个，对应 3 个不同尺度）。 > `getIntermFeat`：是否返回中间层特征（默认否，仅返回最终输出）。 ```python for i in range(num_D): netD NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat) if getIntermFeat: for j in range(n_layers+2): setattr(self, 'scale'+str(i)+'_layer'+str(j), getattr(netD, 'model'+str(j))) else: setattr(self, 'layer'+str(i), netD.model) self.downsample nn.AvgPool2d(3, stride 2, padding [1, 1], count_include_pad False) ``` > 1. 构建多尺度判别器：循环创建 `num_D` 个判别器（每个对应一个尺度） > 每个判别器是 `NLayerDiscriminator`（PatchGAN 的判别器） 的实例，其核心结构为多层卷积。 > 若 `getIntermFeat True`：将每个判别器的每一层单独注册为属性（如 `scale0_layer0`，`scale0_layer1`），以便提取中间特征。 > 若 `getIntermFeat False`：直接将整个判别器模型注册为属性（如 `layer0`，`layer1`）。 > 2. 下采样：使用平均池化（AvgPool2d）对输入图像进行下采样，生成多尺度输入金字塔。池化核大小 3×3，步长 2，padding 1，逐步将图像分辨率减半。 ```python def singleD_forward(self, model, input): if self.getIntermFeat: result [input] for i in range(len(model)): result.append(model[i](result[ 1])) return result[1:] else: return [model(input)] def forward(self, input): num_D self.num_D result [] input_downsampled input for i in range(num_D): if self.getIntermFeat: model [getattr(self,'scale'+str(num_D 1 i)+'_layer'+str(j)) for j in range(self.n_layers+2)] else: model getattr(self, 'layer'+str(num_D 1 i)) result.append(self.singleD_forward(model, input_downsampled)) if i ! (num_D 1): input_downsampled self.downsample(input_downsampled) return result ``` > 原始输入图像通过逐层下采样生成 `num_D` 个不同尺度的图像。例如，若 `num_D 3` ，则生成原始分辨率、1/2 分辨率、1/4 分辨率三个尺度。 > 最粗糙尺度（最低分辨率）开始，依次用对应尺度的判别器处理输入。 > 若 `getIntermFeat True`：提取每个判别器各层的中间特征。 > 若 `getIntermFeat False`：仅返回每个判别器的最终输出。 > 所有判别器的输出（或中间特征）被收集到 `result` 列表中并返回。 ### 改进后的对抗损失 判别器目标：最大化此损失以区分真实图像 $x$ 与生成图像 $G(s)$。 生成器目标：最小化此损失以生成能欺骗判别器的图像。 从多层鉴别器中提取特征，并从真实图像和合成图像中学习匹配这些中间表示。提出特征匹配损失 $\\mathcal{L}_{FM}(G, D_k)$ 计算为： $$ \\mathcal{L}_{FM}(G, D_k) \\mathbb{E}_{\\text{s}, \\text{x}} \\sum_{i 1}^T \\frac {1}{N_i}[\\vert\\vert D_k^{(i)}(\\text{s}, \\text{x}) D_k^{(i)}(\\text{s}, G(\\text{s}))\\vert\\vert _1] $$ $D_k^{(i)}$：鉴别器 $D_k$ 的第 $i$ 层特征提取器。 $\\text{s}$：输入的语义标签图，每个像素值表示对应物体的类别。 $\\text{x}$：输入的真实图像，与语义标签图对应，用于监督。 $T$：总层数。 $N_i$：每层中元素的数量。 L1 范数：计算真实与生成图像在每层特征上的平均差异。 将 GAN 损失和特征匹配损失结合起来，最终的对抗损失为： $$ \\min_G \\left(\\left( \\max_{\\text{D}_1,\\text{D}_2,\\text{D}_3} \\sum_{k 1,2,3}\\mathcal{L}_{GAN}(G, \\text{D}_k) \\right) + \\lambda \\sum_{k 1,2,3}\\mathcal{L}_{FM}(G, \\text{D}_k) \\right) $$ 其中，$\\lambda$ 控制两项的重要性。 注意，对于特征匹配损失 $\\mathcal{L}_{FM}$，$D_k$ 仅作为特征提取器，不会最大化损失 $\\mathcal{L}_{FM}$。 ### 实例级特征 #### 实例边界图 传统的语义标签仅标注像素类别（如“车辆”“行人”），但无法区分同类对象的不同实例（如相邻的两辆车）。所以导致生成图像中同类相邻对象的边界模糊，例如并排的车辆可能被错误地合并成一块区域。 考虑实例边界图（Instance Boundary Map）：解决语义标签无法区分同类相邻对象的问题（如并排的车辆），显著提升对象边界的清晰度。 每个对象实例具有唯一 ID。 若某像素的实例 ID 与其 4 邻域像素不同，则标记为边界（值为1），否则为 0。 实例边界图的输入设计： 生成器输入：将语义标签图（one hot编码）与实例边界图拼接（通道维度），作为生成器的输入。 判别器输入：将语义标签图、实例边界图和真实/生成图像拼接，供判别器区分真假。 #### 实例级特征嵌入 传统方法（如 pix2pix）从语义标签到图像的映射是确定性的（一对一），无法生成多样化的结果。 为了生成不同的图像并允许实例级控制，添加额外的低维特征通道作为生成器网络的输入。论文表明，通过操纵这些特征，可以对图像合成过程进行灵活的控制。此外，由于特征通道是连续的量，提出的模型原则上能够生成无限多的图像。 为了生成低维特征，训练一个编码器网络来为图像中的每个实例找到一个对应于地面真实目标的低维特征向量。编码器网络设计： 架构层面上：编码器网络（E）采用标准的编码器 解码器结构，输入为真实图像（x），输出为每个实例的低维特征向量。 特征一致性：通过实例级平均池化（instance wise average pooling），将同一实例内所有像素的特征取平均，确保实例内部特征一致。 ![](../static/images/PatchGAN/fig3.png) 使用 $G(\\text{s}, E(\\text{x}))$ 替换 $G(\\text{s})$： $$ \\min_{G,E} \\left(\\left( \\max_{\\text{D}_1,\\text{D}_2,\\text{D}_3} \\sum_{k 1,2,3}\\mathcal{L}_{GAN}(G(\\text{s}, E(\\text{x})), \\text{D}_k) \\right) + \\lambda \\sum_{k 1,2,3}\\mathcal{L}_{FM}(G(\\text{s}, E(\\text{x})), \\text{D}_k) \\right) $$ 训练时采用联合训练，编码器与生成器和鉴别器共同训练。 编码器训练完成后，在训练图像中的所有实例上运行编码器，并记录得到的特征。然后对每个语义类别的这些特征执行 K means 聚类。因此，每个簇对特定样式的特征进行编码，例如，道路的沥青或鹅卵石纹理。在推理时，随机选取一个聚类中心作为编码特征。 生成器的输入扩展为语义标签图 + 实例边界图 + 编码器提取的特征，即 $G(\\text{s},E(\\text{x}))$。 #### 编码器代码 [pix2pixHD/models/pix2pixHD_model.py](https://github.com/NVIDIA/pix2pixHD/blob/master/models/pix2pixHD_model.py#L111) ```python def encode_input(self, label_map, inst_map None, real_image None, feat_map None, infer False): if self.opt.label_nc 0: input_label label_map.data.cuda() else: # 为标签图创建 one hot 向量 size label_map.size() oneHot_size (size[0], self.opt.label_nc, size[2], size[3]) input_label torch.cuda.FloatTensor( torch.Size(oneHot_size) ).zero_() input_label input_label.scatter_(1, label_map.data.long().cuda(), 1.0) if self.opt.data_type 16: input_label input_label.half() # 通过 get_edges 方法检测实例 ID 的边界，生成二值边缘图（edge_map） if not self.opt.no_instance: inst_map inst_map.data.cuda() edge_map self.get_edges(inst_map) input_label torch.cat((input_label, edge_map), dim 1) input_label Variable(input_label, volatile infer) # real images for training / 真实图像用于训练 if real_image is not None: real_image Variable(real_image.data.cuda()) # instance map for feature encoding / 实例图用于特征编码 if self.use_features: # 获取预先计算的特征图 if self.opt.load_features: feat_map Variable(feat_map.data.cuda()) if self.opt.label_feat: inst_map label_map.cuda() return input_label, inst_map, real_image, feat_map ``` ### 论文实验结果 实验数据集： Cityscapes：城市场景数据集（2048×1024分辨率），包含2975张训练图像和500张测试图像。 NYU Indoor：室内场景数据集（561×427分辨率），1200张训练图像，249张测试图像。 ADE20K：通用场景数据集（图像宽度统一为512），20210张训练图像，2000张测试图像。 Helen Face：人脸数据集（1024×1024分辨率），2000张训练图像，330张测试图像。 实验参数： LSGANs：最小二乘 GAN。 $\\lambda 10$。 K Means 中 $K 10$。 每个实例特征用 3 维向量编码。 尝试在目标中添加感知损失： $$ \\lambda \\sum_{i 1}^N \\frac{1}{M_i} \\left[ \\vert\\vert F^{(i)}(\\text{x}) F^{(i)}(G(\\text{s})) \\vert\\vert _1 \\right] $$ $\\lambda 10$。 $F^{(i)}$：VGG 中有 $M_i$ 个元素的第 $i$ 层。 感知损失可以作为变量进行对照试验。 Ours(w/o VGG loss): 不使用 VGG 感知损失。 Ours(w/ VGG loss): 使用了 VGG 感知损失来优化生成图像的感知质量。 #### 定量评估 为了量化结果的质量，对合成图像进行语义分割，并比较预测片段与输入的匹配程度。 如果可以生成与输入标签图相对应的真实图像，现成的语义分割模型（例如，使用的 PSPNet）应该能够预测真实标签。 评估指标： 像素准确率（Pixel Accuracy）：生成图像与真实图像在像素级别的一致性。 平均交并比（Mean IoU）：衡量生成图像中对象区域与真实标注的重合度。 人类偏好率（Human Preference Rate）：通过Amazon Mechanical Turk（AMT）平台进行 A/B 测试，统计用户对生成图像的偏好比例。 结果对比 Cityscapes数据集： 像素准确率：本文方法达到 83.78%，显著优于 pix2pix（78.34%）和 CRN（70.55%）。 平均 IoU：本文方法为 0.6389，远高于 pix2pix（0.3948）和 CRN（0.3483），接近真实图像（Oracle：0.6857）。 结论：本文方法在保持高分辨率的同时，显著提升了语义一致性。 NYU Indoor数据集： 在AMT测试中，本文方法以 86.7% 的偏好率优于 pix2pix，以 63.7% 优于 CRN（图8）。 生成图像色彩更丰富，细节更真实（如家具纹理、灯光反射）。 #### 人类主观研究 任务类型分为： 1. 无时间限制（Unlimited Time）：用户同时观看两幅生成图像（不同方法），选择更自然的结果。 2. 有限时间（Limited Time）：图像展示时间随机（1/8秒至8秒），测试快速感知差异。 实验结果如下： 无时间限制测试： 对比 pix2pix：93.8% 用户偏好本文结果；对比 CRN：86.2% 用户偏好本文结果。 即使不使用 VGG 感知损失（仅对抗训练），本文方法仍以 94.6% 和 85.2% 的偏好率领先。 有限时间测试： 随着展示时间延长，用户更易区分本文方法与 CRN 的差异（如细节纹理、边界清晰度）。 在极短时间（1/8秒）内，本文方法仍以显著优势被选中，证明其生成的全局一致性更优。 消融实验： 损失函数分析： 仅用 GAN 损失：偏好率 58.90%。 GAN + 特征匹配损失：偏好率提升至 68.55%。 加入 VGG 感知损失：进一步提升细节质量，但非必需。 实例边界图的影： 用户对车辆边界的偏好率为 64.34%，证明实例信息显著提升边界真实性。 #### 生成结果可视化 场景合成： Cityscapes： 车辆、树木、建筑等对象的细节（如车窗反射、树叶纹理）更接近真实照片。 CRN 生成结果模糊且重复（如车轮形状不清晰），而本文方法避免了此类问题。 ADE20K： 复杂室内外场景（如厨房、街道）的生成质量与真实图像接近，尤其在光照和材质表现上。 多样化生成与交互编辑： 人脸编辑： 通过修改实例特征（如肤色、发型、妆容），实时生成多样化人脸（如添加胡须、调整唇色）。 用户可基于语义标签（如“眼睛”“嘴巴”）进行局部调整。 边缘转照片： 输入边缘图（如人脸轮廓、猫的草图），生成高分辨率自然图像（如逼真肖像、猫的毛发细节）。 ### 论文代码分析 #### Pix2PixHDModel.forward() [pix2pixHD_model](https://github.com/NVIDIA/pix2pixHD/blob/master/models/pix2pixHD_model.py#L152) ```python def forward(self, label, inst, image, feat, infer False): input_label, inst_map, real_image, feat_map self.encode_input(label, inst, image, feat) ``` > 将输入的标签、实例图、真实图像和特征图进行编码 ```python # 生成假的图像 if self.use_features: if not self.opt.load_features: feat_map self.netE.forward(real_image, inst_map) input_concat torch.cat((input_label, feat_map), dim 1) else: input_concat input_label fake_image self.netG.forward(input_concat) ``` > 如果启用实例特征：若未加载预存特征（`load_features False`），通过编码器 `netE` 从真实图像提取特征 `feat_map`。将语义标签 `input_label` 和特征图 `feat_map` 沿通道维度拼接（`dim 1`）。 > 如果不使用特征，则直接以语义标签 `input_label` 为输入。 > 生成器 `netG` 根据输入生成假图像 `fake_image`。 ```python # Fake Detection and Loss pred_fake_pool self.discriminate(input_label, fake_image, use_pool True) loss_D_fake self.criterionGAN(pred_fake_pool, False) # Real Detection and Loss pred_real self.discriminate(input_label, real_image) loss_D_real self.criterionGAN(pred_real, True) # GAN loss (Fake Passability Loss) pred_fake self.netD.forward(torch.cat((input_label, fake_image), dim 1)) loss_G_GAN self.criterionGAN(pred_fake, True) ``` > 1. 通过 `discriminate` 方法检测假图像 `fake_image` 的真实性，并计算假图像的对抗损失 `loss_D_fake`。 > 2. 类似地计算真实图像的对抗损失 `loss_D_real`。 > 3. 直接通过判别器对假图像的输出 `pred_fake` 计算假图像的对抗损失 `loss_G_GAN`，生成器需要欺骗判别器。 ```python # GAN 特征匹配损失 loss_G_GAN_Feat 0 if not self.opt.no_ganFeat_loss: feat_weights 4.0 / (self.opt.n_layers_D + 1) D_weights 1.0 / self.opt.num_D for i in range(self.opt.num_D): for j in range(len(pred_fake[i]) 1): loss_G_GAN_Feat + D_weights * feat_weights * \\ self.criterionFeat(pred_fake[i][j], pred_real[i][j].detach()) * self.opt.lambda_feat ``` > 遍历每个判别器及其各层，对生成图像和真实图像在判别器各层的特征图计算 L1 损失。 > 通过 `feat_weights`（层权重）和` D_weights`（判别器权重）平衡不同层和判别器的贡献，最终乘以全局权重 `lambda_feat`。 ```python # VGG 特征匹配损失 loss_G_VGG 0 if not self.opt.no_vgg_loss: loss_G_VGG self.criterionVGG(fake_image, real_image) * self.opt.lambda_feat ``` > 如果启用 VGG 特征匹配损失，计算假图像和真实图像的 VGG 特征图之间的损失，乘以全局权重 `lambda_feat`。 #### train [pix2pixHD/train.py](https://github.com/NVIDIA/pix2pixHD/blob/master/train.py) ```python ############## Forward Pass ###################### losses, generated model(Variable(data['label']), Variable(data['inst']), Variable(data['image']), Variable(data['feat']), infer save_fake) # sum per device losses losses [ torch.mean(x) if not isinstance(x, int) else x for x in losses ] loss_dict dict(zip(model.module.loss_names, losses)) # calculate final loss scalar loss_D (loss_dict['D_fake'] + loss_dict['D_real']) * 0.5 loss_G loss_dict['G_GAN'] + loss_dict.get('G_GAN_Feat',0) + \\ loss_dict.get('G_VGG',0) ``` > 前向传播：从 `data` 中提取标签（label）、实例图（inst）、真实图像（image）和特征（feat），并调用 `model.forward()` 生成假图像并计算损失。 > 计算损失： > 判别器损失：`loss_D` 为真假图像损失的平均值。 > 生成器损失：`loss_G` 包含 GAN 损失、特征匹配损失（若启用）和 VGG 损失（若启用）。 ```python ############### Backward Pass #################### # 更新生成器权重 optimizer_G.zero_grad() if opt.fp16: # 若启用 FP16，使用 amp.scale_loss 缩放损失以避免梯度下溢。 with amp.scale_loss(loss_G, optimizer_G) as scaled_loss: scaled_loss.backward() else: loss_G.backward() optimizer_G.step() # 更新鉴别器权重 optimizer_D.zero_grad() if opt.fp16: with amp.scale_loss(loss_D, optimizer_D) as scaled_loss: scaled_loss.backward() else: loss_D.backward() optimizer_D.step() ``` > 1. 梯度清零 > 2. 混合精度训练 > 3. 参数更新 ## 对比 方法 提出论文 核心思想 贡献 : :: :: :: : 基础 PatchGAN pix2pix (2017) 局部感受野的真伪判断 高效局部优化，提升细节真实性 3 级金字塔判别器 pix2pixHD (2018) 多尺度图像金字塔 + PatchGAN 高分辨率生成，多尺度一致性优化"},"/StyleTransfer/ref_and_notes/pytorch_basic_workflow.html":{"title":"PyTorch 基本工作流","content":"Reference:[PyTorchWorkflowFundamentals](https://www.learnpytorch.io/01_pytorch_workflow/)*该页面由JupyterNotebook生成，原文件于[Github](https://github.com/Fingsinz/StyleTransfer/tree/main/src/02.pytorch_learning/pytorch_basic_workflow.ipynb)*#先导入包importtorchfromtorchimportnnimportmatplotlib.pyplotasplttorch.__version__##准备数据集数据可以是很多东西，如一个表格、任何类型的图像、视频、歌曲或播客等音频文件，蛋白质结构，文本等等。机器学习是一个由两部分组成：1.把你的数据转换成数字表示。2.选择或构建一个模型来尽可能地学习数据的表征。获得数据之后，需要将数据划分为训练集、验证集和测试集。类型目的占比使用情况: :: :: :: :训练集模型从这些数据里面学习（比如学习的课程材料）\\~60\\~80%必须有验证集模型会根据这些数据进行调整（比如期末考试前的练习）\\~10\\~20%不必有测试集模型根据这些数据进行评估，以测试它学到了什么（比如期末考试）\\~10\\~20%必须有#创建一个y weight*x+bias的数据集weight 0.7bias 0.3X torch.arange(0,1,0.02).unsqueeze(dim 1)y weight*X+bias#划分训练集和测试集train_split int(0.8*len(X))X_train,y_train X[:train_split],y[:train_split]X_test,y_test X[train_split:],y[train_split:]defplot_predictions(train_data X_train,train_labels y_train,test_data X_test,test_labels y_test,predictions None):plt.figure(figsize (5,3))plt.scatter(train_data,train_labels,c \"b\",s 4,label \"Trainingdata\")plt.scatter(test_data,test_labels,c \"g\",s 4,label \"Testingdata\")ifpredictionsisnotNone:plt.scatter(test_data,predictions,c \"r\",s 4,label \"Prediction\")plt.legend(prop {\"size\":8})plot_predictions()##构建模型###PyTorch模型构建要点PyTorch有四个基本模块，可以用它来创建神经网络： [torch.nn](https://pytorch.org/docs/stable/nn.html)； [torch.optim](https://pytorch.org/docs/stable/optim.html)； [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)； [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html)；模块作用: :: :`torch.nn`包含计算图的所有构建块`torch.nn.Parameter`存储可用于`nn.Module`的张量。如果`requires_grad True`则自动计算梯度（用于通过梯度下降更新模型参数），这通常被称为“autograd”`torch.nn.Module`所有神经网络模块的基类，神经网络的所有构建块都是子类。在PyTorch中构建一个神经网络，模型应该继承`nn.Module`，需要实现`forward()`方法`torch.optim`包含各种优化算法（这些算法告诉存储在`nn.Parameter`中的模型参数。如何最好地改变，以改善梯度下降，从而减少损失)`defforward()`所有的`nn.Module`子类都需要一个`forward()`方法，定义传递给特定`nn.Module`的数据进行的计算（例如线性回归公式）简而言之： `nn.Module`包括大的构建块，如神经网络中的层； `nn.Parameter`包括小的参数，比如权重和偏置，众多参数构成`nn.Module`； `forward()`定义了在`nn.Module`中对输入的计算； `torch.optim`包含如何改进`nn.Parameter`中的参数的算法，以更好地表征数据一个简单的神经网络例子：classLinearRegressionModel(nn.Module):def__init__(self):super().__init__()self.weights nn.Parameter(torch.randn(1,dtype torch.float),requires_grad True)self.bias nn.Parameter(torch.randn(1,dtype torch.float),requires_grad True)defforward(self,x:torch.Tensor) >torch.Tensor:returnself.weights*x+self.bias###检视模型中的内容使用`model.parameters()`检查参数：torch.manual_seed(42)model_0 LinearRegressionModel()list(model_0.parameters())使用`model.state_dict()`获得模型状态（包含什么）：model_0.state_dict()###使用`torch.inference_mode()`进行预测`torch.inference_mode()`关闭了一些东西，比如梯度跟踪（训练所必需的，但不是推理所必需的），所以`forward`传递更快。withtorch.inference_mode():y_preds model_0(X_test)plot_predictions(predictions y_preds)由图可知模型预测距离真实值仍有一段距离，所以需要训练模型以达到更好的效果。##训练模型###建立损失函数和优化器为了让模型自己更新参数，需要添加损失函数和优化器。功能作用PyTorch的形式常用值: :: :: :: :损失函数测量模型的预测（`y_preds`）与真值标签（`y_test`）相比的错误程度。越低越好`torch.nn`中有很多内置的损失函数回归问题的平均绝对误差（MAE，`torch.nn.L1Loss()`）；二元分类问题的二元交叉熵（`torch.nn.BCELoss()`）优化器告诉模型如何更新其内部参数以最大程度地降低损失`torch.optim`中的各种优化函数实现随机梯度下降（`torch.optim.SGD()`）；Adam优化器（`torch.optim.Adam()`）为模型添加损失函数和优化器：###训练循环训练过程有以下步骤：序号步骤做法代码: :: :: :: :1前向传播模型将所有训练数据遍历一次，执行其`forward()`函数计算`model(x_train)`2计算损失将模型的输出（预测）与真实数据进行评估，计算损失值`loss loss_fn(y_pred,y_train)`3梯度归零优化器的梯度被设置为零（默认情况下它们是累积的），因此它们可以为特定的训练步骤重新计算`optimizer.zero_grad()`4反向传播损失计算每个要更新的模型参数的损失梯度（每个参数`requires_grad True`）`loss.backward()`5更新参数使用`requires_grad True`更新损耗梯度的参数`optimizer.step()`###测试循环测试过程有以下步骤：序号步骤做法代码: :: :: :: :1前向传播模型将所有训练数据遍历一次，执行其`forward()`函数计算`model(x_train)`2计算损失将模型的输出（预测）与真实数据进行评估，计算损失值`loss loss_fn(y_pred,y_train)`3计算评估指标（可选）计算其他评估指标，例如测试集上的准确性自定义函数###训练并测试代码torch.manual_seed(42)device \"cuda\"iftorch.cuda.is_available()else\"cpu\"X_train X_train.to(device)y_train y_train.to(device)X_test X_test.to(device)y_test y_test.to(device)model_0 model_0.to(device)epochs 200train_loss_values []test_loss_values []epoch_cnt []loss_fn nn.L1Loss()#损失函数optimizer torch.optim.SGD(params model_0.parameters(),lr 0.01)#优化器forepochinrange(epochs):model_0.train()#设置为训练模式#1.前向传播y_pred model_0(X_train)#2.计算损失loss loss_fn(y_pred,y_train)#3.梯度归零、反向传播、更新参数optimizer.zero_grad()loss.backward()optimizer.step()model_0.eval()#设置为评估模式withtorch.inference_mode():test_pred model_0(X_test)test_loss loss_fn(test_pred,y_test.type(torch.float))ifepoch%20 0:epoch_cnt.append(epoch)train_loss_values.append(loss.cpu().detach().numpy())test_loss_values.append(test_loss.cpu().detach().numpy())print(f\"{epoch}/{epochs}MAE训练损失:{loss}MAE测试损失:{test_loss}\")0/200 MAE 训练损失: 0.31288135051727295 MAE 测试损失: 0.48106518387794495 20/200 MAE 训练损失: 0.08908725529909134 MAE 测试损失: 0.21729658544063568 40/200 MAE 训练损失: 0.04543796926736832 MAE 测试损失: 0.11360953003168106 60/200 MAE 训练损失: 0.03818932920694351 MAE 测试损失: 0.08886633068323135 80/200 MAE 训练损失: 0.03132382780313492 MAE 测试损失: 0.07232122868299484 100/200 MAE 训练损失: 0.024458957836031914 MAE 测试损失: 0.05646304413676262 120/200 MAE 训练损失: 0.01758546754717827 MAE 测试损失: 0.04060482606291771 140/200 MAE 训练损失: 0.010716589167714119 MAE 测试损失: 0.024059748277068138 160/200 MAE 训练损失: 0.003851776709780097 MAE 测试损失: 0.008201557211577892 180/200 MAE 训练损失: 0.008932482451200485 MAE 测试损失: 0.005023092031478882损失随着时间的推移而下降，绘图：plt.figure(figsize (5,3))plt.plot(epoch_cnt,train_loss_values,label \"TrainLoss\")plt.plot(epoch_cnt,test_loss_values,label \"TestLoss\")plt.title(\"LossCurves\")plt.ylabel(\"Loss\")plt.xlabel(\"Epochs\")plt.legend()输出一下训练得到的`weight`和`bias`：print(model_0.state_dict())print(f\"{weight},{bias}\")OrderedDict([('weights', tensor([0.6990], device 'cuda:0')), ('bias', tensor([0.3093], device 'cuda:0'))]) 0.7, 0.3由此可知训练得到的参数与实际的参数已经相差很小了。##使用训练后的PyTorch模型进行推理在使用PyTorch模型进行预测（也称为执行推理）时，需要记住三件事：1.将模型设置为评估模式（`model.eval()`）。2.使用推理模式上下文管理器（使用`torch.inference_mode()`）进行预测。3.所有的预测都应该在同一设备上进行（仅在GPU上的数据和模型或仅在CPU上的数据和模型）。前两项确保关闭PyTorch在训练期间在幕后使用的所有有用的计算和设置，但这些计算和设置对于推理是不必要的（这导致更快的计算）。第三个确保你不会遇到跨设备错误。最后查看整体的分布：model_0.eval()withtorch.inference_mode():y_preds model_0(X_test)plot_predictions(predictions y_preds.cpu())##保存和加载模型比如在服务器上训练模型后，需要转移到本地或其他地方进行使用。方法作用: :: :`torch.save`使用Python的`pickle`实用程序将序列化的对象保存到磁盘。模型、张量和其他各种Python对象（如字典）都可以使用`torch.save`保存`torch.load`使用`pickle`的unpickling特性来反序列化并将文件（如模型，张量或字典）加载到内存中。也可以设置加载对象到哪个设备（CPU，GPU等）`torch.nn.Module.load_state_dict`使用已保存的`state_dict()`对象加载模型的参数字典（`model.state_dict()`）###保存模型的参数字典torch.save(obj model_0.state_dict(),f \"model_0.pth\")###加载参数字典到模型中loaded_model_0 LinearRegressionModel()loaded_model_0.load_state_dict(torch.load(f \"model_0.pth\",weights_only True))#测试看看预测结果是否相等loaded_model_0.to(device)loaded_model_0.eval()withtorch.inference_mode():loaded_model_preds loaded_model_0(X_test)loaded_model_preds y_preds"},"/StyleTransfer/ref_and_notes/pytorch_classification.html":{"title":"PyTorch 神经网络分类","content":"Reference:[PyTorchNeuralNetworkClassification](https://www.learnpytorch.io/02_pytorch_classification/)*该页面由JupyterNotebook生成，原文件于[Github](https://github.com/Fingsinz/StyleTransfer/tree/main/src/02.pytorch_learning/pytorch_classification.ipynb)*#先导入包importtorchfromtorchimportnnimportmatplotlib.pyplotasplttorch.__version__分类问题有二分类、多分类、多标签等情况。二分类问题则是或不是；多分类问题具有多个类别区分；多标签问题则一个目标可以被分配多个选项。##分类神经网络的结构分类神经网络的一般架构：项目二分类多分类: :: :: :输入层Shape（`in_features`）与特征数相同与特征数相同隐藏层特定问题特定分析特定问题特定分析每个隐藏层的神经元数量特定问题特定分析，一般从10到512特定问题特定分析，一般从10到512输出层Shape（`out_features`）1（一个类别）每个类1个输出隐藏层激活函数通常是ReLU通常是ReLU输出层激活函数Sigmoid（`torch.sigmoid`）Softmax（`torch.softmax`）损失函数二元交叉熵（`torch.nn.BCELoss`）交叉熵（`torch.nn.CrossEntropyLoss`）优化器SGD，AdamSGD，Adam##准备二分类数据集###输入和输出形状使用Scikit Learn中的`make_circles()`方法生成两个带有不同颜色圆点的圆。*需要安装Scikit Learn：`pipinstallscikit learn`*fromsklearn.datasetsimportmake_circlesn_samples 1000X,y make_circles(n_samples,noise 0.03,random_state 42)plt.scatter(x X[:,0],y X[:,1],c y,cmap plt.cm.RdYlBu)看看输入Shape和输出Shape，然后弄清楚输入层Shape（特征数）和输出层Shape。X.shape,y.shape#输入Shape和输出ShapeX[0].shape,y[0].shape#输入层Shape和输出层Shape这说明X的一个样本有两个特征（向量），而对应的y只有一个特征（标量）。 有两个输入对应一个输出。###划分数据集具体来说：1.将数据转换为张量。2.将数据分成训练集和测试集。X torch.from_numpy(X).type(torch.float)y torch.from_numpy(y).type(torch.float)X.dtype,y.dtype使用Scikit Learn中的函数`train_test_split()`。`test_size 0.2`（80%训练，20%测试），因为分割是随机发生的，所以使用`random_state 42`，使得随机可复现。fromsklearn.model_selectionimporttrain_test_splitX_train,X_test,y_train,y_test train_test_split(X,y,test_size 0.2,random_state 42)len(X_train),len(y_train),len(X_test),len(y_test)##构建分类模型构建模型的步骤：1.设置与设备相关的代码。2.通过继承`nn.module`来构造一个模型。3.定义损失函数和优化器。4.创建一个训练循环。###设置设备#1.设置设备device \"cuda\"iftorch.cuda.is_available()else\"cpu\"device###构建模型对象模型类的操作：1.继承`nn.Module`。2.在构造函数中创建2层`nn.Linear`线性层，能够处理X和y的形状。3.定义一个`forward()`方法，该方法包含模型的前向传递计算。4.实例化模型类并将其发送到目标设备。classCircleModelV0(nn.Module):def__init__(self):super().__init__()self.layer_1 nn.Linear(in_features 2,out_features 5)self.layer_2 nn.Linear(in_features 5,out_features 1)defforward(self,x):returnself.layer_2(self.layer_1)model_0 CircleModelV0().to(device)model_0由上面代码可知该模型类的结构为：`2(输入层) >5(隐藏层) >1(输出层)`也可以使用`nn.Sequential`执行与上面相同的操作。`nn.Sequential`按层出现的顺序对输入数据执行前向传递计算。model_0 nn.Sequential(nn.Linear(in_features 2,out_features 5),nn.Linear(in_features 5,out_features 1)).to(device)model_0自定义模型类可以自定义更多细节，而`nn.Sequential()`则更方便。###定义损失函数和优化器常见损失函数：损失函数适用类型代码: :: :: :交叉熵损失函数多分类`torch.nn.CrossEntropyLoss`平均绝对误差MAE，L1Loss回归问题`torch.nn.L1Loss`均方误差MSE，L2Loss回归问题`torch.nn.MSELoss`常见优化器：优化器适用类型代码: :: :: :随机梯度下降（SGD）分类问题、回归问题等`torch.optim.SGD()`Adam分类问题、回归问题等`torch.optim.Adam()`此处讨论二分类问题，使用一个二元交叉熵损失函数。>注意：损失函数是衡量模型预测错误程度的函数，损失越高，模型越差。>>此外，PyTorch文档经常将损失函数称为“损失准则（losscriterion）”或“准则（criterion）”，这些都是描述同一事物的不同方式。二元交叉熵函数有`torch.nn.BCELoss()`和`torch.nn.BCEWithLogitsLoss()`。 `torch.nn.BCELoss()`：创建一个损失函数，用于测量目标（标签）和输入（特征）之间的二进制交叉熵。 `torch.nn.BCEWithLogitsLoss()`：它内置了一个sigmoid层，其他这与上面的相同。`torch.nn.BCEWithLogitsLoss()`的文档指出，它比在`nn.Sigmoid`层之后使用`torch.nn.BCELoss()`在数值上更稳定。对于优化器，将使用`torch.optim.SGD()`以0.1的学习率优化模型参数。loss_fn nn.BCEWithLogitsLoss()optimizer torch.optim.SGD(params model_0.parameters(),lr 0.1)评估指标可用于提供关于模型运行情况的另一个视角。如果一个损失函数衡量模型的错误程度，那么也有评估指标衡量他的正确程度。defaccuracy_fn(y_true,y_pred):correct torch.eq(y_true,y_pred).sum().item()acc (correct/len(y_pred))*100returnacc##训练分类模型###将原始输出变成标签线性层的公式为：$$y x\\cdot\\text{Weights}^T+bias$$模型的原始输出通常被称为logits。使用激活函数将logits转换成与真值标签相比较的数字。###构建训练和测试循环torch.manual_seed(42)epochs 100X_train,y_train X_train.to(device),y_train.to(device)X_test,y_test X_test.to(device),y_test.to(device)model_0 model_0.to(device)forepochinrange(epochs):model_0.train()y_logits model_0(X_train).squeeze()y_pred torch.round(torch.sigmoid(y_logits))loss loss_fn(y_logits,y_train)acc accuracy_fn(y_true y_train,y_pred y_pred)optimizer.zero_grad()loss.backward()optimizer.step()model_0.eval()withtorch.inference_mode():test_logits model_0(X_test).squeeze()test_pred torch.round(torch.sigmoid(test_logits))test_loss loss_fn(test_logits,y_test)test_acc accuracy_fn(y_true y_test,y_pred test_pred)ifepoch%10 0:print(f\"{epoch}/{epochs}Loss:{loss:.5f},Accu:{acc:.2f}%TestLoss:{test_loss:.5f},TestAccu:{test_acc:.2f}%\")0/100 Loss: 0.70365, Accu: 49.88% Test Loss: 0.71542, Test Accu: 45.50% 10/100 Loss: 0.70059, Accu: 50.25% Test Loss: 0.71142, Test Accu: 45.00% 20/100 Loss: 0.69869, Accu: 50.38% Test Loss: 0.70858, Test Accu: 46.00% 30/100 Loss: 0.69740, Accu: 50.75% Test Loss: 0.70641, Test Accu: 46.00% 40/100 Loss: 0.69648, Accu: 50.75% Test Loss: 0.70469, Test Accu: 45.50% 50/100 Loss: 0.69580, Accu: 50.50% Test Loss: 0.70329, Test Accu: 46.50% 60/100 Loss: 0.69527, Accu: 50.75% Test Loss: 0.70214, Test Accu: 46.00% 70/100 Loss: 0.69487, Accu: 50.88% Test Loss: 0.70117, Test Accu: 45.50% 80/100 Loss: 0.69455, Accu: 50.75% Test Loss: 0.70036, Test Accu: 45.00% 90/100 Loss: 0.69429, Accu: 50.75% Test Loss: 0.69966, Test Accu: 45.00%模型看起来它很好地完成了训练和测试步骤，但结果似乎并没有太大的变化。每次数据分割时，准确率在50%左右。这是一个平衡的二进制分类问题，这意味着模型的性能与随机猜测差不多。##预测评估分类模型从指标来看，模型似乎是随机猜测。绘制一个模型预测的图，它试图预测的数据以及它为某个东西是类0还是类1创建的决策边界。为此，编写一些代码，一个名为`plot_decision_boundary()`的有用函数，该函数创建一个NumPymeshgrid，以可视化地绘制我们的模型预测某些类的不同点。importnumpyasnpdefplot_decision_boundary(model:torch.nn.Module,X:torch.Tensor,y:torch.Tensor):\"\"\"PlotsdecisionboundariesofmodelpredictingonXincomparisontoy.Source https://madewithml.com/courses/foundations/neural networks/(withmodifications)\"\"\"#PuteverythingtoCPU(worksbetterwithNumPy+Matplotlib)model.to(\"cpu\")X,y X.to(\"cpu\"),y.to(\"cpu\")#Setuppredictionboundariesandgridx_min,x_max X[:,0].min() 0.1,X[:,0].max()+0.1y_min,y_max X[:,1].min() 0.1,X[:,1].max()+0.1xx,yy np.meshgrid(np.linspace(x_min,x_max,101),np.linspace(y_min,y_max,101))#MakefeaturesX_to_pred_on torch.from_numpy(np.column_stack((xx.ravel(),yy.ravel()))).float()#Makepredictionsmodel.eval()withtorch.inference_mode():y_logits model(X_to_pred_on)#Testformulti classorbinaryandadjustlogitstopredictionlabelsiflen(torch.unique(y))>2:y_pred torch.softmax(y_logits,dim 1).argmax(dim 1)#mutli classelse:y_pred torch.round(torch.sigmoid(y_logits))#binary#Reshapepredsandploty_pred y_pred.reshape(xx.shape).detach().numpy()plt.contourf(xx,yy,y_pred,cmap plt.cm.RdYlBu,alpha 0.7)plt.scatter(X[:,0],X[:,1],c y,s 40,cmap plt.cm.RdYlBu)plt.xlim(xx.min(),xx.max())plt.ylim(yy.min(),yy.max())plt.figure(figsize (12,6))plt.subplot(1,2,1)plt.title(\"Train\")plot_decision_boundary(model_0,X_train,y_train)plt.subplot(1,2,2)plt.title(\"Test\")plot_decision_boundary(model_0,X_test,y_test)由图可知，模型目前正在尝试用直线分割红点和蓝点。由于我们的数据是圆形的，所以画一条直线最多只能把它从中间切开。在机器学习方面，模型是欠拟合的，这意味着它没有从数据中学习预测模式。##改进模型尝试解决模型的拟合不足问题。 专注于模型（而不是数据）。技巧作用: :: :增加更多隐藏层每一层都可能增加模型的学习能力，每一层都能够学习数据中的某种新模式。更多的层通常被称为使神经网络更深增加更多隐藏神经元与上面类似，每层更多的隐藏单元意味着模型学习能力的潜在增加。更多的隐藏单元通常被称为使你的神经网络更宽增加训练轮数如果模型训练更久，它可能会学到更多改变激活函数有些数据无法仅用直线拟合，使用非线性激活函数可以帮助解决这个问题改变学习率优化器的学习率决定了模型每一步应该改变多少参数，太多了模型会过度校正，太少了模型学习不足改变损失函数不同的问题需要不同的损失函数迁移学习从一个与问题领域相似的问题领域中提取一个预先训练好的模型，并根据问题进行调整###添加非线性classCircleModelV1(nn.Module):def__init__(self):super().__init__()self.layer_1 nn.Linear(in_features 2,out_features 10)self.layer_2 nn.Linear(in_features 10,out_features 10)self.layer_3 nn.Linear(in_features 10,out_features 1)self.relu nn.ReLU()defforward(self,x):returnself.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))model_1 CircleModelV1().to(device)#model_1 nn.Sequential(#nn.Linear(2,10),#nn.ReLU(),#nn.Linear(10,10),#nn.ReLU(),#nn.Linear(10,1),#).to(device)model_1loss_fn nn.BCEWithLogitsLoss()optimizer torch.optim.SGD(params model_1.parameters(),lr 0.1)重新训练：torch.manual_seed(42)epochs 1500X_train,y_train X_train.to(device),y_train.to(device)X_test,y_test X_test.to(device),y_test.to(device)model_1.to(device)forepochinrange(epochs):model_1.train()y_logits model_1(X_train).squeeze()y_pred torch.round(torch.sigmoid(y_logits))loss loss_fn(y_logits,y_train)acc accuracy_fn(y_true y_train,y_pred y_pred)optimizer.zero_grad()loss.backward()optimizer.step()model_1.eval()withtorch.inference_mode():test_logits model_1(X_test).squeeze()test_pred torch.round(torch.sigmoid(test_logits))test_loss loss_fn(test_logits,y_test)test_acc accuracy_fn(y_true y_test,y_pred test_pred)ifepoch%100 0:print(f\"{epoch}/{epochs}Loss:{loss:.5f},Accu:{acc:.2f}%TestLoss:{test_loss:.5f},TestAccu:{test_acc:.2f}%\")0/1500 Loss: 0.69295, Accu: 50.00% Test Loss: 0.69319, Test Accu: 50.00% 100/1500 Loss: 0.69115, Accu: 52.88% Test Loss: 0.69102, Test Accu: 52.50% 200/1500 Loss: 0.68977, Accu: 53.37% Test Loss: 0.68940, Test Accu: 55.00% 300/1500 Loss: 0.68795, Accu: 53.00% Test Loss: 0.68723, Test Accu: 56.00% 400/1500 Loss: 0.68517, Accu: 52.75% Test Loss: 0.68411, Test Accu: 56.50% 500/1500 Loss: 0.68102, Accu: 52.75% Test Loss: 0.67941, Test Accu: 56.50% 600/1500 Loss: 0.67515, Accu: 54.50% Test Loss: 0.67285, Test Accu: 56.00% 700/1500 Loss: 0.66659, Accu: 58.38% Test Loss: 0.66322, Test Accu: 59.00% 800/1500 Loss: 0.65160, Accu: 64.00% Test Loss: 0.64757, Test Accu: 67.50% 900/1500 Loss: 0.62362, Accu: 74.00% Test Loss: 0.62145, Test Accu: 79.00% 1000/1500 Loss: 0.56818, Accu: 87.75% Test Loss: 0.57378, Test Accu: 86.50% 1100/1500 Loss: 0.48153, Accu: 93.50% Test Loss: 0.49935, Test Accu: 90.50% 1200/1500 Loss: 0.37056, Accu: 97.75% Test Loss: 0.40595, Test Accu: 92.00% 1300/1500 Loss: 0.25458, Accu: 99.00% Test Loss: 0.30333, Test Accu: 96.50% 1400/1500 Loss: 0.17180, Accu: 99.50% Test Loss: 0.22108, Test Accu: 97.50%可视化一下：plt.figure(figsize (12,6))plt.subplot(1,2,1)plt.title(\"Train\")plot_decision_boundary(model_1,X_train,y_train)plt.subplot(1,2,2)plt.title(\"Test\")plot_decision_boundary(model_1,X_test,y_test)现在模型的分类就有了显著的效果。##多分类问题###构建多分类数据集利用Scikit Learn的`make_blobs()`方法。这个方法将创建任意数量的类（使用`centers`参数）。1.使用`make_blobs()`创建一些多类数据。2.将数据转换为张量（`make_blobs()`的默认值是使用NumPy数组）。3.使用`train_test_split()`将数据拆分为训练集和测试集。4.可视化数据。importtorchimportmatplotlib.pyplotaspltfromsklearn.datasetsimportmake_blobsfromsklearn.model_selectionimporttrain_test_splitNUM_CLASSES 4NUM_FEATURES 2RANDOM_SEED 42X_blob,y_blob make_blobs(n_samples 1000,n_features NUM_FEATURES,centers NUM_CLASSES,cluster_std 1.5,random_state RANDOM_SEED)X_blob torch.from_numpy(X_blob).type(torch.float)y_blob torch.from_numpy(y_blob).type(torch.LongTensor)X_blob_train,X_blob_test,y_blob_train,y_blob_test train_test_split(X_blob,y_blob,test_size 0.2,random_state RANDOM_SEED)plt.figure(figsize (10,6))plt.scatter(X_blob[:,0],X_blob[:,1],c y_blob,cmap plt.cm.RdYlBu)###构建多元分类模型创建一个`nn.Module`的子类，接受三个超参数： `input_features`：输入特征的数量。 `output_features`：输出特征数（等效于`NUM_CLASSES`或多类分类问题中的类数）。 `hidden_units`：每个隐藏层使用的隐藏神经元的数量。device \"cuda\"iftorch.cuda.is_available()else\"cpu\"fromtorchimportnnclassBlobModel(nn.Module):def__init__(self,input_features,output_features,hidden_units 8):super().__init__()self.model nn.Sequential(nn.Linear(in_features input_features,out_features hidden_units),nn.ReLU(),nn.Linear(in_features hidden_units,out_features hidden_units),nn.ReLU(),nn.Linear(in_features hidden_units,out_features output_features))defforward(self,x):returnself.model(x)model_2 BlobModel(input_features NUM_FEATURES,output_features NUM_CLASSES,hidden_units 8).to(device)model_2###构建多元分类损失函数和优化器loss_fn nn.CrossEntropyLoss()optimizer torch.optim.SGD(model_2.parameters(),lr 0.1)试着看看模型前向输出：y_logits model_2(X_blob_train.to(device))[:5]y_logits再看看经过激活函数Softmax之后的结果：y_pred_probs torch.softmax(y_logits,dim 1)y_pred_probs经过Softmax函数之后，先前的数字变成预测到某类的概率。这些预测概率本质上是说模型认为目标样本（输入）映射到每个类的程度。由于y_pred_probs中的每个类都有一个值，因此最高值的索引是模型认为特定数据样本最属于的类。可以使用`torch.argmax()`检查哪个索引具有最高值。torch.argmax(y_pred_probs[0])###构建多分类训练和测试循环torch.manual_seed(42)epochs 100X_blob_train,y_blob_train X_blob_train.to(device),y_blob_train.to(device)X_blob_test,y_blob_test X_blob_test.to(device),y_blob_test.to(device)model_2.to(device)forepochinrange(epochs):model_2.train()y_logits model_2(X_blob_train)y_pred torch.softmax(y_logits,dim 1).argmax(dim 1)loss loss_fn(y_logits,y_blob_train)acc accuracy_fn(y_true y_blob_train,y_pred y_pred)optimizer.zero_grad()loss.backward()optimizer.step()model_2.eval()withtorch.inference_mode():test_logits model_2(X_blob_test)test_pred torch.softmax(test_logits,dim 1).argmax(dim 1)test_loss loss_fn(test_logits,y_blob_test)tess_acc accuracy_fn(y_true y_blob_test,y_pred test_pred)ifepoch%10 0:print(f\"{epoch}/{epochs}Loss:{loss:.5f},Acc:{acc:.2f}%TestLoss:{test_loss:.5f},TestAcc:{test_acc:.2f}%\")0/100 Loss: 1.15883, Acc: 40.38% Test Loss: 1.07554, Test Acc: 99.00% 10/100 Loss: 0.64476, Acc: 96.75% Test Loss: 0.66069, Test Acc: 99.00% 20/100 Loss: 0.42535, Acc: 98.50% Test Loss: 0.43074, Test Acc: 99.00% 30/100 Loss: 0.25294, Acc: 99.12% Test Loss: 0.24508, Test Acc: 99.00% 40/100 Loss: 0.11232, Acc: 99.25% Test Loss: 0.10229, Test Acc: 99.00% 50/100 Loss: 0.06627, Acc: 99.25% Test Loss: 0.05848, Test Acc: 99.00% 60/100 Loss: 0.05068, Acc: 99.25% Test Loss: 0.04293, Test Acc: 99.00% 70/100 Loss: 0.04300, Acc: 99.25% Test Loss: 0.03491, Test Acc: 99.00% 80/100 Loss: 0.03836, Acc: 99.25% Test Loss: 0.02988, Test Acc: 99.00% 90/100 Loss: 0.03525, Acc: 99.25% Test Loss: 0.02663, Test Acc: 99.00%###评估多分类模型使用准确率评估：model_2.eval()withtorch.inference_mode():y_logits model_2(X_blob_test)y_preds torch.softmax(y_logits,dim 1).argmax(dim 1)print(f\"Testaccuracy:{accuracy_fn(y_true y_blob_test,y_pred y_preds)}%\")Test accuracy: 99.5%可视化评估：plt.figure(figsize (12,6))plt.subplot(1,2,1)plt.title(\"Train\")plot_decision_boundary(model_2,X_blob_train,y_blob_train)plt.subplot(1,2,2)plt.title(\"Test\")plot_decision_boundary(model_2,X_blob_test,y_blob_test)##更多分类评估指标评估指标定义代码: :: :: :正确率模型预测正确的占比`torchmetrics.Accuracy()`或`sklearn.metrics.accuracy_score()`精确率$\\text{Precision} \\frac{TP}{TP+FP}$`torchmetrics.Precision()`或`sklearn.metrics.precision_score()`召回率$\\text{Recall} \\frac{TP}{TP+FN}$`torchmetrics.Recall()`或`sklearn.metrics.recall_score()`F1 Score将查准率和查全率合并为一个指标。1是最好的，0是最坏的`torchmetrics.F1Score()`或`sklearn.metrics.f1_score()`混淆矩阵以表格方式将预测值与真实值进行比较，如果100%正确，矩阵中的所有值将从左上角到右下角（对角线）`torchmetrics.ConfusionMatrix`或`sklearn.metrics.plot_confusion_matrix()`分类报告一些主要分类指标的集合，如精度，召回率和f1分数`sklearn.metrics.classification_report()`"},"/StyleTransfer/ref_and_notes/pytorch_computer_vision.html":{"title":"计算机视觉基础","content":"Reference:[PyTorchComputerVision](https://www.learnpytorch.io/03_pytorch_computer_vision/)*该页面由JupyterNotebook生成，原文件于[Github](https://github.com/Fingsinz/StyleTransfer/tree/main/src/02.pytorch_learning/pytorch_computer_vision.ipynb)*##PyTorch的计算机视觉库PyTorch中关于计算机视觉的库有：模块作用: :: :[torchvision](https://pytorch.org/vision/stable/index.html)包含通常用于计算机视觉问题的数据集、模型架构和图像转换[torchvision.datasets](https://pytorch.org/vision/stable/datasets.html)包含许多计算机视觉数据集，用于解决图像分类、对象检测、图像字幕、视频分类等一系列问题，还包含一系列用于创建自定义数据集的基类[torchvision.models](https://pytorch.org/vision/stable/models.html)包含在PyTorch中实现的性能良好且常用的计算机视觉模型架构，可以将其用于解决问题[torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)图像需要在与模型一起使用之前进行预处理（转换为数字/处理/增强），包含常见的图像转换[torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)PyTorch基础数据集类[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#module torch.utils.data)创建一个可在数据集上迭代的对象（`torch.utils.data.Dataset`）#PyTorchimporttorchfromtorchimportnn#torchvisionimporttorchvisionfromtorchvisionimportdatasetsfromtorchvision.transformsimportToTensor#matplotlibimportmatplotlib.pyplotaspltprint(f\"{torch.__version__}\\n{torchvision.__version__}\")2.5.1+cu124 0.20.1+cu124device \"cuda\"iftorch.cuda.is_available()else\"cpu\"device##准备数据集`torchvision.datasets`包含了大量的示例数据集，可以使用它们来练习编写计算机视觉代码。 MNIST：手写数字数据集，包含数千个手写数字（从0到9）的示例。 FashionMNIST：有10个不同的图像类（不同类型的衣服），这是一个多类分类问题。FashionMNIST可以通过`torchvision.datasets.FashionMNIST`获得。提供以下参数： `root:str`，数据下载到哪个文件夹； `train:Bool`，训练还是测试分割； `download:Bool`，是否下载数据； `transform:torchvision.transforms`，对数据的转换； `target_transform`，对目标（标签）的转换。train_data datasets.FashionMNIST(root \"data\",train True,download True,transform ToTensor(),#图像以PIL格式出现，将其转换为Torch张量target_transform None)test_data datasets.FashionMNIST(root \"data\",train False,#表示测试download True,transform ToTensor())len(train_data),len(test_data)###了解数据集得到数据后，需要了解数据的Shape：image,label train_data[0]image.shape图像张量的形状是`[1，28，28]`或更具体地：`[color_channels 1，height 28，width 28]`。不同的问题会有不同的输入和输出形式。但前提仍然是将数据编码成数字，建立一个模型来找到这些数字中的模式，将这些模式表达成有意义的东西。>除了CHW（通道，高度，宽度）表示，后续还会见到NCHW和NHWC格式，其中N代表图像数量。例如，当batch_size 32，则张量形状可能是[32，1，28，28]。>>PyTorch通常接受NCHW（通道优先）作为许多操作的默认设置。然而，PyTorch还解释说，NHWC（通道最后）性能更好，被认为是最佳实践。>>由于目前的数据集和模型相对较小，这不会有太大的区别。了解数据集的数量后，还可以了解一下类别属性：class_names train_data.classesclass_names可以可视化一下数据：image,label train_data[0]plt.figure(figsize (4,4))plt.imshow(image.squeeze(),cmap \"gray\")#Shape为[1,28,28]，将通道挤压一下plt.title(label)查看更多：torch.manual_seed(42)fig plt.figure(figsize (7,7))rows,cols 4,4foriinrange(1,rows*cols+1):random_idx torch.randint(0,len(train_data),size [1]).item()img,label train_data[random_idx]fig.add_subplot(rows,cols,i)plt.imshow(img.squeeze(),cmap \"gray\")plt.title(class_names[label])plt.axis(False)###构建DataLoader现在已经有了一个数据集，下一步是用`torch.utils.data.DataLoader`准备它。DataLoader有助于将数据加载到模型中。为了训练和推理，它将一个大的数据集转换成一个可迭代的小块。 这些较小的块称为批处理或小批处理，可以通过`batch_size`参数进行设置，这样做可以让计算效率更高。对于小批量（数据的一小部分）而言，每个epoch执行梯度下降的频率更高。 每个batch一次，而不是每个epoch一次。`batch_size`是一个超参数，视情况而定。通常使用2的幂（例如32、64、128、256、512）。fromtorch.utils.dataimportDataLoaderBATCH_SIZE 32train_dataloader DataLoader(train_data,batch_size BATCH_SIZE,shuffle True#每一个epoch都洗牌数据)test_dataloader DataLoader(test_data,batch_size BATCH_SIZE,shuffle False)print(f\"Train:{len(train_dataloader)}\")print(f\"Test:{len(test_dataloader)}\")Train: 1875 Test: 313看看每个batch的shape：train_features_batch,train_labels_batch next(iter(train_dataloader))train_features_batch.shape,train_labels_batch.shape##模型0：构建基线模型数据已加载并准备就绪。基线模型是最简单的模型之一。使用基线作为起点，并尝试使用后续的更复杂的模型对其进行改进。目前基线模型将由两个`nn.Linear()`层组成。因为这是图像数据，所以将使用`nn.Flatten()`层开始。 `nn.Flatten()`将张量的维度压缩为单个向量。classFashionMNISTModelV0(nn.Module):def__init__(self,input_shape:int,hidden_units:int,output_shape:int):super().__init__()self.model nn.Sequential(nn.Flatten(),nn.Linear(in_features input_shape,out_features hidden_units),nn.Linear(in_features hidden_units,out_features output_shape))defforward(self,x):returnself.model(x)设置以下参数： `input_shape 784`：此例中，目标图像中的每个像素都有一个特征（28像素高×28像素宽 784个特征）。 `hidden_units 10`：隐藏层中的神经元数量。 `output_shape len(class_names)`：多分类问题，需要为数据集中的每个类提供一个输出神经元。torch.manual_seed(42)model_0 FashionMNISTModelV0(input_shape 784,hidden_units 10,output_shape len(class_names))设置上损失函数和优化器：loss_fn nn.CrossEntropyLoss()optimizer torch.optim.SGD(params model_0.parameters(),lr 0.1)自定义计算准确率的函数和计时函数：defaccuracy_fn(y_true,y_pred):correct torch.eq(y_true,y_pred).sum().item()acc (correct/len(y_pred))*100returnaccfromtimeitimportdefault_timerastimerdefprint_train_time(start:float,end:float,device:torch.device None):total_time end startprint(f\"{device}:{total_time:.3f}seconds\")returntotal_time构建训练和测试循环（CPU）：torch.manual_seed(42)train_time_start timer()epochs 3forepochinrange(epochs):print(f\"Epoch:{epoch}\\n \")train_loss 0forbatch,(X,y)inenumerate(train_dataloader):model_0.train()y_pred model_0(X)loss loss_fn(y_pred,y)train_loss+ lossoptimizer.zero_grad()loss.backward()optimizer.step()train_loss/ len(train_dataloader)test_loss,test_acc 0,0model_0.eval()withtorch.inference_mode():forX,yintest_dataloader:test_pred model_0(X)test_loss+ loss_fn(test_pred,y)test_acc+ accuracy_fn(y_true y,y_pred test_pred.argmax(dim 1))test_loss/ len(test_dataloader)test_acc/ len(test_dataloader)print(f\"\\n训练loss:{train_loss:.5f}测试loss:{test_loss:.5f},测试acc:{test_acc:.2f}%\\n\")train_time_end timer()total_train_time_model_0 print_train_time(start train_time_start,end train_time_end,device str(next(model_0.parameters()).device))Epoch: 0 训练 loss: 0.59039 测试 loss: 0.50954, 测试 acc: 82.04% Epoch: 1 训练 loss: 0.47633 测试 loss: 0.47989, 测试 acc: 83.20% Epoch: 2 训练 loss: 0.45503 测试 loss: 0.47664, 测试 acc: 83.43% cpu: 37.968 seconds##模型0：预测与评估创建一个函数，接受一个训练过的模型，一个DataLoader，一个损失函数和一个精度函数，使用模型对DataLoader中的数据进行预测，然后使用损失函数和精度函数来评估这些预测。torch.manual_seed(42)defeval_model(model:torch.nn.Module,data_loader:torch.utils.data.DataLoader,loss_fn:torch.nn.Module,accuracy_fn,device:torch.device device):\"\"\"Evaluatesagivenmodelonagivendataset.Args:model(torch.nn.Module):APyTorchmodelcapableofmakingpredictionsondata_loader.data_loader(torch.utils.data.DataLoader):Thetargetdatasettopredicton.loss_fn(torch.nn.Module):Thelossfunctionofmodel.accuracy_fn:Anaccuracyfunctiontocomparethemodelspredictionstothetruthlabels.device(str,optional):Targetdevicetocomputeon.Defaultstodevice.Returns:(dict):Resultsofmodelmakingpredictionsondata_loader.\"\"\"loss,acc 0,0model.eval()withtorch.inference_mode():forX,yindata_loader:#SenddatatothetargetdeviceX,y X.to(device),y.to(device)y_pred model(X)loss+ loss_fn(y_pred,y)acc+ accuracy_fn(y_true y,y_pred y_pred.argmax(dim 1))#Scalelossandaccloss/ len(data_loader)acc/ len(data_loader)return{\"model_name\":model.__class__.__name__,#onlyworkswhenmodelwascreatedwithaclass\"model_loss\":loss.item(),\"model_acc\":acc}#Calculatemodel0resultsontestdatasetmodel_0_results eval_model(model model_0,data_loader test_dataloader,loss_fn loss_fn,accuracy_fn accuracy_fn,device str(next(model_0.parameters()).device))model_0_results##模型1：添加非线性classFashionMNISTModelV1(nn.Module):def__init__(self,input_shape:int,hidden_units:int,output_shape:int):super().__init__()self.model nn.Sequential(nn.Flatten(),nn.Linear(in_features input_shape,out_features hidden_units),nn.ReLU(),nn.Linear(in_features hidden_units,out_features output_shape),nn.ReLU())defforward(self,x:torch.Tensor):returnself.model(x)接着实例化：torch.manual_seed(42)model_1 FashionMNISTModelV1(input_shape 784,hidden_units 10,output_shape len(class_names)).to(device)next(model_1.parameters()).device再次设置损失函数和优化器：loss_fn nn.CrossEntropyLoss()optimizer torch.optim.SGD(params model_1.parameters(),lr 0.1)将训练过程封装成函数：deftrain_step(model:torch.nn.Module,data_loader:torch.utils.data.DataLoader,loss_fn:torch.nn.Module,optimizer:torch.optim.Optimizer,accuracy_fn,device:torch.device device):train_loss,train_acc 0,0model.to(device)forbatch,(X,y)inenumerate(data_loader):#SenddatatoGPUX,y X.to(device),y.to(device)y_pred model(X)loss loss_fn(y_pred,y)train_loss+ losstrain_acc+ accuracy_fn(y_true y,y_pred y_pred.argmax(dim 1))optimizer.zero_grad()loss.backward()optimizer.step()train_loss/ len(data_loader)train_acc/ len(data_loader)print(f\"训练loss:{train_loss:.5f}训练accuracy:{train_acc:.2f}%\")deftest_step(data_loader:torch.utils.data.DataLoader,model:torch.nn.Module,loss_fn:torch.nn.Module,accuracy_fn,device:torch.device device):test_loss,test_acc 0,0model.to(device)model.eval()withtorch.inference_mode():forX,yindata_loader:X,y X.to(device),y.to(device)test_pred model(X)test_loss+ loss_fn(test_pred,y)test_acc+ accuracy_fn(y_true y,y_pred test_pred.argmax(dim 1))test_loss/ len(data_loader)test_acc/ len(data_loader)print(f\"Testloss:{test_loss:.5f}Testaccuracy:{test_acc:.2f}%\\n\")然后调用函数：torch.manual_seed(42)train_time_start timer()epochs 3forepochinrange(epochs):print(f\"Epoch:{epoch}\\n \")train_step(data_loader train_dataloader,model model_1,loss_fn loss_fn,optimizer optimizer,accuracy_fn accuracy_fn)test_step(data_loader test_dataloader,model model_1,loss_fn loss_fn,accuracy_fn accuracy_fn)train_time_end timer()total_train_time_model_1 print_train_time(start train_time_start,end train_time_end,device device)Epoch: 0 训练 loss: 1.09199 训练 accuracy: 61.34% Test loss: 0.95636 Test accuracy: 65.00% Epoch: 1 训练 loss: 0.78101 训练 accuracy: 71.93% Test loss: 0.72227 Test accuracy: 73.91% Epoch: 2 训练 loss: 0.67027 训练 accuracy: 75.94% Test loss: 0.68500 Test accuracy: 75.02% cuda: 40.312 seconds>注意：CUDA与CPU的训练时间在很大程度上取决于您使用的CPU/GPU的质量。>问题：“我使用了GPU，但我的模型没有更快地训练，为什么会这样？\">>答：一个原因可能是因为你的数据集和模型都很小（就像此例子正在使用的数据集和模型一样），使用GPU的好处被传输数据所花费的时间所抵消。在将数据从CPU内存（默认）复制到GPU内存之间存在一个小瓶颈。因此，对于较小的模型和数据集，CPU实际上可能是计算的最佳位置。对于较大的数据集和模型，GPU可以提供的计算速度通常远远超过获取数据的成本。但是，这在很大程度上取决于您使用的硬件。来评估一下模型：torch.manual_seed(42)model_1_results eval_model(model model_1,data_loader test_dataloader,loss_fn loss_fn,accuracy_fn accuracy_fn,device device)model_0_results,model_1_results在这种情况下，看起来向模型添加非线性使它的性能比基线模型更差。从事物的外观来看，模型似乎对训练数据过度拟合。过度拟合意味着我们的模型很好地学习了训练数据，但这些模式并没有推广到测试数据。修复过拟合的两种主要方法包括： 使用较小或不同的模型（某些模型比其他模型更适合某些类型的数据）。 使用更大的数据集（数据越多，模型学习可推广模式的机会就越大）。##模型2：CNN卷积神经网络的典型结构：`输入层 >[卷积层 >激活层 >池化层] >输出层` 其中，`[卷积层 >激活层 >池化层]`的内容可以根据要求放大和重复多次。下表是一个很好的通用指南，可以指导使用哪种模型（尽管也有例外）。类型一般使用模型示例: :: :: :结构化数据（如表格、行、列数据）梯度提升模型，随机森林，XGBoost`sklearn.ensemble`，`XGBoostlibrary`非结构化数据（如图像、音频、语言）卷积神经网络，Transformers`torchvision.models`，`HuggingFaceTransformers`使用`torch.nn`中的`nn.Conv2d()`和`nn.MaxPool2d()`层：classFashionMNISTModelV2(nn.Module):\"\"\"ModelarchitecturecopyingTinyVGGfrom:https://poloclub.github.io/cnn explainer/\"\"\"def__init__(self,input_shape:int,hidden_units:int,output_shape:int):super().__init__()self.block_1 nn.Sequential(nn.Conv2d(in_channels input_shape,out_channels hidden_units,kernel_size 3,#卷积核大小stride 1,#defaultpadding 1),#\"valid\"（无填充），\"same\"（输出与输入具有相同的形状），int表示特定的数字nn.ReLU(),nn.Conv2d(in_channels hidden_units,out_channels hidden_units,kernel_size 3,stride 1,padding 1),nn.ReLU(),nn.MaxPool2d(kernel_size 2,stride 2)#默认stride与池化窗口大小一致)self.block_2 nn.Sequential(nn.Conv2d(hidden_units,hidden_units,3,padding 1),nn.ReLU(),nn.Conv2d(hidden_units,hidden_units,3,padding 1),nn.ReLU(),nn.MaxPool2d(2))self.classifier nn.Sequential(nn.Flatten(),#这个in_features形状是是因为网络的每一层都会压缩和改变我们输入数据的形状。#此例中28*28池化一次变成14*14，再池化变成7*7nn.Linear(in_features hidden_units*7*7,out_features output_shape))defforward(self,x:torch.Tensor):x self.block_1(x)x self.block_2(x)x self.classifier(x)returnxtorch.manual_seed(42)model_2 FashionMNISTModelV2(input_shape 1,hidden_units 10,output_shape len(class_names)).to(device)model_2*该笔记只讲代码，不讲原理。*设置损失函数与优化器：loss_fn nn.CrossEntropyLoss()optimizer torch.optim.SGD(params model_2.parameters(),lr 0.1)进行训练：torch.manual_seed(42)train_time_start_model_2 timer()epochs 3forepochinrange(epochs):print(f\"Epoch:{epoch}\\n \")train_step(data_loader train_dataloader,model model_2,loss_fn loss_fn,optimizer optimizer,accuracy_fn accuracy_fn,device device)test_step(data_loader test_dataloader,model model_2,loss_fn loss_fn,accuracy_fn accuracy_fn,device device)train_time_end_model_2 timer()total_train_time_model_2 print_train_time(start train_time_start_model_2,end train_time_end_model_2,device device)Epoch: 0 训练 loss: 0.59430 训练 accuracy: 78.44% Test loss: 0.40483 Test accuracy: 85.33% Epoch: 1 训练 loss: 0.36487 训练 accuracy: 86.81% Test loss: 0.35013 Test accuracy: 87.23% Epoch: 2 训练 loss: 0.32794 训练 accuracy: 88.12% Test loss: 0.31522 Test accuracy: 88.63% cuda: 52.375 seconds看起来效果甚好。model_2_results eval_model(model model_2,data_loader test_dataloader,loss_fn loss_fn,accuracy_fn accuracy_fn,device device)model_2_results##比较并评估模型这里训练了三个模型： `model_0`：具有两个`nn.Linear()`层的基线模型。 `model_1`：与基线模型相同，除了在`nn.Linear()`层之间有`nn.ReLU()`层。 `model_2`：CNN模型，模仿CNNExplainer网站上的TinyVGG架构。使用`pandas`的`DataFrame`将数据展示：importpandasaspdcompare_results pd.DataFrame([model_0_results,model_1_results,model_2_results])compare_results[\"training_time\"] [total_train_time_model_0,total_train_time_model_1,total_train_time_model_2]compare_results结论： CNN（FashionMNISTModelV2）模型表现最好（损失最低，准确率最高），但训练时间最长。 基线模型（FashionMNISTModelV0）的性能优于model_1（FashionMNISTModelV1）。比较可视化：plt.figure(figsize (4,4))compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind \"barh\")plt.xlabel(\"accuracy(%)\")plt.ylabel(\"model\")可以使用许多不同的评估指标来解决分类问题，其中最直观的是混淆矩阵。混淆矩阵显示了分类模型在预测和真实标签之间混淆的地方。制作混淆矩阵：1.使用训练模型进行预测（混淆矩阵将预测与真实标签进行比较）。2.使用`torchmetrics.ConfusionMatrix`制作混淆矩阵。3.使用`mlxtend.plotting.plot_confusion_matrix()`绘制混淆矩阵。#1.使用训练模型进行预测y_preds []model_2.eval()withtorch.inference_mode():forX,yintest_dataloader:X,y X.to(device),y.to(device)y_logit model_2(X)y_pred torch.softmax(y_logit,dim 1).argmax(dim 1)y_preds.append(y_pred.cpu())y_pred_tensor torch.cat(y_preds)下载并导入`torchmetrics`和`mlxtend`：fromtorchmetricsimportConfusionMatrixfrommlxtend.plottingimportplot_confusion_matrix#2.设置混淆矩阵实例并将预测与目标进行比较confmat ConfusionMatrix(num_classes len(class_names),task 'multiclass')confmat_tensor confmat(preds y_pred_tensor,target test_data.targets)#3.绘图fig,ax plot_confusion_matrix(conf_mat confmat_tensor.numpy(),#matplotlib就像NumPy一样class_names class_names,#将行和列标签转换为类名figsize (4,4))可以看到模型表现相当好，因为大多数黑色方块对角线（理想模型将只在这些方块中有值，其他地方都是0）。混淆矩阵的信息能够进一步检查模型和数据，看看如何改进。"},"/StyleTransfer/ref_and_notes/pytorch_custom_datasets.html":{"title":"自定义数据集","content":"Reference:[PyTorchCustomDatasets](https://www.learnpytorch.io/04_pytorch_custom_datasets/)*该页面由JupyterNotebook生成，原文件于[Github](https://github.com/Fingsinz/StyleTransfer/tree/main/src/02.pytorch_learning/pytorch_custom_datasets.ipynb)*#导入包和设置设备importtorchfromtorchimportnndevice \"cuda\"iftorch.cuda.is_available()else\"cpu\"torch.__version__,device##获取并处理数据###获取数据首先，需要一些数据。这里使用的数据是[Food101数据集](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food 101/)的一个子集。 Food101包含101种不同食物的1000张图像，总计101000张图像（75750张训练图像和25250张测试图像）。为了自定义数据集，选取将3种食物开始：披萨、牛排和寿司。同时每个类并不是1000个图像，而是从随机的10%开始（从小处开始，必要时增加）。可以以下步骤下载数据集： 原始Food101数据集和论文网站。 笔记本（https://www.learnpytorch.io）提供。importrequestsimportzipfilefrompathlibimportPath#Setuppathtodatafolderdata_path Path(\"data/\")image_path data_path/\"pizza_steak_sushi\"#Iftheimagefolderdoesn'texist,downloaditandprepareit...ifimage_path.is_dir():print(f\"{image_path}directoryexists.\")else:print(f\"Didnotfind{image_path}directory,creatingone...\")image_path.mkdir(parents True,exist_ok True)#Downloadpizza,steak,sushidatawithopen(data_path/\"pizza_steak_sushi.zip\",\"wb\")asf:request requests.get(\"https://github.com/mrdbourke/pytorch deep learning/raw/main/data/pizza_steak_sushi.zip\")print(\"Downloadingpizza,steak,sushidata...\")f.write(request.content)#Unzippizza,steak,sushidatawithzipfile.ZipFile(data_path/\"pizza_steak_sushi.zip\",\"r\")aszip_ref:print(\"Unzippingpizza,steak,sushidata...\")zip_ref.extractall(image_path)data\\pizza_steak_sushi directory exists.*也可以自行下载数据集，并划分为train和test。*在此例中，有标准图像分类格式的披萨、牛排和寿司图像。图像分类格式在单独的目录中包含单独的图像类，标题为特定的类名。例如，pizza的所有图像都包含在pizza/目录中。```pizza_steak_sushi/train/pizza/steak/sushi/test/pizza/steak/sushi/```目标是将这个数据存储结构转化为PyTorch可用的数据集。现在试着打开几张图片看看：1.使用`pathlib.Path.glob()`获取所有图像路径，以查找所有以`.jpg`结尾的文件。2.使用Python的`random.choice()`选择一个随机的图像路径。3.使用`pathlib.Path.parent.stem`获取图像类名。4.使用`PIL.image.open()`（PIL代表PythonimageLibrary）打开随机图像路径。5.显示图像并打印一些元数据。importrandomfromPILimportImagerandom.seed(42)image_path_list list(image_path.glob(\"*/*/*.jpg\"))random_image_path random.choice(image_path_list)image_class random_image_path.parent.stemimg Image.open(random_image_path)print(f\"Randomimagepath:{random_image_path}\")print(f\"Imageclass:{image_class}\")print(f\"Imageheight:{img.height}\")print(f\"Imagewidth:{img.width}\")imgRandom image path: data\\pizza_steak_sushi\\test\\sushi\\2394442.jpg Image class: sushi Image height: 408 Image width: 512同样可以使用`matplotlib`：importnumpyasnpimportmatplotlib.pyplotaspltimg_as_array np.asarray(img)plt.figure(figsize (5,5))plt.imshow(img_as_array)plt.title(f\"Imageclass:{image_class}Imageshape:{img_as_array.shape} >[height,width,color_channels]\")plt.axis(False);###转化数据集表示现在希望将图像数据加载到PyTorch中。在PyTorch中使用图像数据之前，需要：1.把它变成张量（图像的数值表示）。2.将其转换为`torch.utils.data.dataset`，随后再转换为`torch.utils.data.DataLoader`，简称它们为Dataset和DataLoader。PyTorch有几种不同类型的预构建数据集和数据集加载器，具体取决于处理的问题。 视觉类：`torchvision.datasets`； 音频类：`torchaudio.datasets`； 文本类：`torchtext.datasets`； 推荐系统：`torchrec.datasets`。#导入包importtorchfromtorch.utils.dataimportDataLoaderfromtorchvisionimportdatasets,transforms使用`torchvision.transforms`转化数据：1.使用`transform.Resize()`调整图像的大小。2.使用`transform.RandomHorizontalFlip()`在水平方向上随机翻转图像（这可以被认为是一种数据增强形式，因为它会人为地改变我们的图像数据）。3.使用`transform.ToTensor()`将图像从PIL图像转换为PyTorch张量。可以使用`torchvision.transforms.Compose()`编译所有这些步骤。data_transform transforms.Compose([transforms.Resize(size (64,64)),transforms.RandomHorizontalFlip(p 0.5),#p为翻转的概率transforms.ToTensor()])接下来试试转换的效果：defplot_transformed_images(image_paths,transform,n 3,seed 42):random.seed(seed)random_image_paths random.sample(image_paths,k n)forimage_pathinrandom_image_paths:withImage.open(image_path)asf:fig,ax plt.subplots(1,2)ax[0].imshow(f)ax[0].set_title(f\"Original\\nSize:{f.size}\")ax[0].axis(\"off\")#permute()会改变图像的形状以适应matplotlib#(PyTorchdefaultis[C,H,W]butMatplotlibis[H,W,C])transformed_image transform(f).permute(1,2,0)ax[1].imshow(transformed_image)ax[1].set_title(f\"Transformed\\nSize:{transformed_image.shape}\")ax[1].axis(\"off\")fig.suptitle(f\"Class:{image_path.parent.stem}\",fontsize 16)plot_transformed_images(image_path_list,transform data_transform,n 3)##使用ImageFolder加载数据目前数据是标准的图像分类格式，所以可以使用`torchvision.datasets.ImageFolder`类。将目标图像目录的文件路径以及我们想要对图像执行的一系列转换传递给它。fromtorchvisionimportdatasetstrain_dir image_path/\"train\"test_dir image_path/\"test\"train_data datasets.ImageFolder(root train_dir,transform data_transform,target_transform None)#转换在标签上执行test_data datasets.ImageFolder(root test_dir,transform data_transform)print(f\"Traindata:\\n{train_data}\\nTestdata:\\n{test_data}\")Train data: Dataset ImageFolder Number of datapoints: 225 Root location: data\\pizza_steak_sushi\\train StandardTransform Transform: Compose( Resize(size (64, 64), interpolation bilinear, max_size None, antialias True) RandomHorizontalFlip(p 0.5) ToTensor() ) Test data: Dataset ImageFolder Number of datapoints: 75 Root location: data\\pizza_steak_sushi\\test StandardTransform Transform: Compose( Resize(size (64, 64), interpolation bilinear, max_size None, antialias True) RandomHorizontalFlip(p 0.5) ToTensor() )现在PyTorch已经注册了数据集。通过检查`classes`和`class_to_idx`属性以及训练集和测试集的长度来检查一下：class_names train_data.classesclass_dict train_data.class_to_idxclass_names,class_dict,len(train_data),len(test_data)再检查一下训练数据和测试数据：img,label train_data[0][0],train_data[0][1]img.shape,img.dtype,label,type(label)图像现在是张量的形式（形状为`[3,64,64] >[通道,高度,宽度]`），标签是与特定类相关的整数形式（由class_to_idx属性引用）。还需要将数据转换为DataLoader。将Dataset转换为DataLoader，模型可以遍历并学习样本和目标（特征和标签）之间的关系。为了简单起见，将使用`batch_size 1`和`num_workers 1`。 `batch_size`已经解释过，批量大小。 `num_workers`定义将创建多少个子进程来加载数据，`num_workers`设置的值越高，PyTorch在加载数据时使用的计算能力就越强。通常通过Python的`os.cpu_count()`将其设置为CPU总数，确保DataLoader使用尽可能多的内核来加载数据。fromtorch.utils.dataimportDataLoadertrain_dataloader DataLoader(dataset train_data,batch_size 1,num_workers 1,shuffle True)test_dataloader DataLoader(dataset test_data,batch_size 1,num_workers 1,shuffle False)最后获取`train_dataloader`中每个可迭代项的Shape信息：img,label next(iter(train_dataloader))img.shape,label.shape##使用自定义DataSet类加载数据如果像`torchvision.datasets.ImageFolder()`这样的预构建数据集创建器不存在，或者针对具体问题的解决方案根本不存在，那么可以自定义一个。创建自定义方式来加载Dataset的优缺点： 优点：可以用几乎任何东西创建数据集，不限于PyTorch预构建的Dataset函数。 缺点：尽管可以用几乎任何东西创建一个数据集，但这并不意味着它就有效；同时会导致编写更多代码，这可能容易出现错误或性能问题。实际操作是继承`torch.utils.data.Dataset`（PyTorch中所有Dataset的基类）来复制`torchvision.datasets.ImageFolder()`。从导入需要的模块开始： Python处理目录的`os`（数据存储在目录中）。 Python处理文件路径的`pathlib`（每个图像都有一个唯一的文件路径）。 PyTorch的所有的东西。 用于加载图像的PIL的Image类。 继承`torch.utils.data.Dataset`创建自定义数据集。 `torchvision.transforms`把图像变成张量。 来自Python的typing模块的各种类型，为代码添加类型提示。importosimportpathlibimporttorchfromPILimportImagefromtorch.utils.dataimportDatasetfromtorchvisionimporttransformsfromtypingimportTuple,Dict,List###获取数据类名首先实现获取数据类名的函数，获取如`['pizza','steak','sushi'],{'pizza':0,'steak':1,'sushi':2}`的信息：deffind_classes(directory:str) >Tuple[List[str],Dict[str,int]]:\"\"\"Findstheclassfoldernamesinatargetdirectory.Assumestargetdirectoryisinstandardimageclassificationformat.Args:directory(str):targetdirectorytoloadclassnamesfrom.Returns:Tuple[List[str],Dict[str,int]]:(list_of_class_names,dict(class_name:idx...))Example:find_classes(\"food_images/train\")>>>([\"class_1\",\"class_2\"],{\"class_1\":0,...})\"\"\"#1.通过扫描目标目录获取类名classes sorted(entry.nameforentryinos.scandir(directory)ifentry.is_dir())#2.如果找不到类名，则引发错误ifnotclasses:raiseFileNotFoundError(f\"Couldn'tfindanyclassesin{directory}.\")#3.创建索引标签的字典class_to_idx {cls_name:ifori,cls_nameinenumerate(classes)}returnclasses,class_to_idx测试一下：find_classes(train_dir)###构建自定义数据集类将构建一个类来复刻`torchvision.datasets.ImageFolder()`的功能。分析如下：1.继承`torch.utils.data.Dataset`。2.用`targ_dir`参数（目标数据目录）和`transform`参数初始化子类。3.创建属性：目标图像路径、`transform`（可以是`None`），`classes`和`class_to_idx`（来自`find_classes()`函数）。4.创建一个函数从文件中加载图像并返回它们，可以使用`PIL`或`torchvision.io`。5.重写`torch.utils.data.Dataset`的`__len__`方法，返回数据集中的样本数量。（不必需）6.重写`torch.utils.data.Dataset`的`__getitem__`方法以返回数据集中的单个样本。（必需）fromtorch.utils.dataimportDatasetclassCustomImageFolder(Dataset):def__init__(self,targ_dir:str,transform None) >None:self.paths list(pathlib.Path(targ_dir).glob(\"*/*.jpg\"))self.transform transformself.classes,self.class_to_idx find_classes(targ_dir)defload_image(self,index:int) >Image.Image:image_path self.paths[index]returnImage.open(image_path)def__len__(self) >int:returnlen(self.paths)def__getitem__(self,index:int) >Tuple[torch.Tensor,int]:img self.load_image(index)class_name self.paths[index].parent.name#要求data_folder/class_name/image.jpegclass_idx self.class_to_idx[class_name]ifself.transform:returnself.transform(img),class_idx#(X,y)else:returnimg,class_idx#(X,y)重新设置数据转变器：train_transforms transforms.Compose([transforms.Resize((64,64)),transforms.RandomHorizontalFlip(p 0.5),transforms.ToTensor()])test_transforms transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor()])接着实例化数据：train_data_custom CustomImageFolder(targ_dir train_dir,transform train_transforms)test_data_custom CustomImageFolder(targ_dir test_dir,transform test_transforms)train_data_custom.classes,train_data_custom.class_to_idx,len(train_data_custom),len(test_data_custom)###测试`__getitem__`直接上函数：#1.TakeinaDatasetaswellasalistofclassnamesdefdisplay_random_images(dataset:torch.utils.data.dataset.Dataset,classes:List[str] None,n:int 10,display_shape:bool True,seed:int None):#2.Adjustdisplayifntoohighifn>10:n 10display_shape Falseprint(f\"Fordisplaypurposes,nshouldn'tbelargerthan10,settingto10andremovingshapedisplay.\")#3.Setrandomseedifseed:random.seed(seed)#4.Getrandomsampleindexesrandom_samples_idx random.sample(range(len(dataset)),k n)#5.Setupplotplt.figure(figsize (16,5))#6.Loopthroughsamplesanddisplayrandomsamplesfori,targ_sampleinenumerate(random_samples_idx):targ_image,targ_label dataset[targ_sample][0],dataset[targ_sample][1]#7.Adjustimagetensorshapeforplotting:[color_channels,height,width] >[color_channels,height,width]targ_image_adjust targ_image.permute(1,2,0)#Plotadjustedsamplesplt.subplot(1,n,i+1)plt.imshow(targ_image_adjust)plt.axis(\"off\")ifclasses:title f\"class:{classes[targ_label]}\"ifdisplay_shape:title title+f\"\\nshape:{targ_image_adjust.shape}\"plt.title(title)调用测试：#DisplayrandomimagesfromImageFoldercreatedDatasetdisplay_random_images(train_data,n 5,classes class_names,seed None)display_random_images(train_data_custom,n 5,classes class_names,seed None)#Trysettingtheseedforreproducibleimages看起来生效。###把自定义数据类变成DataLoader通过`CustomImageFolder`类，可以将原始图像转换为数据集（特征映射到标签或X映射到y）。因为自定义数据集的继承`torch.utils.data`，所以可以通过`torch.utils.data.DataLoader()`直接使用它们。fromtorch.utils.dataimportDataLoadertrain_dataloader_custom DataLoader(dataset train_data_custom,batch_size 1,num_workers 0,shuffle True)test_dataloader_custom DataLoader(dataset test_data_custom,batch_size 1,num_workers 0,shuffle False)最后获取`train_dataloader_custom`中每个可迭代项的Shape信息：img_custom,label_custom next(iter(train_dataloader_custom))img_custom.shape,label_custom.shape##其他形式的转换（数据增强）目前已经看到了对数据的一些变换，但还有更多，可以在[torchvision.transforms文档](https://pytorch.org/vision/stable/transforms.html)中查阅。变换的目的是以某种方式改变图像，如裁剪、随机删除部分、随即旋转等等。进行这类转换通常被称为数据增强。数据增强是通过人为地增加训练集的多样性来改变数据的过程。对图像执行数据增强的许多示例在：[https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html)研究表明，随机变换（如`transform.RandAugment()`和`transform.TrivialAugmentWide()`）通常比手工选择的变换表现得更好。在`transforms.TrivialAugmentWide()`中需要注意的主要参数是`num_magnitude_bins 31`。 它定义了将选择多少范围的强度值来应用某个转换，0表示没有范围，31表示最大范围（最高强度的最高机会）。将`transforms.TrivialAugmentWide()`合并到`transforms.Compose()`中：fromtorchvisionimporttransformstrain_transforms transforms.Compose([transforms.Resize((224,224)),transforms.TrivialAugmentWide(num_magnitude_bins 31),transforms.ToTensor()])test_transforms transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])看看效果：image_path_list list(image_path.glob(\"*/*/*.jpg\"))plot_transformed_images(image_paths image_path_list,transform train_transforms,n 3,seed None)##模型0：没有数据增强的TinyVGG定义transform：simple_transform transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor(),])加载数据：importosfromtorchvisionimportdatasetsfromtorch.utils.dataimportDataLoadertrain_data_simple datasets.ImageFolder(root train_dir,transform simple_transform)test_data_simple datasets.ImageFolder(root test_dir,transform simple_transform)BATCH_SIZE 32NUM_WORKERS 3#个人修改，不全部使用print(f\"batchsize:{BATCH_SIZE},workers:{NUM_WORKERS}\")train_dataloader_simple DataLoader(train_data_simple,batch_size BATCH_SIZE,shuffle True,num_workers NUM_WORKERS)test_dataloader_simple DataLoader(test_data_simple,batch_size BATCH_SIZE,shuffle False,num_workers NUM_WORKERS)len(train_dataloader_simple),len(test_dataloader_simple)batch size: 32, workers: 3直接创建模型，同视觉一节：classTinyVGG(nn.Module):\"\"\"ModelarchitecturecopyingTinyVGGfrom:https://poloclub.github.io/cnn explainer/\"\"\"def__init__(self,input_shape:int,hidden_units:int,output_shape:int) >None:super().__init__()self.conv_block_1 nn.Sequential(nn.Conv2d(input_shape,hidden_units,kernel_size 3,stride 1,padding 1),nn.ReLU(),nn.Conv2d(hidden_units,hidden_units,kernel_size 3,stride 1,padding 1),nn.ReLU(),nn.MaxPool2d(kernel_size 2,stride 2))self.conv_block_2 nn.Sequential(nn.Conv2d(hidden_units,hidden_units,kernel_size 3,padding 1),nn.ReLU(),nn.Conv2d(hidden_units,hidden_units,kernel_size 3,padding 1),nn.ReLU(),nn.MaxPool2d(2))self.classifier nn.Sequential(nn.Flatten(),nn.Linear(in_features hidden_units*16*16,out_features output_shape))defforward(self,x:torch.Tensor):returnself.classifier(self.conv_block_2(self.conv_block_1(x)))torch.manual_seed(42)model_0 TinyVGG(input_shape 3,#(3,RGB)hidden_units 10,output_shape len(train_data.classes)).to(device)model_0###使用`torchinfo`了解模型形状需要安装`torchinfo`库：`pipinstalltorchinfo`。使用：`summary(model,input_size (batch_size,model_shape))`fromtorchinfoimportsummarysummary(model_0,input_size [1,3,64,64])#对示例输入大小进行测试传递`torchinfo.summary()`的输出提供了关于模型的大量信息。 `Totalparams`是模型中参数的总数； `EstimatedTotalSize`是估计的总大小（MB）。还可以看到输入和输出形状的变化，因为特定`input_size`的数据在模型中移动。###封装每step的训练和测试函数deftrain_step(model:torch.nn.Module,dataloader:torch.utils.data.DataLoader,loss_fn:torch.nn.Module,optimizer:torch.optim.Optimizer):model.train()train_loss,train_acc 0,0forbatch,(X,y)inenumerate(dataloader):X,y X.to(device),y.to(device)y_pred model(X)loss loss_fn(y_pred,y)train_loss+ loss.item()optimizer.zero_grad()loss.backward()optimizer.step()y_pred_class torch.argmax(torch.softmax(y_pred,dim 1),dim 1)train_acc+ (y_pred_class y).sum().item()/len(y_pred)train_loss train_loss/len(dataloader)train_acc train_acc/len(dataloader)returntrain_loss,train_accdeftest_step(model:torch.nn.Module,dataloader:torch.utils.data.DataLoader,loss_fn:torch.nn.Module):model.eval()test_loss,test_acc 0,0withtorch.inference_mode():forbatch,(X,y)inenumerate(dataloader):X,y X.to(device),y.to(device)test_pred_logits model(X)loss loss_fn(test_pred_logits,y)test_loss+ loss.item()test_pred_labels test_pred_logits.argmax(dim 1)test_acc+ ((test_pred_labels y).sum().item()/len(test_pred_labels))test_loss test_loss/len(dataloader)test_acc test_acc/len(dataloader)returntest_loss,test_acc###封装训练函数deftrain(model:torch.nn.Module,train_dataloader:torch.utils.data.DataLoader,test_dataloader:torch.utils.data.DataLoader,optimizer:torch.optim.Optimizer,loss_fn:torch.nn.Module nn.CrossEntropyLoss(),epochs:int 5):results {\"train_loss\":[],\"train_acc\":[],\"test_loss\":[],\"test_acc\":[]}forepochinrange(epochs):train_loss,train_acc train_step(model model,dataloader train_dataloader,loss_fn loss_fn,optimizer optimizer)test_loss,test_acc test_step(model model,dataloader test_dataloader,loss_fn loss_fn)print(f\"Epoch:{epoch+1}\"f\"train_loss:{train_loss:.4f}\"f\"train_acc:{train_acc:.4f}\"f\"test_loss:{test_loss:.4f}\"f\"test_acc:{test_acc:.4f}\")results[\"train_loss\"].append(train_loss.item()ifisinstance(train_loss,torch.Tensor)elsetrain_loss)results[\"train_acc\"].append(train_acc.item()ifisinstance(train_acc,torch.Tensor)elsetrain_acc)results[\"test_loss\"].append(test_loss.item()ifisinstance(test_loss,torch.Tensor)elsetest_loss)results[\"test_acc\"].append(test_acc.item()ifisinstance(test_acc,torch.Tensor)elsetest_acc)returnresults###构建训练和测试循环torch.manual_seed(42)torch.cuda.manual_seed(42)NUM_EPOCHS 5model_0 TinyVGG(input_shape 3,#3,RGBhidden_units 10,output_shape len(train_data.classes)).to(device)loss_fn nn.CrossEntropyLoss()optimizer torch.optim.Adam(params model_0.parameters(),lr 0.001)fromtimeitimportdefault_timerastimerstart_time timer()model_0_results train(model model_0,train_dataloader train_dataloader_simple,test_dataloader test_dataloader_simple,optimizer optimizer,loss_fn loss_fn,epochs NUM_EPOCHS)end_time timer()print(f\"耗时:{end_time start_time:.3f}seconds\")Epoch: 1 train_loss: 1.1078 train_acc: 0.2578 test_loss: 1.1362 test_acc: 0.2604 Epoch: 2 train_loss: 1.0846 train_acc: 0.4258 test_loss: 1.1622 test_acc: 0.1979 Epoch: 3 train_loss: 1.1153 train_acc: 0.2930 test_loss: 1.1695 test_acc: 0.1979 Epoch: 4 train_loss: 1.0990 train_acc: 0.2891 test_loss: 1.1343 test_acc: 0.1979 Epoch: 5 train_loss: 1.0989 train_acc: 0.2930 test_loss: 1.1435 test_acc: 0.1979 耗时: 46.904 seconds效果很差，试试可视化损失，封装函数：defplot_loss_curves(results:Dict[str,List[float]]):\"\"\"Plotstrainingcurvesofaresultsdictionary.Args:results(dict):dictionarycontaininglistofvalues,e.g.{\"train_loss\":[...],\"train_acc\":[...],\"test_loss\":[...],\"test_acc\":[...]}\"\"\"#Getthelossvaluesoftheresultsdictionary(trainingandtest)loss results['train_loss']test_loss results['test_loss']#Gettheaccuracyvaluesoftheresultsdictionary(trainingandtest)accuracy results['train_acc']test_accuracy results['test_acc']#Figureouthowmanyepochstherewereepochs range(len(results['train_loss']))#Setupaplotplt.figure(figsize (10,3))#Plotlossplt.subplot(1,2,1)plt.plot(epochs,loss,label 'train_loss')plt.plot(epochs,test_loss,label 'test_loss')plt.title('Loss')plt.xlabel('Epochs')plt.legend()#Plotaccuracyplt.subplot(1,2,2)plt.plot(epochs,accuracy,label 'train_accuracy')plt.plot(epochs,test_accuracy,label 'test_accuracy')plt.title('Accuracy')plt.xlabel('Epochs')plt.legend();调用：plot_loss_curves(model_0_results)##探究损失函数查看训练和测试损失曲线是查看模型是否过拟合的好方法。 过拟合模型是在训练集上比在验证/测试集上表现更好，训练损失远低于测试损失。 当训练和测试损失没有想要的那么低时，这被认为是欠拟合。训练和测试损失曲线的理想位置是它们彼此紧密排列。###处理过拟合由于过拟合的主要问题是模型太好地拟合训练数据，防止过拟合的一种常见技术称为正则化。预防过拟合的操作： 使用更多数据：拥有更多的数据使模型有更多的机会学习样式，这些样式可能更容易推广到新的示例。 简化模型：如果当前模型已经过拟合训练数据，则模型可能过于复杂。这意味着它对数据的模式学习得太好，无法很好地推广到看不见的数据。简化模型的一种方法是减少它使用的层数或减少每层中隐藏单元的数量。 数据增强：人为地为数据添加了更多的多样性。如果模型能够学习增强数据中的模式，则模型可能能够更好地概括看不见的数据。 迁移学习：迁移学习涉及利用一个模型已经学会使用的模式（也称为预训练权重）作为您自己任务的基础。在此例子中，可以使用一个在各种图像上预训练的计算机视觉模型，然后稍微调整它，使其更专门用于食物图像。 使用dropout层：dropout层随机删除神经网络中隐藏层之间的连接，有效地简化了模型，也使剩余的连接更好。 使用衰减的学习率：在模型训练时慢慢降低学习率。越接近收敛，越希望权重更新越小。 使用早停：早期停止在模型训练开始过度拟合之前停止。例如，假设模型的损失在过去10（这个数字是任意的）个epoch中停止下降，可能希望在这里停止模型训练，并使用损失最低的模型权重（10epoch之前）。###处理欠拟合当模型拟合不足时，它被认为对训练集和测试集的预测能力较差。从本质上讲，欠拟合模型将无法将损失值降低到期望的水平。目前的损失曲线，认为TinyVGG模型model_0对数据拟合不足。处理欠拟合背后的主要思想是提高模型的预测能力。处理欠拟合的操作： 增加模型隐藏层或隐层神经元：如果模型拟合不足，可能没有足够的能力来学习所需的模式/权重/数据表示来进行预测。为模型添加更多预测能力的一种方法是增加这些层中隐藏层/单元的数量。 调整学习率：也许模型的学习率太高了。而且它试图在每个时期更新权重太多，从而无法学习任何东西。在这种情况下，可以降低学习率。 使用迁移学习：迁移学习能够防止过拟合和欠拟合。它涉及到使用以前工作模型中的模式，并根据当前问题进行调整。 训练更长时间：模型可能需要更多的时间来学习数据的表示。如果你在小型实验中发模型没有学习到任何东西，也许让它训练更多的epoch可能会带来更好的性能。 减少正则化：也许因为试图防止过度拟合导致模型是欠拟合的。##模型1：数据增强后的TinyVGG修改数据transform：train_transform_trivial_augment transforms.Compose([transforms.Resize((64,64)),transforms.TrivialAugmentWide(num_magnitude_bins 31),transforms.ToTensor()])test_transform transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor()])再次处理数据集：train_data_augmented datasets.ImageFolder(train_dir,transform train_transform_trivial_augment)test_data_simple datasets.ImageFolder(test_dir,transform test_transform)train_data_augmented,test_data_simple转成DataLoader：BATCH_SIZE 32NUM_WORKERS 3torch.manual_seed(42)train_dataloader_augmented DataLoader(train_data_augmented,batch_size BATCH_SIZE,shuffle True,num_workers NUM_WORKERS)test_dataloader_simple DataLoader(test_data_simple,batch_size BATCH_SIZE,shuffle False,num_workers NUM_WORKERS)重新实例化模型：torch.manual_seed(42)model_1 TinyVGG(input_shape 3,hidden_units 10,output_shape len(train_data_augmented.classes)).to(device)model_1开始训练：torch.manual_seed(42)torch.cuda.manual_seed(42)NUM_EPOCHS 5loss_fn nn.CrossEntropyLoss()optimizer torch.optim.Adam(params model_1.parameters(),lr 0.001)start_time timer()model_1_results train(model model_1,train_dataloader train_dataloader_augmented,test_dataloader test_dataloader_simple,optimizer optimizer,loss_fn loss_fn,epochs NUM_EPOCHS)end_time timer()print(f\"耗时:{end_time start_time:.3f}seconds\")Epoch: 1 train_loss: 1.1073 train_acc: 0.2500 test_loss: 1.1060 test_acc: 0.2604 Epoch: 2 train_loss: 1.0793 train_acc: 0.4258 test_loss: 1.1380 test_acc: 0.2604 Epoch: 3 train_loss: 1.0805 train_acc: 0.4258 test_loss: 1.1684 test_acc: 0.2604 Epoch: 4 train_loss: 1.1287 train_acc: 0.3047 test_loss: 1.1618 test_acc: 0.2604 Epoch: 5 train_loss: 1.0895 train_acc: 0.4258 test_loss: 1.1470 test_acc: 0.2604 耗时: 47.582 seconds看起来效果也不好，绘制损失趋势图：plot_loss_curves(model_1_results)##比较并评估模型使用`pandas`并绘图：importpandasaspdmodel_0_df pd.DataFrame(model_0_results)model_1_df pd.DataFrame(model_1_results)#Setupaplotplt.figure(figsize (15,8))#Getnumberofepochsepochs range(len(model_0_df))#Plottrainlossplt.subplot(2,2,1)plt.plot(epochs,model_0_df[\"train_loss\"],label \"Model0\")plt.plot(epochs,model_1_df[\"train_loss\"],label \"Model1\")plt.title(\"TrainLoss\")plt.xlabel(\"Epochs\")plt.legend()#Plottestlossplt.subplot(2,2,2)plt.plot(epochs,model_0_df[\"test_loss\"],label \"Model0\")plt.plot(epochs,model_1_df[\"test_loss\"],label \"Model1\")plt.title(\"TestLoss\")plt.xlabel(\"Epochs\")plt.legend()#Plottrainaccuracyplt.subplot(2,2,3)plt.plot(epochs,model_0_df[\"train_acc\"],label \"Model0\")plt.plot(epochs,model_1_df[\"train_acc\"],label \"Model1\")plt.title(\"TrainAccuracy\")plt.xlabel(\"Epochs\")plt.legend()#Plottestaccuracyplt.subplot(2,2,4)plt.plot(epochs,model_0_df[\"test_acc\"],label \"Model0\")plt.plot(epochs,model_1_df[\"test_acc\"],label \"Model1\")plt.title(\"TestAccuracy\")plt.xlabel(\"Epochs\")plt.legend();最后封装一个函数，使得可以外部输入图片路径，然后进行预测：importtorchvisiondefpred_and_plot_image(model:torch.nn.Module,image_path:str,class_names:List[str] None,transform None,device:torch.device device):\"\"\"Makesapredictiononatargetimageandplotstheimagewithitsprediction.\"\"\"#1.加载图像并将张量值转换为float32target_image torchvision.io.read_image(str(image_path)).type(torch.float32)#2.将图像像素值除以255，得到[0,1]之间的值target_image target_image/255.#3.作数据转换iftransform:target_image transform(target_image)#4.确保模型在目标设备上model.to(device)#5.打开模型评估模式model.eval()withtorch.inference_mode():#为图像添加额外的维度target_image target_image.unsqueeze(dim 0)#对具有额外维度的图像进行预测，并将其发送到目标设备target_image_pred model(target_image.to(device))#6.转换logits >预测概率target_image_pred_probs torch.softmax(target_image_pred,dim 1)#7.转换预测概率 >预测标签target_image_pred_label torch.argmax(target_image_pred_probs,dim 1)#8.将图像与预测和预测概率一起绘制plt.imshow(target_image.squeeze().permute(1,2,0))#确保它的大小适合matplotlibifclass_names:title f\"Pred:{class_names[target_image_pred_label.cpu()]}Prob:{target_image_pred_probs.max().cpu():.3f}\"else:title f\"Pred:{target_image_pred_label}Prob:{target_image_pred_probs.max().cpu():.3f}\"plt.title(title)plt.axis(False);test_img_path \"data/pizza_steak_sushi/test/pizza/1687143.jpg\"custom_image_transform transforms.Compose([transforms.Resize((64,64))])pred_and_plot_image(model model_1,image_path test_img_path,class_names class_names,transform custom_image_transform,device device)"},"/StyleTransfer/ref_and_notes/pytorch_install.html":{"title":"PyTorch 安装","content":" title: PyTorch 安装 keywords: PyTorch desc: PyTorch 安装 date: 2025 02 07 id: pytorch Reference: [Zero to Mastery Learn PyTorch for Deep Learning](https://www.learnpytorch.io/) ## PyTorch 基本环境搭建 1. 创建并激活环境 ```bat python m venv [venv name] [venv name]\\Scripts\\activate ``` 2. 安装 Pytorch ```bat pip install torch torchvision torchaudio ``` 验证 Pytorch 安装，出现版本号则为正常。 ## PyTorch GPU 环境搭建 在搭建虚拟环境后，如果需要在 GPU 上运行，需要安装 PyTorch GPU 版本。 1. 确定自己的 GPU CUDA 版本。 ```bat nvidia smi ``` 2. 下载对应的 PyTorch GPU 版本。[官方引导下载](https://pytorch.org/get started/locally/) 附镜像页面链接： PyTorch官方镜像 [Torch](https://download.pytorch.org/whl/torch/) [TorchVision](https://download.pytorch.org/whl/torchvision/) [TorchAudio](https://download.pytorch.org/whl/torchaudio/) [阿里云镜像源](https://mirrors.aliyun.com/pytorch wheels/) 支持的 CUDA：10.0、10.1、10.2、11.0、11.1、11.3、11.5、11.6、11.7、11.8、12.1 3. 检测是否可用。 ```python import torch print(torch.cuda.is_available()) ```"}}
{"/StyleTransfer/ref_and_notes/index.html":{"title":"参考文献及笔记","content":" title: 参考文献及笔记 keywords: desc: 参考文献阅读及其代码测试 date: 2025 01 16 class: heading_no_counter ### TODO 1. Meta Networks for Neural Style Transfer 2. 学习 PyTorch 框架(2) \t [ ] 迁移学习 [ ] 实验跟踪 [ ] 论文复制 [ ] 模型部署 3. ...... ### 涉及文献 1. [VGG——Very Deep Convolutional Networks for Large Scale Image Recognition](https://arxiv.org/abs/1409.1556) 2. [ResNet——Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) 3. [U Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) 4. [GAN——Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) 5. [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) 6. [PatchGAN 起源 —— Image to Image Translation with Conditional Adversarial Networks](https://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image To Image_Translation_With_CVPR_2017_paper.pdf) 7. [将PatchGAN扩展为多尺度（3级金字塔）—— High Resolution Image Synthesis and Semantic Manipulation with Conditional GANs](https://arxiv.org/abs/1711.11585) 8. [Unpaired Image to Image Translation using Cycle Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593) 9. [Meta Networks for Neural Style Transfer](https://arxiv.org/abs/1709.04111) ### 风格迁移实战代码参考 1. Image Style Transfer Using Convolutional Neural Networks：[https://github.com/b06b01073/style transfer/tree/main](https://github.com/b06b01073/style transfer/tree/main)"},"/StyleTransfer/ref_and_notes/resnet.html":{"title":"ResNet：残差网络","content":" title: ResNet：残差网络 keywords: ResNet desc: ResNet文献及笔记 date: 2025 03 02 id: ref_ResNet [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) *He K , Zhang X , Ren S ,et al.Deep Residual Learning for Image Recognition[J].IEEE, 2016.DOI:10.1109/CVPR.2016.90.* > Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers 8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR 10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. **摘要**：更深的神经网络会更难进行训练。我们提出一个残差学习框架，可以简化比以前使用的网络深度更深的网络的训练。我们明确地将层重新表述为参考层输入的学习残差函数，而不是学习未参考的函数。我们提供了全面的经验证据，表明这些残差网络更容易优化，并且可以从相当大的深度中获得精度。在 ImageNet 数据集上，我们评估了深度高达 152 层的残差网络——比 VGG 网络深 8 倍，但仍然具有较低的复杂性。这些残差网络的集合在 ImageNet测试集上的误差达到 3.57%。该结果在 ILSVRC 2015 分类任务中获得第一名。我们还对 100 层和 1000 层的 CIFAR 10 进行了分析。表征的深度对于许多视觉识别任务至关重要。仅仅由于我们的深度表示，我们在 COCO 对象检测数据集上获得了 28% 的相对改进。深度残差网络是我们提交 ILSVRC 和 COCO 2015 竞赛的基础，我们还在 ImageNet 检测，ImageNet 本地化，COCO 检测和 COCO 分割任务中获得了第一名。 ## 残差块模型 通过引入残差块（Residual Block）和快捷连接（Shortcut Connection），解决了深度神经网络训练中的退化问题（随着深度增加，训练误差不降反升）。 1. **残差块**：将网络层的映射目标从 **直接学习目标函数 $\\mathcal{H}(x)$** 变成 **学习残差函数 $\\mathcal{F}(x) \\mathcal{H}(x) x$**，最终输出为 $\\mathcal{F}(x)+x$。 若恒等映射（Identity Mapping）是最优解，则残差函数更容易趋近于零，简化了优化过程。 > 注：恒等映射是一个输入与输出相同的函数，即 $f(x) x$。在神经网络中，它表现为直接将输入传递到输出，不做任何变换。 2. **快捷连接**：在堆叠层之间添加跨层连接，直接将输入与输出相加，无需额外参数。 快捷连接直接将输入 $x$ 加到残差函数 $\\mathcal{F}(x)$ 的输出上，相当于强制网络仅学习输入与目标之间的 **残差**（差异），而非完整的映射。 整个网络仍然可以通过反向传播的 SGD 进行端到端训练。 ![残差块](../static/images/ResNet/fig1.png) ## 残差网络理论 论文表明： 极深残差网络很容易优化，但是当深度增加时，对应的“普通”网络（简单地堆叠层）会表现出更高的训练误差； 深度残差网络可以很容易地从显着增加的深度中获得准确度增益，产生的结果比以前的网络要好得多。 与先前工作的关键区别： 残差函数 vs 完整映射：ResNet显式学习残差（$\\mathcal{F}(x) \\mathcal{H}(x) x$），而传统网络直接学习 $\\mathcal{H}(x) $。 无门控 vs 门控：高速公路网络依赖参数化的门控函数，而ResNet通过恒等映射，无需参数，保持信息无损传递。 ### 残差学习 如果假设多个非线性层可以渐近逼近复杂函数 $\\mathcal{H}(x)$，则等价于假设这多个非线性层可以渐近逼近残差函数。残差学习将其重新定义为 $\\mathcal{F}(x) \\mathcal{H}(x) x$（假设输入和输出具有相同的维数）。 因此，不需要期望堆叠层来近似 $\\mathcal{H}(x)$，而实际学习的是残差函数，最终输出为 $y \\mathcal{F}(x)+x$。 若恒等映射 $\\mathcal{H}(x) x$ 是最优解，则残差函数 $\\mathcal{F}(x)$ 只需趋近于零，比直接学习完整映射更简单。 在实际场景中，恒等映射不太可能是最优的，但我们的重新表达可能有助于解决这个问题。 如果最优函数更接近于恒等映射而不是零映射，求解器应该更容易找到参考恒等映射的扰动，而不是学习函数作为新函数。 ### 快捷连接 输入 $x$ 直接跨过堆叠的层，与残差函数 $\\mathcal{F}(x)$ 的输出相加。 当输入输出维度相同时，快捷连接直接传递 $x$（无参数）；维度不同时，快捷连接执行线性投影 $W_s$ 以匹配维度： $$ \\begin{aligned} \\text{y} &\\mathcal{F}(x)+x,& 维度相同\\\\ \\text{y} &\\mathcal{F}(x)+W_s x,& 维度不同 \\end{aligned} $$ 残差块中，ReLU 激活函数位于残差函数 $\\mathcal{F}(x)$ 之后，即：$y \\sigma(\\mathcal{F}(x)+x)$，其中 $\\sigma$ 表示 ReLU。 并且，残差函数是灵活的，可以包含多个堆叠层，全连接层、卷积层等等。但只有一个层时，并没有优势。 ### 瓶颈结构 为了降低计算复杂度，适用于极深网络（如ResNet 50/101/152）。 组成：1 × 1 卷积（降维）→ 3 × 3 卷积 → 1 × 1 卷积（恢复维度），形成“瓶颈”结构（下图右边）。 ![瓶颈结构](../static/images/ResNet/fig2.png) ### 网络架构 ![ResNet](../static/images/ResNet/fig3.png) ResNet 9 适用于轻量级任务，如CIFAR 10。补充 ResNet 9 的大致架构： 层级 操作 : :: : conv1 3 × 3 卷积，64 通道，步长为 1 conv2_x 1 个基础残差块（2 层 3 × 3 卷积， 64 通道） conv3_x 1 个基础残差块（2 层 3 × 3 卷积， 128 通道，下采样） conv4_x 1 个基础残差块（2 层 3 × 3 卷积， 256 通道，下采样） 全局平均池化 输出尺寸 1 × 1 全连接层 若干个神经元输出 大致对比： 网络 残差块类型 阶段块数（conv2_x到conv5_x） : :: :: : ResNet 9 基础块 [1,1,1,0] ResNet 18 基础块 [2,2,2,2] ResNet 34 基础块 [3,4,6,3] ResNet 50 瓶颈块 [3,4,6,3] ResNet 101 瓶颈块 [3,4,23,3] ResNet 152 瓶颈块 [3,8,36,3] ## 论文实验总结 论文的第四部分 Experiments 通过大量实验验证了残差网络（ResNet）的性能优势，涵盖多个数据集和任务，并深入分析了不同设计选择的影响。 *DeepSeek 总结* ### ImageNet图像分类实验 实验设置： 数据集：ImageNet 2012（1.28M 训练图像，50K 验证图像）。 网络结构：测试了 18 层、34 层、50 层、101 层、152 层的普通网络（Plain Net）和残差网络（ResNet）。 训练细节：SGD 优化器，初始学习率 0.1，分阶段衰减；数据增强包括随机裁剪、水平翻转、颜色扰动；使用批量归一化（BN）但无 Dropout。 关键结果： 退化问题验证：34 层普通网络的训练误差高于 18 层普通网络，而 34 层 ResNet 显著优于 18 层 ResNet，证明残差学习解决了退化问题。 极深网络表现：ResNet 152 在 ImageNet 上实现单模型 3.57% top 5 错误率（集成模型），赢得 ILSVRC 2015 分类任务冠军。 对比 VGG 和 GoogLeNet：ResNet 152 的复杂度（11.3 亿 FLOPs ）低于 VGG 19（19.6 亿 FLOPs），但性能显著更优。 ### CIFAR 10分析实验 实验目的：验证残差学习在更小数据集上的泛化能力。 网络设计：堆叠简单残差块（每个残差块包含两个3×3卷积层），测试20层到1202层的网络。 关键发现： 极深网络的可行性：ResNet 110（170 万参数）达到 6.43% 测试错误率，优于当时的先进方法（如高速公路网络）。 1202层网络训练：虽然训练误差趋近于零，但测试误差因过拟合略升至 7.93%，表明需要更强正则化。 ### 快捷连接的消融实验 对比选项： 选项 A：维度不匹配时用零填充（无参数）。 选项 B：维度不匹配时用 1 × 1 卷积调整（含参数）。 选项 C：所有快捷连接均为 1 × 1 卷积（大量参数）。 结论： 选项 B 略优于 A：因零填充无法学习残差。 选项 C 提升有限：参数过多但收益不高，最终选择选项 B 用于极深网络（如 ResNet 50/101/152）。 ### 目标检测与分割实验 任务与框架： 目标检测：基于 Faster R CNN，将 VGG 16 替换为 ResNet 101。 数据集：PASCAL VOC 2007/2012、MS COCO。 关键结果： COCO 检测：ResNet 101 相比 VGG 16，mAP@[.5, .95] 提升 6%（相对提升28%），验证了深层特征的泛化能力。 竞赛表现：在 ILSVRC 2015 中，ResNet 赢得检测、定位、分割任务冠军。 ### 极深网络的可视化与分析 残差响应分析： ResNet的残差函数输出的响应值普遍较小，表明网络更倾向于学习接近恒等映射的微调。 网络越深，单个残差块的调整幅度越小，验证了残差学习的“微扰动”假设。 ### 对比其他先进模型 与高速公路网络对比： ResNet在超过100层时仍能提升性能，而高速公路网络（Highway Networks）在极深时表现下降。 ResNet的快捷连接无参数，计算更高效。 ## 代码实现 基于残差网络的 CIFAR 10 分类实验代码在 [Github](https://github.com/Fingsinz/StyleTransfer/blob/main/src/01.ref_and_note/03.ResNet.py)。 实验记录如下： <table> <tr> <td><img src \"../static/images/ResNet/fig4.png\" /></td> <td><img src \"../static/images/ResNet/fig5.png\" /></td> </tr> </table> 部分代码如下： <details> <summary>实现简单残差块</summary> ```python class BasicBlock(nn.Module): \"\"\"残差块\"\"\" expansion 1 def __init__(self, in_channels, out_channels, stride 1, downsample None): super(BasicBlock, self).__init__() # 第一层卷积 → 批量归一化 → ReLU → 第二层卷积 → 批量归一化 self.conv1 nn.Conv2d( in_channels, out_channels, kernel_size 3, stride stride, padding 1, bias False ) self.bn1 nn.BatchNorm2d(out_channels) self.relu nn.ReLU(inplace True) self.conv2 nn.Conv2d( out_channels, out_channels, kernel_size 3, stride 1, padding 1, bias False ) self.bn2 nn.BatchNorm2d(out_channels) self.downsample downsample def forward(self, x): identity x # 恒等映射 # 第一层卷积 → 批量归一化 → ReLU → 第二层卷积 → 批量归一化 out self.conv1(x) out self.bn1(out) out self.relu(out) out self.conv2(out) out self.bn2(out) if self.downsample is not None: identity self.downsample(x) out + identity # 残差连接，F(x) + x out self.relu(out) # ReLU return out ``` </details> <details> <summary>基于残差块构建 ResNet 9</summary> ```python class ResNet9(nn.Module): \"\"\"ResNet9 model\"\"\" def __init__(self, num_classes): super(ResNet9, self).__init__() self.in_channels 64 # 初始卷积层 self.conv1 nn.Conv2d(3, 64, kernel_size 7, stride 2, padding 3, bias False) self.bn1 nn.BatchNorm2d(64) self.relu nn.ReLU(inplace True) self.maxpool nn.MaxPool2d(kernel_size 3, stride 2, padding 1) # 残差块 self.layer1 self._make_layer(BasicBlock, 64, 1, stride 1) self.layer2 self._make_layer(BasicBlock, 128, 1, stride 2) self.layer3 self._make_layer(BasicBlock, 256, 1, stride 2) # 全局平均池化和全连接层 self.avgpool nn.AdaptiveAvgPool2d((1, 1)) self.fc nn.Linear(256 * BasicBlock.expansion, num_classes) def _make_layer(self, block, out_channels, blocks, stride): downsample None # 检查是否需要下采样：如果步长不为 1 或输入输出通道数不匹配，则创建下采样层。 if stride ! 1 or self.in_channels ! out_channels * block.expansion: downsample nn.Sequential( nn.Conv2d( self.in_channels, out_channels * block.expansion, kernel_size 1, stride stride, bias False ), nn.BatchNorm2d(out_channels * block.expansion), ) layers [] layers.append(block(self.in_channels, out_channels, stride, downsample)) self.in_channels out_channels * block.expansion for _ in range(1, blocks): layers.append(block(self.in_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): x self.conv1(x) x self.bn1(x) x self.relu(x) x self.maxpool(x) x self.layer1(x) x self.layer2(x) x self.layer3(x) x self.avgpool(x) x torch.flatten(x, 1) x self.fc(x) return x ``` </details> <details> <summary>基于残差块构建 ResNet 9</summary> ```python class ResNet18(nn.Module): \"\"\"ResNet18 model\"\"\" def __init__(self, num_classes 1000): super(ResNet18, self).__init__() self.in_channels 64 # 初始卷积层 self.conv1 nn.Conv2d(3, 64, kernel_size 7, stride 2, padding 3, bias False) self.bn1 nn.BatchNorm2d(64) self.relu nn.ReLU(inplace True) self.maxpool nn.MaxPool2d(kernel_size 3, stride 2, padding 1) # 残差块 self.layer1 self._make_layer(BasicBlock, 64, 2, stride 1) self.layer2 self._make_layer(BasicBlock, 128, 2, stride 2) self.layer3 self._make_layer(BasicBlock, 256, 2, stride 2) self.layer4 self._make_layer(BasicBlock, 512, 2, stride 2) # 全局平均池化和全连接层 self.avgpool nn.AdaptiveAvgPool2d((1, 1)) self.fc nn.Linear(512 * BasicBlock.expansion, num_classes) def _make_layer(self, block, out_channels, blocks, stride): downsample None if stride ! 1 or self.in_channels ! out_channels * block.expansion: downsample nn.Sequential( nn.Conv2d( self.in_channels, out_channels * block.expansion, kernel_size 1, stride stride, bias False ), nn.BatchNorm2d(out_channels * block.expansion), ) layers [] layers.append(block(self.in_channels, out_channels, stride, downsample)) self.in_channels out_channels * block.expansion for _ in range(1, blocks): layers.append(block(self.in_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): x self.conv1(x) x self.bn1(x) x self.relu(x) x self.maxpool(x) x self.layer1(x) x self.layer2(x) x self.layer3(x) x self.layer4(x) x self.avgpool(x) x torch.flatten(x, 1) x self.fc(x) return x ``` </details>"},"/StyleTransfer/ref_and_notes/cgan.html":{"title":"cGAN：条件 GAN","content":" title: cGAN：条件 GAN keywords: cGAN desc: cGAN 简介 date: 2025 03 06 id: cGAN [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) *Mirza M , Osindero S .Conditional Generative Adversarial Nets[J].Computer Science, 2014:2672 2680.DOI:10.48550/arXiv.1411.1784.* [Image to Image Translation with Conditional Adversarial Networks](https://ieeexplore.ieee.org/document/8100115) *Isola P , Zhu J Y , Zhou T ,et al.Image to Image Translation with Conditional Adversarial Networks[C]//IEEE Conference on Computer Vision & Pattern Recognition.IEEE, 2016.DOI:10.1109/CVPR.2017.632.* ## cGAN 条件生成对抗网络（Conditional Generative Adversarial Networks, cGAN）是生成对抗网络（GAN）的一种扩展形式，通过引入 **条件信息**（如标签、文本、图像等），使生成器和判别器能够根据特定条件生成或判别数据。 核心思想是通过条件约束，控制生成内容的属性和结构，从而解决普通 GAN 生成结果不可控的问题。 ### cGAN 的核心原理 条件信息的引入： 生成器（Generator）：输入不仅包含随机噪声 $z$，还包括条件信息 $c$（如类别标签、另一张图像等）。生成器需根据 $c$ 生成对应的数据 $G(zc)$。 判别器（Discriminator）：输入包含真实数据 $x$ 或生成数据 $G(zc)$，同时结合条件信息 $c$。判别器的任务是判断数据是否真实且与条件匹配，即 $D(xc)$ 或 $D(G(zc)c)$。 cGAN 的损失函数在普通 GAN 的基础上加入了条件约束： $$ \\mathcal{L}_{cGAN}(G,D) \\mathbb{E}_{x,c}[\\log D(xc)] + \\mathbb{E}_{z,c}[\\log(1 D(G(zc)c)]] $$ 生成器 $G$ 的目标：生成与条件 $c$ 匹配的逼真数据，使 $D(G(zc)c)$ 趋近于1。 判别器 $D$ 的目标：区分真实数据 $xc$ 和生成数据 $G(zc)c$。 ### cGAN 对比普通 GAN 特性 普通GAN 条件GAN（cGAN） : :: :: : 输入 随机噪声 $z$ 随机噪声 $z$ + 条件信息 $c$ 生成控制 完全随机 通过条件 $c$ 控制生成内容 应用场景 无约束生成（如随机图像生成） 需特定条件生成（如根据文本生成图像） 典型任务 生成随机人脸、艺术品 图像到图像转换（pix2pix）、文本到图像生成、可控生成（如风格迁移）、图像修复、图像翻译（如黑白→彩色） ### 代码示例 ```python # 生成器（U Net结构为例） class Generator(nn.Module): def __init__(self): super().__init__() # 输入：噪声z + 条件图像c self.encoder Encoder() # 下采样层 self.decoder Decoder() # 上采样层（含跳跃连接） def forward(self, z, c): x torch.cat([z, c], dim 1) # 拼接噪声和条件 return self.decoder(self.encoder(x)) # 判别器（PatchGAN结构为例） class Discriminator(nn.Module): def __init__(self): super().__init__() # 输入：真实/生成图像 + 条件图像c self.conv_blocks nn.Sequential( nn.Conv2d(3 + 3, 64, kernel_size 4, stride 2), # 假设条件c是3通道图像 nn.LeakyReLU(0.2), # 更多卷积层... ) def forward(self, x, c): x torch.cat([x, c], dim 1) # 拼接图像和条件 return self.conv_blocks(x) ``` ## Image to Image Translation with cGAN 本文提出使用条件 GANs 作为通用解决方案，通过对抗训练自动学习任务相关的损失函数，避免人工设计损失函数的复杂性。 > *条件生成对抗网络（Conditional Generative Adversarial Networks, cGANs）最初由 Mehdi Mirza 和 Simon Osindero 在 2014 年的论文 《Conditional Generative Adversarial Nets》 中提出。这篇论文首次将条件信息（如类别标签或辅助数据）引入 GAN 框架，使生成器和判别器能够基于特定条件进行训练和生成。* 条件 GANs 的优势： 条件输入：生成器和判别器均以输入图像为条件，确保输出与输入的结构对齐（*如下图输入边缘图生成对应照片案例中，生成器和判别器都观察输入的边缘*）。 ![](../static/images/cGAN/fig1.png) 结合L1损失：在对抗损失基础上引入 L1 损失，保留低频信息（如整体布局），而对抗损失负责高频细节（如纹理和锐度），解决传统 L2 损失导致的模糊问题。 ### 方法细节 #### 目标函数 cGAN 的目标可以表示为： $$ \\mathcal{L}_{cGAN}(G,D) \\mathbb{E}_{x,c}[\\log D(xc)] + \\mathbb{E}_{z,c}[\\log(1 D(G(zc)c)]] $$ 目标函数上，总损失函数为对抗损失与 L1 损失的加权和： $$ G^* \\arg\\min_G \\max_D \\mathcal{L}_{cGAN}(G, D) + \\lambda \\mathcal{L}_{L1}(G) $$ 对抗损失：迫使生成器输出逼真的图像，判别器区分生成图像与真实图像。 L1 损失：约束生成图像与真实图像在像素级的一致性，减少模糊（所以不使用 L2 损失）。 随机性的引入：生成器的输入包含随机噪声（通过 Dropout 实现），但实验表明生成结果仍具有较低随机性。这表明当前方法在建模条件分布的多样性方面仍有改进空间。 #### 网络架构 在网络架构上： 生成器：带跳跃连接。 判别器（马尔可夫随机场）：[PatchGAN](./patchgan.html)，尝试对图像中的每个 N × N 块进行真假分类。在图像上卷积运行这个鉴别器，平均所有响应来提供 D 的最终输出。 #### 训练与推演过程 训练中： 遵循 GAN 中的优化算法，交替 $D$ 和 $G$ 的 step 训练。 在优化 $D$ 时将目标函数除以 2，减慢 $D$ 相对于 $G$ 学习的速率。 使用小批量 SGD 并应用 Adam 求解器，学习率为 0.0002，动量参数 $\\beta_1 0.5$，$\\beta_2 0.999$。 推演时： 与训练阶段相同的方式运行生成器。 ### 实验与验证 论文的第四部分通过广泛的实验验证了条件生成对抗网络（cGAN）在多种图像到图像转换任务中的有效性和通用性. #### 实验任务与数据集 作者在以下任务中测试了框架的通用性，涵盖图形和视觉任务： 语义标签 ↔ 照片（Cityscapes数据集）：将语义分割标签转换为真实街景照片。 建筑标签 → 照片（CMP Facades数据集）：将建筑立面线框图转换为真实建筑照片。 地图 ↔ 航拍图（Google Maps数据）：实现卫星地图与航拍图的双向转换。 黑白 → 彩色（ImageNet数据）：为灰度图像自动着色。 边缘 → 照片（UT Zappos50K、Amazon Handbag数据）：从边缘图生成鞋类、手提包等实物图像。 草图 → 照片（人类手绘草图）：扩展边缘到照片的模型至非结构化输入。 白天 → 夜晚（Webcam数据）：转换场景光照条件。 热成像 → 彩色照片（多光谱行人检测数据集）：融合热成像与可见光信息。 图像修复（Paris StreetView数据集）：补全图像中缺失的像素区域。 每个任务均使用相同架构（U Net 生成器 + PatchGAN 判别器）和损失函数（L1 + cGAN），仅更换训练数据。[跳转链接](https://phillipi.github.io/pix2pix/) #### 数据需求与训练效率 小数据集表现： 建筑标签任务仅需400张图像，训练时间不到2小时（单块Titan X GPU）。 昼夜转换任务使用91个摄像头的图像，训练17个周期即收敛。 推理速度： 所有模型在GPU上运行时间均小于1秒，支持实时应用。 #### 评估方法 (1) AMT 感知实验 设计：通过 Amazon Mechanical Turk 平台进行“真实 vs. 生成”的二选一测试，参与者需在 1 秒内观察图像后判断真伪。 结果： 地图→航拍图任务中，18.9%的生成图像被误认为真实（显著优于L1基线）。 航拍图→地图任务中，生成图像仅6.1%被误判（与L1基线无显著差异），因地图的几何结构更易暴露细节错误。 (2) FCN score 设计：使用预训练的FCN 8s模型（在Cityscapes上训练）对生成图像进行语义分割，计算分割精度（像素准确率、类别准确率、IoU）。 意义：衡量生成图像是否保留了输入标签的语义结构。 关键结果： L1 + cGAN 组合在Cityscapes任务中取得最高分数（像素准确率66%，接近真实数据的80%）。 单独使用 L1 损失会导致模糊，单独使用cGAN则可能生成结构错误但逼真的图像。 #### 目标函数分析 通过消融实验验证各损失组件的贡献： L1 损失：强制像素级匹配，减少模糊但导致色彩单调。 cGAN 损失：提升图像逼真性和高频细节（如边缘锐利、色彩丰富），但可能引入结构错误。 L1 + cGAN：结合二者优势，在逼真性和结构准确性间取得平衡。 #### 生成器架构分析 ![](../static/images/cGAN/fig2.png) U Net vs. 编码器 解码器： U Net 通过跳跃连接保留低级特征（如边缘位置），在图像着色等任务中显著优于普通编码器 解码器。 即使仅用 L1 损失训练，U Net 仍优于编码器 解码器，证明跳跃连接对信息传递的关键作用。 #### PatchGAN 尺寸分析 ![](../static/images/cGAN/fig3.png) *Patch 大小的影响。对于不同的损失函数，输出中的不确定性表现不同。在 L1 下，不确定区域变得模糊和去饱和。1x1 PixelGAN 鼓励更大的色彩多样性，但对空间统计没有影响。16x16 PatchGAN 创建了局部清晰的结果，但也导致了超出其可观察范围的平铺伪影。70×70 PatchGAN强制输出在空间和光谱（色彩）维度上都是清晰的，即使不正确。完整的 286×286 ImageGAN 生成的结果在视觉上与 70×70 PatchGAN 相似，但根据FCN评分指标，质量略低。请参阅 https://phillipi.github.io/pix2pix/ 了解更多示例。* 测试不同感受野的判别器： 1×1 PixelGAN：仅提升色彩多样性，对空间结构无影响。 16×16 PatchGAN：生成局部锐利图像，但出现拼贴伪影。 70×70 PatchGAN：最佳平衡，生成全局一致且细节清晰的图像。 286×286 ImageGAN（全图判别器）：参数量大、训练困难，且 FCN score 下降。 #### 全卷积扩展性测试 PatchGAN 固定大小的 Patch 可以应用于任意大的图像。如在 256×256 分辨率训练生成器，直接应用于 512×512 图像。结果是生成高分辨率图像时仍保持质量，证明框架的扩展性。 #### 语义分割任务 实验设计：将 cGAN 应用于照片→语义标签的逆任务。 结果： 仅用 cGAN（无L1）可生成粗略标签，但准确率低于 L1 回归。 作者认为，结构化输出任务（如分割）因目标明确，更适合传统回归损失。 #### 失败案例分析 ![](../static/images/cGAN/fig4.png) *每对图像中，左侧为输入，右侧为输出。* 常见问题： 输入稀疏或异常时，生成器产生伪影（如缺失边缘的区域）。 对非常规输入（如抽象草图）的泛化能力有限。 ### pix2pix 代码示例 [Github](https://github.com/junyanz/pytorch CycleGAN and pix2pix/blob/master/models/pix2pix_model.py) ```python class Pix2PixModel(BaseModel): # ... 省略 ... def __init__(self, opt): BaseModel.__init__(self, opt) # 指定要打印的训练损失。训练/测试脚本将调用<BaseModel.get_current_losses> self.loss_names ['G_GAN', 'G_L1', 'D_real', 'D_fake'] # 指定要保存/显示的图像。训练/测试脚本将调用<BaseModel.get_current_visuals> self.visual_names ['real_A', 'fake_B', 'real_B'] # 指定要保存到磁盘的模型。训练/测试脚本将调用<BaseModel.save_networks>和<BaseModel.load_networks> if self.isTrain: self.model_names ['G', 'D'] else: # 在测试期间，只加载G self.model_names ['G'] # 定义网络（生成器和鉴别器） self.netG networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids) if self.isTrain: # 定义一个鉴别器；条件gan需要同时获取输入和输出图像；因此，D 的 channels input_nc + output_nc self.netD networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids) if self.isTrain: # 定义损失函数 self.criterionGAN networks.GANLoss(opt.gan_mode).to(self.device) self.criterionL1 torch.nn.L1Loss() # 初始化优化器；优化器将由函数自动创建<BaseModel.setup>。 self.optimizer_G torch.optim.Adam(self.netG.parameters(), lr opt.lr, betas (opt.beta1, 0.999)) self.optimizer_D torch.optim.Adam(self.netD.parameters(), lr opt.lr, betas (opt.beta1, 0.999)) self.optimizers.append(self.optimizer_G) self.optimizers.append(self.optimizer_D) # ... 省略 ... def forward(self): self.fake_B self.netG(self.real_A) # G(A) ``` #### 判别器的反向传播（`backward_D`） 1. 生成假样本输入判别器 生成器生成假图像 `fake_B`，将其与输入图像 `real_A` 拼接为 `fake_AB`。 通过 `fake_AB.detach()` 切断梯度回传，防止生成器参数在判别器训练时被更新。 判别器对假样本的预测结果 `pred_fake` 与标签 `False` 计算损失 `loss_D_fake`。 2. 处理真实样本 将真实图像对 `real_A` 和 `real_B` 拼接为 `real_AB`。 判别器对真实样本的预测结果 `pred_real` 与标签 `True` 计算损失 `loss_D_real`。 3. 计算总损失并反向传播 总损失为真假样本损失的平均值：$\\text{loss}_{\\text{D}} (\\text{loss}_{\\text{D_fake}} + \\text{loss}_{\\text{D_real}}) / 2$ 执行 `loss_D.backward()` 计算梯度，通过 `optimizer_D.step()` 更新判别器参数。 ```python def backward_D(self): \"\"\"计算鉴别器的 GAN 损失\"\"\" # Fake；通过分离 fake_B 来停止对生成器的反向传播 # 使用条件 GAN，需要将输入和输出都提供给网络 fake_AB torch.cat((self.real_A, self.fake_B), 1) pred_fake self.netD(fake_AB.detach()) self.loss_D_fake self.criterionGAN(pred_fake, False) # Real real_AB torch.cat((self.real_A, self.real_B), 1) pred_real self.netD(real_AB) self.loss_D_real self.criterionGAN(pred_real, True) # 结合损失和计算梯度 self.loss_D (self.loss_D_fake + self.loss_D_real) * 0.5 self.loss_D.backward() ``` #### 生成器的反向传播（`backward_G`） 1. 对抗损失（GAN Loss） 将生成的假图像 `fake_B` 与输入图像 `real_A` 拼接为 `fake_AB`，输入判别器得到预测结果 `pred_fake`。 计算对抗损失 `loss_G_GAN`，目标是让判别器认为生成的图像为真（标签 `True`）。 2. L1 重建损失 计算生成图像 `fake_B` 与真实图像 `real_B` 的像素级 L1 损失 `loss_G_L1`，乘以权重系数 `lambda_L1`（通过 `opt.lambda_L1` 控制）。 3. 计算总损失并反向传播 总损失为对抗损失与L1损失之和：$\\text{loss}_{\\text{G}} \\text{loss}_{\\text{G_GAN}} + \\text{loss}_{\\text{G_L1}}$ 执行 `loss_G.backward()` 计算梯度，通过 `optimizer_G.step()` 更新生成器参数。 ```python def backward_G(self): \"\"\"计算生成器的 GAN 和 L1 损失\"\"\" # 1. G(A) 应该骗过判别器 fake_AB torch.cat((self.real_A, self.fake_B), 1) pred_fake self.netD(fake_AB) self.loss_G_GAN self.criterionGAN(pred_fake, True) # 2. G(A) B self.loss_G_L1 self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1 # 结合损失并计算梯度 self.loss_G self.loss_G_GAN + self.loss_G_L1 self.loss_G.backward() ``` #### 训练流程（`optimize_parameters`） 1. 前向传播生成假图像：`self.forward()` 调用生成器生成 `fake_B`。 2. 更新判别器： 解冻判别器参数（`set_requires_grad(self.netD, True)`）。 清零梯度（`optimizer_D.zero_grad()`）。 计算判别器损失并反向传播（`backward_D()`）。 更新参数（`optimizer_D.step()`）。 3. 更新生成器： 冻结判别器参数（`set_requires_grad(self.netD, False)`）。 清零梯度（`optimizer_G.zero_grad()`）。 计算生成器损失并反向传播（`backward_G()`）。 更新参数（`optimizer_G.step()`）。 ```python def optimize_parameters(self): self.forward() # 计算生成器生成的假图像: G(A) # 更新 D self.set_requires_grad(self.netD, True) # 启用 D 的反向传播 self.optimizer_D.zero_grad() self.backward_D() self.optimizer_D.step() # 更新 G self.set_requires_grad(self.netD, False) # D 在更新 G 时不需要梯度 self.optimizer_G.zero_grad() self.backward_G() self.optimizer_G.step() ``` ### 局限性与启示 随机性不足：生成结果偏向确定性，难以建模多模态输出（如同一输入对应多种合理输出）。 复杂任务表现：在高度结构化任务（如语义分割）中，cGANs 效果不及纯 L1 回归，表明对抗训练更适用于需细节生成的图形任务。 社区应用：开源代码（pix2pix）被广泛用于艺术创作（如草图转肖像、背景去除），验证了其易用性和扩展性。"},"/StyleTransfer/ref_and_notes/cyclegan.html":{"title":"CycleGAN：循环 GAN","content":" title: CycleGAN：循环 GAN keywords: CycleGAN desc: 循环 GAN date: 2025 03 11 id: CycleGAN [Unpaired Image to Image Translation using Cycle Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593) *Zhu J Y , Park T , Isola P ,et al.Unpaired Image to Image Translation using Cycle Consistent Adversarial Networks[J].IEEE, 2017.DOI:10.1109/ICCV.2017.244.* > Image to image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G:X→Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under constrained, we couple it with an inverse mapping F:Y→X and introduce a cycle consistency loss to push F(G(X))≈X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach. **摘要**：图像到图像的转换是一类视觉和图形的问题，其目标是使用对齐的图像对进行学习输入图像和输出图像之间的映射。但是对于许多任务而言，并不存在大量配对的训练数据。我们提出了一种学习在没有配对例子的情况下，将图像从源域 X 转换到目标域 Y 的方法。我们的目标是学习一个映射 G：X → T，使得使用对抗性损失无法区分是来自 G(x) 的图像分布还是 Y 的图像分布。因为这个映射是高度欠约束的，我们将这个映射与其逆映射 F：Y → X 耦合，并引入循环一致性损失，以推动 F(G(X)) ≈ X（反之亦然）。在不存在配对训练数据的几个任务上，如风格转移、对象变形、季节转移、照片增强等，给出了定性结果。与先前几种方法的定量比较表明了我们方法的优越性。 ## CycleGAN 模型 引入循环一致性损失，实现了无需配对训练数据的跨域图像转换。 双向映射：同时学习 $G:X\\rightarrow Y$ 和 $F:Y\\rightarrow X$，确保生成的图像在目标域中分布一致。 循环一致性约束：约束 $F(G(x))\\approx x$ 和 $G(F(y))\\approx y$，防止生成器陷入模式崩溃。 ## CycleGAN 具体框架 该框架的目标是通过给定训练集 $\\{x_i\\}^{N}_{i 1}$ 和 $\\{y_i\\}^{M}_{j 1}$，学习源域和目标域之间的映射。 其中，$x_i \\in X$，$y_i \\in Y$；$x \\sim p_{data}(x)$，$y \\sim p_{data}(y)$。 ![](../static/images/CycleGAN/fig1.png) 如上图所示，**模型包含两个映射：$G:X\\rightarrow Y$ 和 $F:Y\\rightarrow X$**。 另外，还加上两个对抗判别器 $D_X$ 和 $D_Y$： $D_X$ 用于区分图像 $\\{x\\}$ 和生成图像 $\\{F(y)\\}$； $D_Y$ 用于区分图像 $\\{y\\}$ 和生成图像 $\\{G(x)\\}$。 引入对抗性损失和循环一致性损失： **对抗性损失用于匹配生成图像的分布和目标域的分布；** **引入循环一致性损失，用于防止学习到的映射 $G$ 和 $F$ 相互矛盾。** ### 对抗损失 对于映射 $G:X\\rightarrow Y$ 和判别器 $D_Y$，定义对抗损失为： $$ \\mathcal{L}_{GAN}(G,D_Y,X,Y) \\mathbb{E}_{y \\sim p_{data}(y)} \\log D_Y(y) + \\mathbb{E}_{x \\sim p_{data}(x)} \\log(1 D_Y(G(x))) $$ $G$ 试图生成与目标域 $Y$ 相似的图像 $G(x)$，而判别器 $D_Y$ 试图区分生成图像 $G(x)$ 和真实图像 $y$。 **$G$ 的目标是最小化这个目标，而判别器 $D_Y$ 则试图最大化这个目标，即：$\\min_{G}\\max_{D_Y}\\mathcal{L}(G,D_Y,X,Y)$。** 类似地引入映射 $F:Y\\rightarrow X$ 和 $D_X$ 的对抗损失： $$ \\mathcal{L}_{GAN}(F,D_X,Y,X) \\mathbb{E}_{x \\sim p_{data}(x)} \\log D_X(x) + \\mathbb{E}_{y \\sim p_{data}(y)} \\log(1 D_X(F(y))) $$ **$F$ 的目标是最小化这个目标，而判别器 $D_X$ 则试图最大化这个目标，即：$\\min_{F}\\max_{D_X}\\mathcal{L}(F,D_X,Y,X)$。** ### 循环一致性损失 这两个映射转换应该是可以循环的，即 $x\\rightarrow G(x) \\rightarrow F(G(x)) \\approx x$ 和 $y\\rightarrow F(y) \\rightarrow G(F(y)) \\approx y$。 所以使用循环一致性损失以促进转换循环： $$ \\mathcal{L}_{cyc}(G,F) \\mathbb{E}_{x \\sim p_{data}(x)} \\left[\\vert\\vert F(G(x)) x \\vert\\vert _1 \\right] + \\mathbb{E}_{y \\sim p_{data}(y)} \\left[\\vert\\vert G(F(y)) y \\vert\\vert _1 \\right] $$ ### 完整的目标函数 总目标为： $$ \\mathcal{L}(G,F,D_X,D_Y) \\mathcal{L}_{GAN}(G,D_Y,X,Y) + \\mathcal{L}_{GAN}(F,D_X,Y,X) + \\lambda \\mathcal{L}_{cyc}(G,F) $$ $\\lambda$ 控制两个损失的权重。 存在解： $$ G^*, F^* \\arg\\min_{G,F}\\max_{D_X,D_Y} \\mathcal{L}(G,F,D_X,D_Y) $$ ## CycleGAN 实现细节 ### 生成器 基于 Johnson 等人提出的残差网络（ResNet）结构，适用于图像生成任务（如风格迁移、超分辨率）。 组件： 初始卷积层：`c7s1 k` 表示一个 7 × 7 的卷积层，步长为 1，输出通道数为 k，后接实例归一化（InstanceNorm）和 ReLU 激活。 下采样层：`dk` 表示 3 × 3 卷积层，步长为 2，通道数为 k，用于逐步降低分辨率。 残差块：`Rk` 包含两个 3 × 3 卷积层，每层通道数为 k，保留输入与输出的残差连接。 上采样层：`uk` 表示 3 × 3 分数步长卷积（反卷积），步长为 1/2，用于恢复分辨率。 末端卷积层：`c7s1 3` 生成 RGB 三通道输出。 结构配置： 低分辨率（128 × 128）：6 个残差块，结构为：`c7s1 64 → d128 → d256 → 6×R256 → u128 → u64 → c7s1 3`。 高分辨率（256 × 256 及以上）：9 个残差块，结构为：`c7s1 64 → d128 → d256 → 9×R256 → u128 → u64 → c7s1 3`。 关键设计： 实例归一化（Instance Normalization）：提升生成图像的视觉质量，尤其在风格迁移任务中抑制内容无关的变量。 反射填充（Reflection Padding）：减少边缘伪影，提升生成图像的平滑度。 ### 判别器 来自 Isola 等人的 70 × 70 PatchGAN。 层级结构： 初始层：C64 为 4 × 4 卷积，步长为 2，输出 64 通道，无实例归一化，使用 LeakyReLU（斜率 0.2）。 后续层：C128 → C256 → C512，每层为 4 × 4 卷积 + 实例归一化 + LeakyReLU，逐步增加通道数。 末端输出：通过卷积生成 1 通道的判别结果（局部区域的真假概率图）。 关键设计： 局部判别：关注图像局部区域（70 × 70 patch）的真实性，而非全局，降低计算量。 全卷积结构：支持任意尺寸输入，适用于不同分辨率的任务。 ### 训练策略 #### 损失函数改进 最小二乘 GAN（LSGAN）代替负对数似然，采用均方误差（MSE）目标函数，缓解梯度消失问题，生成图像质量更高，训练更稳定。 训练 $G$ 最小化 $\\mathbb{E}_{x \\sim p_{data}(x)}[(D(G(x)) 1)^2]$； 训练 $D$ 最小化 $\\mathbb{E}_{y \\sim p_{data}(y)}[(D(y) 1)^2] + \\mathbb{E}_{x \\sim p_{data}(x)}[D(G(x))^2]$。 原来的损失函数： $$ \\begin{cases} \\mathcal{L}_{GAN}(G,D_Y,X,Y) \\mathbb{E}_{y \\sim p_{data}(y)} \\log D_Y(y) + \\mathbb{E}_{x \\sim p_{data}(x)} \\log(1 D_Y(G(x)))\\\\ \\mathcal{L}_{GAN}(F,D_X,Y,X) \\mathbb{E}_{x \\sim p_{data}(x)} \\log D_X(x) + \\mathbb{E}_{y \\sim p_{data}(y)} \\log(1 D_X(F(y))) \\end{cases} $$ 用 LSGAN 代替后： $$ \\begin{cases} \\mathcal{L}_{GAN}(G,D_Y,X,Y) \\mathbb{E}_{y \\sim p_{data}(y)} [(D_Y(y) 1)^2] + \\mathbb{E}_{x \\sim p_{data}(x)} [D_Y(G(x))^2]\\\\ \\mathcal{L}_{GAN}(F,D_X,Y,X) \\mathbb{E}_{x \\sim p_{data}(x)} [(D_X(x) 1)^2] + \\mathbb{E}_{y \\sim p_{data}(y)} [D_X(F(y))^2] \\end{cases} $$ 对于 $G$ 和 $D_Y$，采用的是 Least Squares GAN（LSGAN）形式，其具体表达式为上面的 $\\mathcal{L}_{GAN}(G,D_Y,X,Y)$，包含两部分： 第一项 $\\mathbb{E}_{y\\sim p_{data}(y)}[(D_Y(y) 1)^2]$，用于训练判别器 $D_Y$ ，要求对真实图像 $y$ 的输出接近 1，即让 $D_Y$ 识别真实图像为真实。 第二项 $\\mathbb{E}_{x\\sim p_{data}(x)}[D_Y(G(x))^2]$，用于训练判别器 $D_Y$ ，要求对生成图像 $G(x)$ 的输出接近 0，即让 $D_Y$ 识别生成图像为假。 同时，在训练生成器 $G$ 时，只关心让生成图像 $G(x)$ 看起来足够真实，也就是使得 $D_Y(G(x))$ 输出接近 1。因此，生成器 $G$ 的目标就是最小化：$\\mathbb{E}_{x\\sim p_{data}(x)}[(D_Y(G(x)) 1)^2]$，这一目标正好出现在上式中的第一项，但生成器只会更新这一部分，而判别器 $D_Y$ 则同时考虑了真实图像和生成图像两部分的损失。 对于反方向（$F$ 与 $D_X$），也有相似的表达 $ L_{GAN}(F,D_X,Y,X)$。这里，生成器 $F$ 训练时的目标是最小化 $\\mathbb{E}_{y\\sim p_{data}(y)}[(D_X(F(y)) 1)^2]$。 将两个方向的对抗性损失合并，再加上后续的循环一致性损失，形成了总损失函数： $$ \\mathcal{L}(G,F,D_X,D_Y) \\mathcal{L}_{GAN}(G,D_Y,X,Y) + \\mathcal{L}_{GAN}(F,D_X,Y,X) + \\lambda \\mathcal{L}_{cyc}(G,F) $$ 在上式中： $L_{GAN}(G,D_Y,X,Y)$ 部分包含了生成器 $G$ 的训练目标 $\\mathbb{E}_{x\\sim p_{data}(x)}[(D_Y(G(x)) 1)^2]$（生成器更新时使用）以及判别器 $D_Y$ 的训练目标（结合真实图像的 $\\mathbb{E}_{y\\sim p_{data}(y)}[(D_Y(y) 1)^2]$ 和生成图像的 $\\mathbb{E}_{x\\sim p_{data}(x)}[D_Y(G(x))^2]$）。 $L_{GAN}(F,D_X,Y,X)$ 部分类似地，包含了生成器 $F$ 与判别器 $D_X$ 的对应目标。 #### 恒等映射损失（Identity Loss） 在特定任务中，强制生成器在输入目标域图像时保持原图不变： $$ \\mathcal{L}_{identity} \\mathbb{E}_y [\\vert\\vert G(y) y \\vert\\vert _1] + \\mathbb{E}_x [\\vert\\vert F(x) x \\vert\\vert _1] $$ 作用：防止生成器过度改变输入的颜色或光照（例如避免将白天的照片转为日落色调）。 #### 训练稳定性优化 图像缓冲区（Image Buffer）：存储最近生成的50张图像，用于更新判别器。避免判别器仅依赖当前生成器的最新输出，减少模式崩溃风险。 学习率调度：初始学习率设为 0.0002（Adam优化器），前 100 个 epoch 保持不变。后100个epoch线性衰减至0，逐步收敛模型。 权重初始化：从高斯分布 $N(0,0.02)$ 采样，避免初始值过大或过小。 批量大小：固定为 1（单张图像训练），节省显存并适应高分辨率任务。 任务特定调整： 艺术风格迁移：加入恒等映射损失（权重为 $0.5\\lambda$，$\\lambda 10$）。 季节转换/物体变换：使用 ImageNet 子集，图像尺寸统一为 256 × 256。 #### 训练集与训练参数 数据集配置： Cityscapes（语义标签↔照片）：2975 张训练图像，分辨率 128 × 128。 Google Maps（地图↔航拍图）：1096 张图像，分辨率 256 × 256，按地理纬度划分训练/测试集。 艺术风格迁移：从 Wikiart和 Flickr 收集数据，筛选后包含莫奈（1074 张）、梵高（401 张）等风格。 季节转换：从 Flickr 下载约 2000 张冬夏景观图，分辨率 256 × 256。 超参数设置： 循环一致性损失权重：$\\lambda 10$（通过消融实验验证其重要性）。 训练周期：通常为 200 个 epoch（前 100 固定学习率，后 100 线性衰减）。 硬件与框架：基于 PyTorch 和 Torch 实现，使用 NVIDIA GPU 加速。 ### 代码分析 [CycleGAN/models/cycle_gan_model.py](https://github.com/junyanz/pytorch CycleGAN and pix2pix/blob/master/models/cycle_gan_model.py) ```python class CycleGANModel(BaseModel): \"\"\" CycleGAN paper: https://arxiv.org/pdf/1703.10593.pdf \"\"\" def __init__(self, opt): \"\"\"初始化\"\"\" BaseModel.__init__(self, opt) self.loss_names ['D_A', 'G_A', 'cycle_A', 'idt_A', 'D_B', 'G_B', 'cycle_B', 'idt_B'] visual_names_A ['real_A', 'fake_B', 'rec_A'] visual_names_B ['real_B', 'fake_A', 'rec_B'] if self.isTrain and self.opt.lambda_identity > 0.0: visual_names_A.append('idt_B') visual_names_B.append('idt_A') self.visual_names visual_names_A + visual_names_B if self.isTrain: self.model_names ['G_A', 'G_B', 'D_A', 'D_B'] else: self.model_names ['G_A', 'G_B'] ``` > 做初始化、损失配置等等。 ```python self.netG_A networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids) self.netG_B networks.define_G(opt.output_nc, opt.input_nc, opt.ngf, opt.netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids) if self.isTrain: # define discriminators self.netD_A networks.define_D(opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids) self.netD_B networks.define_D(opt.input_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids) ``` > 定义生成器和判别器。 > 此处 G_A 为 G, G_B 为 F, D_A 为 D_Y, D_B 为 D_X。 ```python if self.isTrain: if opt.lambda_identity > 0.0: # 仅当输入和输出图像具有相同数量的通道时才有效 assert(opt.input_nc opt.output_nc) self.fake_A_pool ImagePool(opt.pool_size) self.fake_B_pool ImagePool(opt.pool_size) ``` > 创建图像缓冲区以存储以前生成的图像 ```python self.criterionGAN networks.GANLoss(opt.gan_mode).to(self.device) # define GAN loss. self.criterionCycle torch.nn.L1Loss() self.criterionIdt torch.nn.L1Loss() self.optimizer_G torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), lr opt.lr, betas (opt.beta1, 0.999)) self.optimizer_D torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr opt.lr, betas (opt.beta1, 0.999)) self.optimizers.append(self.optimizer_G) self.optimizers.append(self.optimizer_D) ``` > 定义损失函数和优化器（对抗损失：`MSELoss`；循环一致性损失：`L1Loss`；循环重建损失：`L1Loss`）。 > 初始化优化器；schedulers 将由函数 `BaseModel.setup` 自动创建。 ```python def forward(self): self.fake_B self.netG_A(self.real_A) # G_A(A) self.rec_A self.netG_B(self.fake_B) # G_B(G_A(A)) self.fake_A self.netG_B(self.real_B) # G_B(B) self.rec_B self.netG_A(self.fake_A) # G_A(G_B(B)) ``` > 前向传播。`netG_A` 生成假图像 B，`netG_B` 重建图像 A；`netG_B` 生成假图像 A，`netG_A` 重建图像 B。 > 生成假图像与循环重建。 ```python def backward_D_basic(self, netD, real, fake): pred_real netD(real) loss_D_real self.criterionGAN(pred_real, True) pred_fake netD(fake.detach()) loss_D_fake self.criterionGAN(pred_fake, False) loss_D (loss_D_real + loss_D_fake) * 0.5 loss_D.backward() return loss_D ``` > 判别器对真实图像进行预测，接着计算对真实图像损失；然后对假图像进行预测，计算对假图像的损失。 > 最后将两个损失相加，然后反向传播。 ```python def backward_D_A(self): \"\"\"Calculate GAN loss for discriminator D_A\"\"\" fake_B self.fake_B_pool.query(self.fake_B) self.loss_D_A self.backward_D_basic(self.netD_A, self.real_B, fake_B) def backward_D_B(self): \"\"\"Calculate GAN loss for discriminator D_B\"\"\" fake_A self.fake_A_pool.query(self.fake_A) self.loss_D_B self.backward_D_basic(self.netD_B, self.real_A, fake_A) ``` > 在缓冲池种随机选择一个假图像，然后计算判别器的损失。 ```python def backward_G(self): lambda_idt self.opt.lambda_identity lambda_A self.opt.lambda_A lambda_B self.opt.lambda_B if lambda_idt > 0: self.idt_A self.netG_A(self.real_B) self.loss_idt_A self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt self.idt_B self.netG_B(self.real_A) self.loss_idt_B self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt else: self.loss_idt_A 0 self.loss_idt_B 0 self.loss_G_A self.criterionGAN(self.netD_A(self.fake_B), True) self.loss_G_B self.criterionGAN(self.netD_B(self.fake_A), True) self.loss_cycle_A self.criterionCycle(self.rec_A, self.real_A) * lambda_A self.loss_cycle_B self.criterionCycle(self.rec_B, self.real_B) * lambda_B self.loss_G self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B self.loss_G.backward() ``` > 分别计算恒等损失、对抗损失和循环一致性损失。 ```python def optimize_parameters(self): self.forward() self.set_requires_grad([self.netD_A, self.netD_B], False) self.optimizer_G.zero_grad() self.backward_G() self.optimizer_G.step() self.set_requires_grad([self.netD_A, self.netD_B], True) self.optimizer_D.zero_grad() self.backward_D_A() self.backward_D_B() self.optimizer_D.step() ``` > 1. 前向计算生成图像。 > 2. 冻结判别器，计算梯度，更新参数。 > 3. 解冻生成器，计算梯度，更新参数。 ### 代码复现 复现代码：[Github/01.ref_and_note/05.CycleGAN.py](https://github.com/Fingsinz/StyleTransfer/blob/main/src/01.ref_and_note/05.CycleGAN.py) ## CycleGAN 实验结果 ### 与其他基线模型的性能对比 定量评估： AMT感知测试（地图↔航拍图任务）： CycleGAN 的欺骗率显著优于基线方法（如 CoGAN、SimGAN 等），在 256 × 256 分辨率下： 地图→航拍图：26.8% vs 基线的 0.6% 2.6% 航拍图→地图：23.2% vs 基线的 0.3% 2.9% FCN 分数（Cityscapes 标签→照片任务）： CycleGAN 的 FCN 分数（0.58）接近有监督方法pix2pix（0.85），远超其他无监督方法（如 BiGAN/ALI 的 0.41）。 语义分割指标（照片→标签任务）： CycleGAN 的类别平均交并比（Class IOU）为0.16，显著高于SimGAN（0.07）和CoGAN（0.08）。 定性分析： 生成图像在视觉质量上接近有监督方法 pix2pix（见图5、图6），尤其在风格迁移和物体变换任务中表现突出。 ### 消融实验（Ablation Study） 关键结论： 对抗损失（GAN Loss）：移除后生成图像质量严重下降（FCN 分数从 0.58 降至 0.22 ），表明对抗训练对生成真实性至关重要。 循环一致性损失（Cycle Loss）：移除后模型无法保持输入与输出的合理映射（如标签→照片任务中生成无意义图像）。 单向循环约束（仅前向或后向）：导致模式崩溃或训练不稳定（如马→斑马任务中生成单一结果）。 ### 多任务应用效果 风格迁移： 成功将照片转换为莫奈、梵高等艺术家的整体风格，而非单幅作品风格。 加入身份映射损失（Identity Loss）后，有效保留输入色彩（如避免白昼→黄昏的错误转换）。 物体变换： 马↔斑马、苹果↔橙子等任务中，生成图像在纹理和颜色上高度逼真。 季节转换： 冬季↔夏季景观转换中，能合理调整植被颜色和光照（如积雪→绿树）。 照片增强： 智能手机拍摄的花卉照片→DSLR浅景深效果，部分成功。 ### 局限性分析 几何变换能力弱：在需改变物体形状的任务（如狗↔猫）中，生成器仅调整颜色/纹理，无法改变结构（见图17）。 数据分布依赖性：训练数据缺乏多样性时易失败（如未包含“骑马”场景的斑马数据集）。 与有监督方法的差距：复杂任务（如语义分割）中，CycleGAN可能混淆类别（如树↔建筑），需弱监督辅助。 ### 与神经风格迁移（Gatys et al.）的对比 优势： CycleGAN可学习整体风格集合（如莫奈全部作品），而Gatys方法依赖单一样本。 生成结果更自然逼真（见图15 16），尤其在需要高真实性的任务（如照片增强）中表现更优。 劣势： Gatys方法在单样本风格迁移中更灵活，而CycleGAN需针对每个风格集合重新训练。 然而，CycleGAN在几何变换和复杂语义任务中仍有局限，未来需结合弱监督或更强大的生成架构进一步突破。"},"/StyleTransfer/ref_and_notes/unet.html":{"title":"U-Net 卷积网络","content":" title: U Net 卷积网络 keywords: UNet desc: U Net文献及笔记 date: 2025 02 26 id: ref_UNet [U Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) *Ronneberger O , Fischer P , Brox T .U Net: Convolutional Networks for Biomedical Image Segmentation[C]//International Conference on Medical Image Computing and Computer Assisted Intervention.Springer International Publishing, 2015.DOI:10.1007/978 3 319 24574 4_28.* > There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end to end from very few images and outperforms the prior best method (a sliding window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at [this](http://lmb.informatik.uni freiburg.de/people/ronneber/u net) . **摘要**：人们普遍认为，深度网络的成功训练需要数千个带注释的训练样本。在本文中，我们提出了一种网络和训练策略，该策略依赖于大量据增强来更有效地使用可用的带注释样本。该架构由捕获上下文的收缩路径和实现精确定位的对称扩展路径组成。我们证明，这种网络可以从很少的图像进行端到端的训练，并且在电子显微镜堆栈中神经元结构分割的 ISBI 挑战中优于先前的最佳方法（滑动窗口卷积网络）。使用在透射光显微镜图像（相位对比和DIC）上训练的相同网络，我们在这些类别中以很大的优势赢得了2015年ISBI细胞跟踪挑战赛。此外，网络速度很快。在最近的GPU上，分割512x512图像所需的时间不到一秒钟。所有的实施（基于Caffe）和训练后的网络可在以下网址获得 [http://lmb.informatik.uni freiburg.de/people/ronneber/u net](http://lmb.informatik.uni freiburg.de/people/ronneber/u net). ## U Net 模型 U 型网络结构由一个收缩路径和一个对称的扩展路径组成，能够有效地捕捉上下文和实现精确的局部定位： 1. **收缩路径（下采样）**：通过重复 3 × 3 卷积、ReLU 激活和 2 × 2 最大池化捕获信息，提取高层语义特征。 2. **扩展路径（上采样）**：通过转置卷积（上卷积）恢复空间分辨率，同时跳跃连接将收缩路径的浅层特征与扩展路径的深层特征拼接，结合细节与语义信息，提升定位深度。 *通过卷积操作，无全连接层，支持任意尺寸输入。* **数据增强策略**：针对生物医学数据标注稀缺的问题，采用弹性变形（随机位移向量的平滑插值）、旋转、平移等增强方法，显著提升模型对形变和噪声的鲁棒性。 **加权损失函数**：通过预计算权重图，平衡类别频率并强化相邻细胞边界区域的学习。使用形态学操作计算边界距离，权重公式： $$ w(\\text{x}) w_c(\\text{x}) + w_0 \\cdot \\exp\\left( \\frac {(d_1(x) + d_2(x))^2} {2 \\sigma^2}\\right) $$ 能够有效地捕捉上下文和实现精确的局部定位 ## 网络理论 在许多视觉任务中，特别是在生物医学图像处理中，期望的输出应该包括定位，即为每个像素分配一个类标签。U Net 网络可以进行图像中的定位。 网络体系结构如下图： ![](../static/images/UNet/fig1.jpg) *包括一条收缩路径（左侧）和一条扩张路径（右侧）。每个蓝色框对应一个多通道特征图，通道数量显示在方框顶部，x y 尺寸位于方框的左下。白框表示复制的要素图。箭头表示不同的操作。* 收缩路径与普通的卷积网络相同，通过一系列的卷积和池化操作。两个 3 × 3 卷积（未填充卷积）的重复应用组成，每个卷积后面都有一个 ReLU 单元和一个 2 × 2 最大池化操作，步幅为 2，用于下采样。 扩展路径中的每一步都包括特征映射的上采样，然后进行 2 × 2 卷积（“上卷积”），将特征通道的数量减半，与收缩路径中相应裁剪的特征映射进行连接，以及两个 3 × 3 卷积，每个卷积后面都有一个 ReLU。 在最后一层，使用 1 × 1 卷积将每个 64 个组件的特征向量映射到所需的类数。这个网络总共有 23 个卷积层。 ## 网络训练 ### 输入与训练配置 1. 无填充卷积（Valid Convolution）：由于未填充的卷积，输出图像小于恒定边界宽度的输入，输出尺寸缩小，需对特征图进行裁剪以实现跳跃连接的尺寸匹配。 2. 单张图像作为批次（batch size 1）：为了最小化开销并最大限度地利用 GPU 内存，倾向于大输入块，而不是大批量。 ### 损失函数设计 **损失函数由最终特征图上的像素级的 soft max 与交叉熵损失函数相结合来计算。** 网络的最终输出通过逐像素soft max生成类别概率图，soft max公式为： $$ p_k(\\text{x}) \\frac {\\exp(a_k(\\text{x}))} {\\sum_{k' 1}^K \\exp(a_{k'}(\\text{x}))} $$ 其中，$a_k(\\text{x})$ 是像素 $\\text{x}$ 在第 $k$ 个通道的激活值，$K$ 为类别数。 $p_k(\\text{x})$ 是近似的最大值函数。例如，对于 $a_k(\\text{x})$ 最大的 $k$，$p_k(\\text{x})\\approx 1$；对于所有其他 $k$，$p_k(\\text{x})\\approx 0$。 交叉熵损失函数计算预测概率与真实标签的差异： $$ E \\sum_{\\text{x} \\in \\Omega} w(\\text{x}) \\log (p_{\\ell (\\text{x})}(\\text{x})) $$ $\\ell(\\text{x})$ 是像素的真实类别标签，$w(\\text{x})$ 是预计算的权重图。 权重图的作用： 类别平衡：某些类别（如细胞边界）像素稀少，通过权重图 $w(\\text{x})$ 增加其损失权重，避免模型忽略小目标。 强化边界学习：对相邻细胞的接触边界赋予更高权重，迫使网络精确区分相邻对象。权重计算公式为（其中，$d_1$ 和 $d_2$ 分别是像素到最近细胞边界和第二近细胞边界的距离，$w_0 10$ 和 $\\sigma \\approx 5$ 控制权重强度和衰减范围）： $$ w(\\text{x}) w_c(\\text{x}) + w_0 \\cdot \\exp\\left( \\frac {(d_1(x) + d_2(x))^2} {2 \\sigma^2}\\right) $$ ### 权重初始化策略 高斯分布初始化，其中标准差为 $\\sqrt{2 / N}$，$N$ 是每个神经元的输入节点数。 对于 3 × 3 卷积核且前一层的特征通道数为 64，则 $N 3 \\times 3 \\times 64 576$ ，标准差为 $\\sqrt{2/576}\\approx 0.059$。 确保每层输出的特征图具有近似单位方差，避免梯度爆炸或消失。 ### 数据增强方法 1. 弹性形变： 生成方式：在 3 × 3 的粗糙网格上生成随机位移向量（服从高斯分布，标准差 10 像素），通过双三次插值得到每个像素的平滑位移场，对训练图像施加形变。 作用：模拟生物组织的自然形变，增强模型对形状变化的鲁棒性，减少对标注数据量的依赖。 2. 平移与旋转：增加模型对位置和角度的不变性。 3. 灰度变换：适应显微镜图像的光照变化。 4. 隐式增强：在收缩路径末端加入Dropout层（随机丢弃部分神经元），防止过拟合并提升泛化能力。 ### 优化器与超参数 随机梯度下降（SGD）：使用 Caffe 框架的SGD实现，动量（momentum）设为0.99，加速收敛并平滑参数更新方向。 高动量的意义：在单批次训练下，高动量使优化过程受历史梯度影响更大，缓解单样本噪声带来的波动。 ## 代码 U Net 模型详细代码在 [Github milesial/Pytorch UNet](https://github.com/milesial/Pytorch UNet/tree/master)。 简单表示为： <details> <summary>U Net 模型</summary> ```python class UNet(nn.Module): def __init__(self, n_channels, n_classes, bilinear False): super(UNet, self).__init__() self.n_channels n_channels self.n_classes n_classes self.bilinear bilinear self.inc (DoubleConv(n_channels, 64)) self.down1 (Down(64, 128)) self.down2 (Down(128, 256)) self.down3 (Down(256, 512)) factor 2 if bilinear else 1 self.down4 (Down(512, 1024 // factor)) self.up1 (Up(1024, 512 // factor, bilinear)) self.up2 (Up(512, 256 // factor, bilinear)) self.up3 (Up(256, 128 // factor, bilinear)) self.up4 (Up(128, 64, bilinear)) self.outc (OutConv(64, n_classes)) def forward(self, x): x1 self.inc(x) x2 self.down1(x1) x3 self.down2(x2) x4 self.down3(x3) x5 self.down4(x4) x self.up1(x5, x4) x self.up2(x, x3) x self.up3(x, x2) x self.up4(x, x1) logits self.outc(x) return logits ``` </details> ## U Net 在风格迁移上的作用 U Net 可以作为风格迁移的生成器。 风格迁移通常包括两个部分： **内容损失**：这是风格迁移中要保留的图像结构部分，要求生成图像的内容尽可能与输入的内容图像保持一致。 **风格损失**：这部分要求生成图像在纹理和颜色等方面尽量接近风格图像。 ### U Net 的结构特点 U Net 是一个典型的 **编码器 解码器结构**，它由两个主要部分组成： **编码器（Contracting Path）**：通过一系列卷积层和池化层逐步压缩图像的空间维度，同时提取图像的高层次特征（如纹理、边缘等）。这部分类似于许多生成模型中的特征提取部分，从内容图像中提取出详细的结构特征和低层次的纹理信息。 **解码器（Expanding Path）**：通过上采样操作逐步恢复图像的空间分辨率，并结合从编码器获得的特征信息进行精细的图像重建。通过跳跃连接，模型将这些特征与风格图像的特征相结合，并生成最终的风格迁移图像。 其中，**跳跃连接（Skip Connections）** 是 U Net 的关键特性之一。跳跃连接能够将编码器中的高分辨率特征直接传递到解码器中，从而帮助解码器在生成图像时更好地保留细节和空间信息。 ### U Net 在风格迁移中的优势 **保留细节**：风格迁移的目标是将风格图像的艺术风格应用到内容图像上，而这需要保留内容图像中的细节和结构。U Net 的跳跃连接使得解码器能够从编码器中获取高分辨率的特征，这有助于保持内容图像的空间结构。对于风格迁移任务，这一点尤其重要，因为风格迁移不仅仅是改变图像的纹理或颜色，还需要尽量保留原图的形状和结构。 **生成精细的图像**：U Net 的解码器部分逐渐增加图像的分辨率，并逐步生成更精细的图像细节。在风格迁移任务中，解码器通过学习将风格图像的纹理特征应用到内容图像的不同区域，从而在保持内容的同时，赋予图像新的风格。 **灵活的训练方式**：U Net 模型可以通过预训练的网络（如 VGG16）提取图像的内容和风格特征，并在训练过程中最小化内容损失和风格损失的加权和。在风格迁移的过程中，U Net 的特征提取能力使得模型能够更好地理解图像的局部细节和整体结构，从而使得生成的图像既有风格图像的艺术效果，又保留了内容图像的特征。 ### 实际操作中的优势 **逐步生成图像**：通过解码器的逐步上采样，U Net 可以从低分辨率到高分辨率逐步生成细节，类似于其他生成模型（如 GAN）中的生成过程。在风格迁移中，这样的逐步生成有助于图像的细节平滑过渡，从而避免了粗糙的风格转化效果。 **适应不同的风格图像**：U Net 不依赖于特定的风格图像，而是通过学习生成适应不同风格图像的合成图像。因此，它可以适用于各种艺术风格的迁移。 ### 与其他生成模型（如 GAN）对比 虽然 GAN（生成对抗网络）在风格迁移中也有很大的应用，但 U Net 在一些风格迁移任务中仍然是一个不错的选择，特别是在一些细节保留和训练稳定性方面： **训练稳定性**：相比于 GAN，U Net 在训练过程中可能更稳定，因为它不涉及生成器和判别器的博弈。GAN 的训练容易出现不稳定的情况（如模式崩溃等问题），而 U Net 通过直接优化损失函数（内容损失和风格损失）来训练，因此在风格迁移的过程中，通常会更容易收敛。 **图像质量**：U Net 生成的图像质量和细节也非常高，尤其是在风格迁移中，跳跃连接帮助保留了图像的结构特征，使得风格转移更自然，避免了图像过度平滑或失真。 ### VGG19 和 U Net 进行风格迁移 风格迁移简单 demo —— VGG19 + U Net，详细代码在：[Github](https://github.com/Fingsinz/StyleTransfer/blob/main/src/01.ref_and_note/02.UNet_StyleTransfer.py) 使用 VGG19 的 conv1_2 和 conv2_2 计算风格损失（捕捉纹理） 使用 conv3_4 和 conv4_4 计算内容损失（保留结构） U Net架构： 4个下采样块，每个块包含两次卷积 使用转置卷积进行上采样 通过中心裁剪实现跳跃连接的特征对齐 U Net 实验效果如下： <table> <th>Content</th> <th>Style</th> <th>Result</th> <tr> <td><img src \"../static/images/UNet/content.JPEG\"></td> <td><img src \"../static/images/UNet/style.jpg\"></td> <td><img src \"../static/images/UNet/result.png\"></td> </tr> </table>"},"/StyleTransfer/ref_and_notes/vgg.html":{"title":"VGG 网络","content":" title: VGG 网络 keywords: VGG desc: VGG文献及笔记 date: 2025 03 05 id: ref_VGG [Very Deep Convolutional Networks for Large Scale Image Recognition](https://arxiv.org/abs/1409.1556) *Simonyan K , Zisserman A .Very Deep Convolutional Networks for Large Scale Image Recognition[J].Computer Science, 2014.DOI:10.48550/arXiv.1409.1556.* > In this work we investigate the effect of the convolutional network depth on its accuracy in the large scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior art configurations can be achieved by pushing the depth to 16 19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state of the art results. We have made our two best performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. **摘要**：在这项工作中，我们研究了卷积网络深度对其在大规模图像识别设置中的准确性的影响。我们的主要贡献是使用具有非常小（3x3）卷积滤波器的架构对增加深度的网络进行全面评估，这表明通过将深度推至 16 19 权重层可以实现对现有技术配置的显着改进。这些发现是我们 2014 年 ImageNet 挑战赛提交的基础，我们的团队分别在本地化和分类轨道中获得了第一名和第二名。我们还表明，我们的方法可以很好地推广到其他数据集，从而获得最先进的结果。我们已经公开了两个性能最好的卷积神经网络模型，以促进对计算机视觉中深度视觉表示的进一步研究。 ## VGG 网络架构 架构设计创新： 固定输入尺寸为 224 × 224，采用统一的小卷积核（3 × 3）和步长（1），通过填充保持空间分辨率，配合最大池化（2 × 2，步长 2）逐步下采样。 移除局部响应归一化（LRN），因其对性能提升有限且增加计算开销。 全连接层统一为 3 层（4096 4096 1000），最后一层为 SoftMax 分类器。 ## VGG 网络配置 翻译自论文中表格 1。 <table style \"text align:center\"> <caption>ConvNet Configuration</caption> <tr> <th>A</th> <th>A LRN</th> <th>B</th> <th>C</th> <th>D</th> <th>E</th> </tr> <tr> <td>11 层</td> <td>11 层</td> <td>13 层</td> <td>16 层</td> <td>16 层</td> <td>19 层</td> </tr> <tr> <td colspan \"6\">输入层 224 x 224 RGB 图像</td> </tr> <tr> <td>conv3 64</td> <td>conv3 64 <br /> <strong>LRN</strong> </td> <td>conv3 64 <br /> <strong>conv3 64</strong> </td> <td>conv3 64 <br /> conv3 64</td> <td>conv3 64 <br /> conv3 64</td> <td>conv3 64 <br /> conv3 64</td> </tr> <tr> <td colspan \"6\">最大池化</td> </tr> <tr> <td>conv3 128</td> <td>conv3 128</td> <td>conv3 128 <br /> <strong>conv3 128</strong> </td> <td>conv3 128 <br /> conv3 128 </td> <td>conv3 128 <br /> conv3 128 </td> <td>conv3 128 <br /> conv3 128 </td> </tr> <tr> <td colspan \"6\">最大池化</td> </tr> <tr> <td>conv3 256 <br /> conv3 256 </td> <td>conv3 256 <br /> conv3 256 </td> <td>conv3 256 <br /> conv3 256 </td> <td>conv3 256 <br /> conv3 256 <br /> <strong>conv1 256</strong></td> <td>conv3 256 <br /> conv3 256 <br /> <strong>conv3 256</strong></td> <td>conv3 256 <br /> conv3 256 <br /> conv3 256 <br /> <strong>conv3 256</strong></td> </tr> <tr> <td colspan \"6\">最大池化</td> </tr> <tr> <td>conv3 512 <br /> conv3 512 </td> <td>conv3 512 <br /> conv3 512 </td> <td>conv3 512 <br /> conv3 512 </td> <td>conv3 512 <br /> conv3 512 <br /> <strong>conv1 512</strong></td> <td>conv3 512 <br /> conv3 512 <br /> <strong>conv3 512</strong></td> <td>conv3 512 <br /> conv3 512 <br /> conv3 512 <br /> <strong>conv3 512</strong></td> </tr> <tr> <td colspan \"6\">最大池化</td> </tr> <tr> <td>conv3 512 <br /> conv3 512 </td> <td>conv3 512 <br /> conv3 512 </td> <td>conv3 512 <br /> conv3 512 </td> <td>conv3 512 <br /> conv3 512 <br /> <strong>conv1 512</strong></td> <td>conv3 512 <br /> conv3 512 <br /> <strong>conv3 512</strong></td> <td>conv3 512 <br /> conv3 512 <br /> conv3 512 <br /> <strong>conv3 512</strong></td> </tr> <tr> <td colspan \"6\">最大池化</td> </tr> <tr> <td colspan \"6\">全连接层（FC 4096）</td> </tr> <tr> <td colspan \"6\">全连接层（FC 4096）</td> </tr> <tr> <td colspan \"6\">全连接层（FC 1000）</td> </tr> <tr> <td colspan \"6\">soft max</td> </tr> </table> 通过逐步增加卷积层数（11 层到 19 层），验证了网络深度的增加能显著提升分类精度。最佳模型（**VGG 16** 和 **VGG 19**）在 ImageNet 2014 挑战赛的分类和定位任务中分别取得第二和第一的成绩。 用三个 3 × 3 的卷积层代替一个 7 × 7 的卷积层： 1. 结合了三个非线性校正层而不是单个校正层，这使得决策函数更具区分性； 2. 减少了参数的数量，三个 3 × 3 卷积层的叠加等效于一个 7 × 7 卷积层，但参数量减少 81%。 3. 这可以看作是对 7 × 7 卷积滤波器施加正则化，迫使它们通过 3 × 3 滤波器（中间注入非线性）进行分解。 1 × 1 卷积层（配置 C，表 1）的结合是一种在不影响卷积层感受野的情况下增加决策函数的非线性的方法。 ## 文中分类实验 实验证明，增加网络深度（16 19层）能显著提升模型表达能力。 小卷积核的效益：通过叠加 3 × 3 卷积，在保持感受野的同时减少参数量，并引入更多非线性激活（ReLU），增强模型表达能力。 全卷积网络：测试时通过全卷积化处理任意尺寸输入，避免重复计算多个裁剪区域，显著提升效率。 多尺度策略：训练和测试时的尺度抖动有效提升模型对尺寸变化的适应能力。 参数共享：通过浅层网络预训练初始化深层网络的前几层和全连接层，加速收敛并缓解梯度不稳定问题。 ## 代码自实现 基于 VGG 网络的 CIFAR 10 分类实验代码在 [Github](https://github.com/Fingsinz/StyleTransfer/blob/main/src/01.ref_and_note/04.VGG.py)。 实验记录如下： <table> <tr> <td><img src \"../static/images/VGG/fig1.png\" /></td> <td><img src \"../static/images/VGG/fig2.png\" /></td> </tr> </table> **_my 表示为自实现，_py 表示调用 torchvision.models 的模型* <details> <summary>VGG16</summary> ```python class VGG16(nn.Module): def __init__(self, in_channels 3, num_classes 10): super(VGG16, self).__init__() self.features nn.Sequential( self._make_conv_block(in_channels, 64, 2), # conv3 64 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(64, 128, 2), # conv3 128 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(128, 256, 3), # conv3 256 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(256, 512, 3), # conv3 512 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(512, 512, 3), # conv3 512 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool ) self.avgpool nn.AdaptiveAvgPool2d((7, 7)) # 全局平均池化 self.classifier nn.Sequential( nn.Linear(512 * 7 * 7, 4096), # 全连接层（FC 1） nn.ReLU(inplace True), nn.Dropout(0.5), nn.Linear(4096, 4096), # 全连接层（FC 2） nn.ReLU(inplace True), nn.Dropout(0.5), nn.Linear(4096, num_classes), # 全连接层（FC 3） ) # nn.CrossEntropyLoss() 计算损失时隐式 soft max def _make_conv_block(self, in_channels, out_channels, num_blocks, kernel_size 3, padding 1): layers [] for _ in range(num_blocks): layers.append(nn.Conv2d(in_channels, out_channels, kernel_size kernel_size, padding padding)) layers.append(nn.BatchNorm2d(out_channels)) # 每个卷积层后加入nn.BatchNorm2d，显著提升训练稳定性 layers.append(nn.ReLU(inplace True)) in_channels out_channels return nn.Sequential(*layers) def forward(self, x): x self.features(x) x self.avgpool(x) x torch.flatten(x, 1) x self.classifier(x) return x ``` </details> <details> <summary>VGG19</summary> ```python class VGG19(nn.Module): def __init__(self, in_channels 3, num_classes 10): super(VGG19, self).__init__() self.features nn.Sequential( self._make_conv_block(in_channels, 64, 2), # conv3 64 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(64, 128, 2), # conv3 128 × 2 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(128, 256, 4), # conv3 256 × 4 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(256, 512, 4), # conv3 512 × 4 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool self._make_conv_block(512, 512, 4), # conv3 512 × 4 nn.MaxPool2d(kernel_size 2, stride 2), # maxpool ) self.avgpool nn.AdaptiveAvgPool2d((7, 7)) # 全局平均池化 self.classifier nn.Sequential( nn.Linear(512 * 7 * 7, 4096), # FC 1 nn.ReLU(inplace True), nn.Dropout(0.5), nn.Linear(4096, 4096), # FC 2 nn.ReLU(inplace True), nn.Dropout(0.5), nn.Linear(4096, num_classes), # FC 3 ) def _make_conv_block(self, in_channels, out_channels, num_blocks, kernel_size 3, padding 1): layers [] for _ in range(num_blocks): layers.append(nn.Conv2d(in_channels, out_channels, kernel_size kernel_size, padding padding)) layers.append(nn.BatchNorm2d(out_channels)) # 每个卷积层后加入nn.BatchNorm2d，显著提升训练稳定性 layers.append(nn.ReLU(inplace True)) in_channels out_channels return nn.Sequential(*layers) def forward(self, x): x self.features(x) x self.avgpool(x) x torch.flatten(x, 1) x self.classifier(x) return x ``` </details>"},"/StyleTransfer/ref_and_notes/pytorch_install.html":{"title":"PyTorch 安装","content":" title: PyTorch 安装 keywords: PyTorch desc: PyTorch 安装 date: 2025 02 07 id: pytorch Reference: [Zero to Mastery Learn PyTorch for Deep Learning](https://www.learnpytorch.io/) ## PyTorch 基本环境搭建 1. 创建并激活环境 ```bat python m venv [venv name] [venv name]\\Scripts\\activate ``` 2. 安装 Pytorch ```bat pip install torch torchvision torchaudio ``` 验证 Pytorch 安装，出现版本号则为正常。 ## PyTorch GPU 环境搭建 在搭建虚拟环境后，如果需要在 GPU 上运行，需要安装 PyTorch GPU 版本。 1. 确定自己的 GPU CUDA 版本。 ```bat nvidia smi ``` 2. 下载对应的 PyTorch GPU 版本。[官方引导下载](https://pytorch.org/get started/locally/) 附镜像页面链接： PyTorch官方镜像 [Torch](https://download.pytorch.org/whl/torch/) [TorchVision](https://download.pytorch.org/whl/torchvision/) [TorchAudio](https://download.pytorch.org/whl/torchaudio/) [阿里云镜像源](https://mirrors.aliyun.com/pytorch wheels/) 支持的 CUDA：10.0、10.1、10.2、11.0、11.1、11.3、11.5、11.6、11.7、11.8、12.1 3. 检测是否可用。 ```python import torch print(torch.cuda.is_available()) ```"},"/StyleTransfer/ref_and_notes/pytorch_modular.html":{"title":"PyTorch 模块化","content":" title: PyTorch 模块化 keywords: PyTorch desc: PyTorch 模块化 date: 2025 02 13 id: pytorch_modular Reference: [PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/) ## 前言 模块化后的 Python 文件： `data_setup.py`：用于准备和下载数据的文件。 `engine.py`：包含各种训练函数的文件。 `model_builder.py` 或 `model.py`：创建 PyTorch 模型的文件。 `train.py`：利用所有其他文件并训练目标 PyTorch 模型的文件 `utils.py`：专用于有用的实用程序功能的文件。 然后就可以使用类似于下面的命令调用脚本进行模型训练： ```bat python train.py model tinyvgg batch_size 32 lr 0.001 num_epochs 10 ``` 模块化后的目录： ``` root/ ├── data_setup.py ├── engine.py ├── model.py ├── train.py └── utils.py └── models/ │ ├── xxx.pth └── data/ ├── train/ │ └── xxx.jpg └── test/ └── xxx.jpg ``` ## data_setup.py：建立 Datasets 和 DataLoaders 获得数据后，将其转换为 PyTorch `Dataset` 和 `DataLoader` 。 将 `Dataset` 和 `DataLoader` 创建代码转换为一个名为 `create_dataloaders()` 的函数。 <details> <summary>data_setup.py</summary> ```python \"\"\" @file: data_setup.py @brief: 包含从图像文件夹创建 PyTorch dataloader 的函数。 @author: @date: 2025 02 13 \"\"\" import os from torchvision import datasets, transforms from torch.utils.data import DataLoader NUM_WORKERS 3 def create_dataloaders( train_dir: str, test_dir: str, transform: transforms.Compose, batch_size: int, num_workers: int NUM_WORKERS, ) > tuple[DataLoader, DataLoader, list[str]]: \"\"\" 从图像文件夹创建PyTorch dataloader。 参数: train_dir (str): 训练图像文件夹的路径。 test_dir (str): 测试图像文件夹的路径。 transform (transforms.Compose): 应用的图像变换。 batch_size (int): DataLoader 的批量大小。 num_workers (int): 用于加载数据的工作线程数。 返回: 包含训练 DataLoader、测试 DataLoader 和类名列表的元组。 \"\"\" train_data datasets.ImageFolder(train_dir, transform transform) test_data datasets.ImageFolder(test_dir, transform transform) class_names train_data.classes train_dataloader DataLoader( train_data, batch_size batch_size, shuffle True, num_workers num_workers, pin_memory True, ) test_dataloader DataLoader( test_data, batch_size batch_size, shuffle True, num_workers num_workers, pin_memory True, ) return train_dataloader, test_dataloader, class_names ``` </details> ## model.py: 构建模型 将模型放入其文件中使得可以一次又一次地重用它。 <details> <summary>data_setup.py</summary> ```python \"\"\" @file: model.py @brief: TinyVGG 定义 @author: @date: 2025 02 13 \"\"\" import torch import torch.nn as nn class TinyVGG(nn.Module): \"\"\" 一个简单的 VGG 类神经网络图像分类任务。 参数: input_channels (int): 输入通道数 (e.g., 3 for RGB images)。 hidden_units (int): 卷积层中隐藏单元的数量。 output_shape (int): 输出类的数量。 \"\"\" def __init__(self, input_channels: int, hidden_units: int, output_shape: int) > None: super().__init__() self.block_1 nn.Sequential( nn.Conv2d(in_channels input_channels, out_channels hidden_units, kernel_size 3, stride 1, padding 1), nn.ReLU(), nn.Conv2d(in_channels hidden_units, out_channels hidden_units, kernel_size 3, stride 1, padding 1), nn.ReLU(), nn.MaxPool2d(2) ) self.block_2 nn.Sequential( nn.Conv2d(hidden_units, hidden_units, kernel_size 3, padding 1), nn.ReLU(), nn.Conv2d(hidden_units, hidden_units, kernel_size 3, padding 1), nn.ReLU(), nn.MaxPool2d(2) ) self.classifier nn.Sequential( nn.Flatten(), nn.Linear(in_features hidden_units * 16 * 16, out_features output_shape) ) def forward(self, x: torch.Tensor) > torch.Tensor: \"\"\" 前向传播 参数: x (torch.Tensor): 输入张量，形状：(batch_size, input_channels, height, width)。 返回: torch.Tensor: 输出张量，形状：(batch_size, output_shape)。 \"\"\" return self.classifier(self.block_2(self.block_1(x))) ``` </details> ## engine.py：训练和测试模型 在前面编写了几个训练函数： `train_step()`：接受一个模型、一个 DataLoader、一个损失函数和一个优化器，并在 DataLoader 上训练一个 step。 `test_step`：接受一个模型、一个 DataLoader 和一个损失函数，并在 DataLoader 上测试模型。 `train()`：执行 `train_step()` 和 `test_step()`。并返回一个结果字典。 <details> <summary>engine.py</summary> *需要下载 tqdm 包。* ```python \"\"\" @file: engine.py @brief: PyTorch模型的训练和测试函数 @author: @date: 2025 02 13 \"\"\" import torch from tqdm.auto import tqdm from typing import Dict, List, Tuple def train_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, device: torch.device) > Tuple[float, float]: \"\"\" 模型逐步训练 参数: model: 要训练的模型 dataloader: 用于训练的 DataLoader loss_fn: 损失函数 optimizer: 优化器 device: 设备 (e.g. GPU or CPU) 返回值: 包含训练损失和正确率的元组 \"\"\" model.train() train_loss, train_acc 0, 0 for batch, (X, y) in enumerate(dataloader): X, y X.to(device), y.to(device) y_pred model(X) loss loss_fn(y_pred, y) train_loss + loss.item() optimizer.zero_grad() loss.backward() optimizer.step() y_pred_class torch.argmax(torch.softmax(y_pred, dim 1), dim 1) train_acc + (y_pred_class y).sum().item() / len(y_pred) train_loss train_loss / len(dataloader) train_acc train_acc / len(dataloader) return train_loss, train_acc def test_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, device: torch.device) > Tuple[float, float]: \"\"\" 模型逐步测试 参数: model: 要测试的模型 dataloader: 用于测试数据的 DataLoader loss_fn: 损失函数 device: 设备 (e.g. GPU or CPU) 返回值: 包含训练损失和正确率的元组 \"\"\" model.eval() test_loss, test_acc 0, 0 with torch.inference_mode(): for batch, (X, y) in enumerate(dataloader): X, y X.to(device), y.to(device) test_pred_logits model(X) loss loss_fn(test_pred_logits, y) test_loss + loss.item() test_pred_labels test_pred_logits.argmax(dim 1) test_acc + ((test_pred_labels y).sum().item() / len(test_pred_labels)) test_loss test_loss / len(dataloader) test_acc test_acc / len(dataloader) return test_loss, test_acc def train(model: torch.nn.Module, train_dataloader: torch.utils.data.DataLoader, test_dataloader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer, loss_fn: torch.nn.Module, epochs: int, device: torch.device) > Dict[str, List]: \"\"\" 训练模型多个 epoch 参数: model: 训练的模型 train_dataloader: 训练数据的 DataLoader test_dataloader: 测试数据的 DataLoader optimizer: 优化器 loss_fn: 损失函数 epochs: 训练轮数 device: 设备 (e.g. GPU or CPU) 返回值: 包含训练和测试损失和正确率的字典 \"\"\" results {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": [] } for epoch in tqdm(range(epochs)): train_loss, train_acc train_step(model model, dataloader train_dataloader, loss_fn loss_fn, optimizer optimizer, device device) test_loss, test_acc test_step(model model, dataloader test_dataloader, loss_fn loss_fn, device device) print( f\"\\nEpoch: {epoch+1} \" f\"train_loss: {train_loss:.4f} \" f\"train_acc: {train_acc:.4f} \" f\"test_loss: {test_loss:.4f} \" f\"test_acc: {test_acc:.4f}\" ) results[\"train_loss\"].append(train_loss) results[\"train_acc\"].append(train_acc) results[\"test_loss\"].append(test_loss) results[\"test_acc\"].append(test_acc) return results ``` </details> ## utils.py：实用函数集合 通常情况下，在训练期间或训练后需要保存模型。 将 helper 函数存储在名为 `utils.py` （utilities的缩写）的文件中。 <details> <summary>utils.py</summary> ```python \"\"\" @file: utils.py @brief: 包含实用函数 @author: @date: 2025 02 13 \"\"\" import torch from pathlib import Path from typing import Union def save_model(model: torch.nn.Module, target_dir: str, model_name: str): \"\"\" 保存模型 参数: model (torch.nn.Module): 要保存的模型。 target_dir (str): 保存模型的目标目录。 model_name (str): 保存的模型文件的名称。 异常: 断言错误: 如果 model_name 不以.pt或. pth结尾。 \"\"\" target_dir_path Path(target_dir) target_dir_path.mkdir(parents True, exist_ok True) assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with .pt or .pth\" model_save_path target_dir_path / model_name print(f\"[INFO] Saving model to: {model_save_path}\") torch.save(obj model.state_dict(), f model_save_path) ``` </details> ## train.py：训练、评估和保存模型 在其它项目里，经常会遇到将其所有功能组合在一个 `train.py` 文件中。 在这里的 `train.py` 文件中，将结合我们创建的其他 Python脚本的所有功能，并使用它来训练模型。 有以下步骤： 1. 从目录中导入 `data_setup`、`engine`、`model`、`utils` 等各种依赖项。 2. 设置各种超参数，如批量大小，epoch数，学习率和隐藏单元数（这些可以在将来通过 Python 的 argparse 设置）。 3. 设置训练和测试目录。 4. 设置使用设备。 5. 创建必要的数据转换。 6. 使用 `data_setup.py` 创建 DataLoader。 7. 使用 `model.py` 创建模型。 8. 设置损失函数和优化器。 9. 使用 `engine.py` 训练模型。 10. 使用 `utils.py` 保存模型。 <details> <summary>train.py</summary> ```python \"\"\" @file: train.py @brief: 训练模型 @author: @date: 2025 02 13 \"\"\" import os import torch import data_setup, engine, model, utils from torchvision import transforms # 训练超参数 NUM_EPOCHS 5 BATCH_SIZE 32 HIDDEN_UNITS 10 LEARNING_RATE 0.001 # 数据路径 train_dir \"../data/pizza_steak_sushi/train\" test_dir \"../data/pizza_steak_sushi/test\" # 训练设备 device \"cuda\" if torch.cuda.is_available() else \"cpu\" if __name__ \"__main__\": # 数据预处理 train_transform transforms.Compose([ transforms.Resize((64, 64)), transforms.ToTensor(), ]) # 创建数据加载器 train_dataloader, test_dataloader, class_names data_setup.create_dataloaders( train_dir train_dir, test_dir test_dir, transform train_transform, batch_size BATCH_SIZE, ) # 创建模型 model model.TinyVGG( input_channels 3, hidden_units HIDDEN_UNITS, output_shape len(class_names), ).to(device) # 设置损失函数和优化器 loss_fn torch.nn.CrossEntropyLoss() optimizer torch.optim.Adam(params model.parameters(), lr LEARNING_RATE) # 启动训练 engine.train(model model, train_dataloader train_dataloader, test_dataloader test_dataloader, optimizer optimizer, loss_fn loss_fn, epochs NUM_EPOCHS, device device) # 保存模型 utils.save_model(model model, target_dir \"models\", model_name \"tinyvgg_model.pth\") ``` </details> *可研究 `argparse` 的使用优化。* 测试调用： ```bat python .\\train.py 0% 0/5 [00:00<?, ?it/s] Epoch: 1 train_loss: 1.1113 train_acc: 0.3047 test_loss: 1.0953 test_acc: 0.3400 20%██████████████▌ 1/5 [00:18<01:14, 18.60s/it] Epoch: 2 train_loss: 1.1072 train_acc: 0.2969 test_loss: 1.0744 test_acc: 0.4233 40%█████████████████████████████▏ 2/5 [00:36<00:55, 18.45s/it] Epoch: 3 train_loss: 1.0931 train_acc: 0.4141 test_loss: 1.0908 test_acc: 0.3617 60%███████████████████████████████████████████▊ 3/5 [00:55<00:36, 18.45s/it] Epoch: 4 train_loss: 1.0891 train_acc: 0.4180 test_loss: 1.0931 test_acc: 0.3722 80%██████████████████████████████████████████████████████████▍ 4/5 [01:13<00:18, 18.46s/it] Epoch: 5 train_loss: 1.0622 train_acc: 0.4766 test_loss: 1.0636 test_acc: 0.4621 100%█████████████████████████████████████████████████████████████████████████ 5/5 [01:32<00:00, 18.43s/it] [INFO] Saving model to: models\\tinyvgg_model.pth ```"},"/StyleTransfer/ref_and_notes/pytorch_tensor.html":{"title":"PyTorch 张量","content":"Reference:[ZerotoMasteryLearnPyTorchforDeepLearning](https://www.learnpytorch.io/)*该页面由JupyterNotebook生成，原文件于[Github](https://github.com/Fingsinz/StyleTransfer/tree/main/src/02.pytorch_learning/pytorch_tensor.ipynb)*importtorchtorch.__version__##什么是张量张量用于表示数据，是机器学习的基本组成部分。 图片可以是三维张量，如`[height,width,channel]`，如经典的lena图片用张量表示：importnumpyasnpfromPILimportImage#使用pillow打开图片,转换为numpy矩阵,再转换为torch张量img torch.from_numpy(np.array(Image.open(\"imgs/lena.jpg\")))img.shape##创建张量Tensors说明文档：[https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html)1.Scalar，标量是一个单独的数字，用张量的术语来说是一个零维张量。scalar torch.tensor(3.0)#维度同样可以通过tensor.dim()获取print(f\"scalar为{scalar},维度为{scalar.ndim},常量通过item方法获取{scalar.item()}数字\")scalar为3.0, 维度为0, 常量通过item方法获取3.0数字2.Vector，向量是一个一维张量，类似于数组。vector torch.tensor([1.0,2.0,3.0])print(f\"vector为{vector},维度为{vector.ndim},通过shape属性获取形状{vector.shape}\")vector为tensor([1., 2., 3.]), 维度为1, 通过shape属性获取形状torch.Size([3])3.Matrix，矩阵是一个二维张量。matrix torch.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]])print(f\"{matrix},\\n维度为{matrix.ndim},通过shape属性获取形状{matrix.shape}\")tensor([[1., 2., 3.], [4., 5., 6.]]), 维度为2, 通过shape属性获取形状torch.Size([2, 3])总结：结构表示维度: :: :: :scalar一个数字0vector一组数字1matrix一个矩阵2tensor若干维度0维表示scalar，每一维表示一个vector###`torch.rand()`生成随机张量实际上在机器学习中很少会手动创建张量，更多是随机生成。#创建指定大小的随机张量random_tensor torch.rand(size (3,4))random_tensor,random_tensor.dtype###填充全零或全一张量zeros torch.zeros(size (3,4))ones torch.ones(size (3,4))zeros,zeros.dtypeones,ones.dtype###创建一个范围张量#创建一个从0到9的张量的两种方法#zero_to_ten1 torch.range(0,10)#将弃用zero_to_ten2 torch.arange(start 0,end 10,step 1)#zero_to_ten1,zero_to_ten1.dtypezero_to_ten2,zero_to_ten2.dtype#创建一个形状一样的向量same_shape torch.zeros_like(input zero_to_ten2)same_shape,same_shape.dtype##张量数据类型Tensor的DataTypes：[https://pytorch.org/docs/stable/tensors.html#data types](https://pytorch.org/docs/stable/tensors.html#data types)有些数据类型是特定于CPU，而有些更适合GPU。同时确保精度问题，可以选用不同精度的浮点数类型。float32_tensor torch.tensor([3.0,6.0,9.0],dtype None,#默认为None，即torch。Float32或传递的任何数据类型device None,#默认为None，使用默认的张量类型requires_grad False)#如果为True，则记录对张量执行的操作float32_tensor.shape,float32_tensor.dtype,float32_tensor.device可以修改张量的数据类型：float64_tensor float32_tensor.type(torch.float64)float64_tensor.dtype在进行带张量的操作时，除了张量的Shape要匹配之外，还需要注意张量的dtype和device。 `tensor.shape`：获取Shape。 `tensor.dtype`：获取dtype。 `tensor.device`：获取device。##张量的操作###张量的基础操作张量的加减乘操作如下：test_tensor torch.tensor([1,2,3])test_tensor+10,test_tensor*10,test_tensor 1,test_tensor#在不赋值的时候是不变的也可以通过函数实现：torch.add(test_tensor,10),torch.mul(test_tensor,10),torch.sub(test_tensor,1)注意，**矩阵乘法遵循其规则，与形状相关。**$$M_{m\\timesn} M_{m\\timesk}@M_{k\\timesn}$$*`@`在Python中是矩阵乘法*tensor torch.tensor([1,2,3])tensor*tensor,tensor@tensor,torch.matmul(tensor,tensor)#torch.matmul是矩阵乘法，且比@操作更快 $[1,2,3]*[1,2,3] [1*1,2*2,3*3] [1,4,9]$ $[1,2,3]@[1,2,3] 1*1+2*2+3*3 14$`torch.mm()`是`torch.matmul()`的缩写。另外提供一些操作进行矩阵变换： `torch.transpose(input,dim0,dim1)`，`input`是输入矩阵，`dim0`和`dim1`是要交换的维度。 `torch.T`：转置矩阵。###求最小值、最大值、平均值、总和等x torch.arange(0,100,10)print(x)print(f\"最小值:{x.min()}\")print(f\"最大值:{x.max()}\")#print(f\"Mean:{x.mean()}\")#会报错print(f\"均值:{x.type(torch.float32).mean()}\")#没有float数据类型将无法工作print(f\"总和:{x.sum()}\")tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90]) 最小值: 0 最大值: 90 均值: 45.0 总和: 450*一些方法，如torch.mean()，要求张量位于torch.float32（最常见）或其他特定数据类型中，否则操作将失败。*###求最小最大值的位置print(x)print(f\"Indexwheremaxvalueoccurs:{x.argmax()}\")print(f\"Indexwhereminvalueoccurs:{x.argmin()}\")tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90]) Index where max value occurs: 9 Index where min value occurs: 0###张量形状重塑、堆叠、挤压和扩展因为深度学习模型（神经网络）都是关于以某种方式操纵张量的。因为矩阵乘法的规则，如果有形状不匹配，就会遇到错误。这些方法帮助你确保你的张量的正确元素与其他张量的正确元素混合在一起。方法描述: :: :[torch.reshape(input,shape)](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape)或`torch.Tensor.reshape()`在兼容的情况下把`input`重塑成`shape`的形状[Tensor.view(shape)](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)以不同的形状返回原始张量的视图，但与原始张量共享相同的数据[torch.stack(tensors,dim 0)](https://pytorch.org/docs/1.9.1/generated/torch.stack.html)沿着一个新的维度`dim`连接一系列张量，所有张量必须是相同的大小[torch.squeeze(input)](https://pytorch.org/docs/stable/generated/torch.squeeze.html)挤压`input`，删除值为1的所有维度[torch.unsqueeze(input,dim)](https://pytorch.org/docs/1.9.1/generated/torch.unsqueeze.html)在`dim`处添加值为1的维度并返回[torch.permute(input,dims)](https://pytorch.org/docs/stable/generated/torch.permute.html)返回原始输入的视图，其维度重新排列`tensor.reshape()`：importtorchx torch.arange(1.,9.)x_reshaped x.reshape(1,8)#重塑print(f\"x.shape:{x.shape},x_reshaped.shape:{x_reshaped.shape}\")x.shape: torch.Size([8]), x_reshaped.shape: torch.Size([1, 8])`tensor.view()`：改变视图也会改变原来的张量。x_viewed x.view(2,4)#重塑print(f\"x.shape:{x.shape},x_viewed.shape:{x_viewed.shape}\")#修改x_viewd,x同步变化x_viewed[:,0] 5print(x)print(x_viewed)x.shape: torch.Size([8]), x_viewed.shape: torch.Size([2, 4]) tensor([5., 2., 3., 4., 5., 6., 7., 8.]) tensor([[5., 2., 3., 4.], [5., 6., 7., 8.]])用该函数改变一个张量的视图实际上只会创建同一个张量的新视图。如果想要将新张量在自身之上堆叠五次，可以使用`torch.stack()`来实现。x_stacked torch.stack([x,x,x,x],dim 0)x_stacked同时可以移除单维度：print(x_reshaped.shape)x_squzzed x_reshaped.squeeze()print(x_squzzed.shape)torch.Size([1, 8]) torch.Size([8])与`torch.squeeze()`相反，可以使用`torch.unsqueeze()`在特定索引处添加一个维度值1：print(x_squzzed.shape)x_unsquzzed x_squzzed.unsqueeze(dim 0)print(x_unsquzzed.shape)torch.Size([8]) torch.Size([1, 8])`torch.permute(input,dims)`重排张量的维度：img torch.rand(size (128,256,3))img_permuted img.permute(2,0,1)img.shape,img_permuted.shape###张量取下标importtorchx torch.arange(1,10).reshape(1,3,3)print(f\"{x},{x.shape}\")print(f\"x[0]:\\n{x[0]}\")print(f\"x[0][0]:{x[0][0]}\")print(f\"x[0][0][0]:{x[0][0][0]}\")tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]), torch.Size([1, 3, 3]) x[0]: tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) x[0][0]: tensor([1, 2, 3]) x[0][0][0]: 1*可以使用`:`来指定此维度中的所有值，使用逗号`,`来添加另一个维度。*##Pytorch张量和NumpyNumPy和PyTorch数据结构互转： NumpyArray >PyTorchTensor：`torch.from_numpy(ndarray)`。 PyTorchTensor >NumpyArray：`torch.Tensor.numpy()`。##Tensor随机值`torch.rand()`方法可以生成一个给定大小而值随机的张量，但是每次生成都会不一样。如果需要每次随机都一样，需要固定下随机数种子。importtorchimportrandomRANDOM_SEED 42torch.manual_seed(seed RANDOM_SEED)random_tensor_A torch.rand(3,4)torch.random.manual_seed(seed RANDOM_SEED)random_tensor_B torch.rand(3,4)print(f\"TensorA:\\n{random_tensor_A}\\n\")print(f\"TensorB:\\n{random_tensor_B}\\n\")print(f\"A B?\")print(random_tensor_A random_tensor_B)Tensor A: tensor([[0.8823, 0.9150, 0.3829, 0.9593], [0.3904, 0.6009, 0.2566, 0.7936], [0.9408, 0.1332, 0.9346, 0.5936]]) Tensor B: tensor([[0.8823, 0.9150, 0.3829, 0.9593], [0.3904, 0.6009, 0.2566, 0.7936], [0.9408, 0.1332, 0.9346, 0.5936]]) A B? tensor([[True, True, True, True], [True, True, True, True], [True, True, True, True]])##GPU下使用张量导入PyTorch：importtorchtorch.cuda.is_available()设置设备类型：#Setdevicetypedevice \"cuda\"iftorch.cuda.is_available()else\"cpu\"device检查设备数：torch.cuda.device_count()###张量在CPU和GPU间移动通过调用`to(device)`将张量（和模型）放在特定的设备上。GPU可以提供比CPU更快的数值计算，但有时候某些操作不支持在GPU中执行，所以需要将张量进行移动。张量移动到GPU侧：tensor torch.tensor([1,2,3])print(tensor,tensor.device)tensor_on_gpu tensor.to(device)print(tensor_on_gpu)tensor([1, 2, 3]) cpu tensor([1, 2, 3], device 'cuda:0')张量移动到CPU侧：通过使用`tensor.CPU()`tensor_back_on_cpu tensor_on_gpu.cpu()print(tensor_back_on_cpu)#上面的代码返回CPU内存中GPU张量的副本，原始张量仍然在GPU上。print(tensor_on_gpu)tensor([1, 2, 3]) tensor([1, 2, 3], device 'cuda:0')"},"/StyleTransfer/ref_and_notes/gan.html":{"title":"GAN：生成对抗网络","content":" title: GAN：生成对抗网络 keywords: GAN desc: GAN文献及笔记 date: 2025 02 01 id: ref_GAN [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) *Goodfellow I , Pouget Abadie J , Mirza M ,et al.Generative Adversarial Nets[J].MIT Press, 2014.DOI:10.3156/JSOFT.29.5_177_2.* > We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. **摘要**：论文提出一个通过对抗过程估计生成模型的新框架。在对抗过程中，同时训练两个模型：一个捕获数据分布的生成器 G，和一个估计样本来自训练数据还是生成器 G 的判别器 D。生成器 G 的训练过程是最大化判别器 D 犯错的概率。这个框架相当于一个极大极小二人博弈游戏。在任意的 G 和 D 函数空间中，存在唯一解，使得生成器 G 能复刻训练集的数据分布，同时判别器 D 对于生成器 G 生成的任意样本作出的判断都是真假参半（真假概率各半）。如果生成器 G 和判别器 D 都定义为多层感知器，那么整个系统可以使用误差反向传播进行训练。在模型的训练过程以及样本的生成中，不需要使用马尔科夫链或者展开的近似推理网络。通过对生成的样本进行定性和定量评估，实验证明了该框架的潜力。 ## GAN 模型 GAN包含两个模型： 1. **生成器（Generator, G）**：将随机噪声映射到数据空间，目标是生成与真实数据分布 $p_{\\text{data}}$ 一致的样本。 2. **判别器（Discriminator, D）**：区分输入样本来自真实数据还是生成器，输出为样本真实性的概率。 两者通过**极小极大博弈**进行训练： 为了学习生成器在数据 $x$ 上的分布 $p_g$，定义输入噪声变量 $p_z(z)$ 的先验，然后将数据空间的映射表示为 $G(z; \\theta_g)$，其中 $G$ 是由具有参数 $\\theta_g$ 的多层感知器表示的可微函数。 定义第二个多层感知器 $D(x; \\theta_d)$，其输出一个标量。$D(x)$ 表示 $x$ 来自真实数据而不是 $p_g$ 的概率。 **训练判别器 D，最大化正确分类训练样本和生成器 G 生成样本的概率；同时训练生成器 G，最小化 $\\log(1−D(G(z)))$，即让生成样本 $G(z)$ 被判别器误判为真实样本（$D(G(z))→1$）**。 综上所述，GAN 的训练过程可表示为： $$ \\min_G \\max_D V(D, G) \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log(1 D(G(z)))]\\tag{1} $$ 当生成器 G 分布 $p_g p_{data}$，判别器 D 的最优解为 $D^*(x) \\frac{1}{2}$ 时，目标函数达到最小值 $\\log({\\frac{1}{2}}) + \\log(1 \\frac{1}{2}) \\log{4}$。 ## 网络理论 ### 算法步骤 *小批量随机梯度下降，$k$ 为超参数。* ``` for i 1, iterations do for k steps do m 个噪声样本的小批量样本 {z(1),., z(m)}，来自噪声先验 pg(z)。 m 个真实样本的小批量样本 {x(1),., x(m)}，来自真实数据集。 通过公式 2 提升判别器随机梯度来更新判别器。 end for m 个噪声样本的小批量样本 {z(1),., z(m)}，来自噪声先验 pg(z)。 通过公式 3 降低生成器随机梯度来更新生成器。 end for ``` $$ \\nabla_{\\theta_d} \\frac{1}{m} \\sum_{i 1}^m \\left[\\log D(x^{(i)}) + \\log(1 D(G(z^{(i)}))) \\right] \\tag{2} $$ $$ \\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i 1}^m \\log\\left(1 D(G(z^{(i)}))\\right) \\tag{3} $$ 训练过程的巧思： 1. **在 $k$ 步优化判别器 D 和 $1$ 步优化生成器 G 之间交替进行**：在 one step 的内部循环中优化判别器 D 在计算上是不可行的，并且容易在有限的数据集上导致过拟合。 2. **最大化 $\\log D(G(z))$ 代替最小化 $\\log(1−D(G(z)))$ 训练生成器 G**：在生成器 G 效果很差时，判别器 D可以以高置信度拒绝样本，这种情况下，$\\log(1−D(G(z)))$ 不起作用。 原文中提供下图： ![](../static/images/GAN/fig1.png) *注：黑色散点线为真实数据的分布；绿色实线为生成器 $G$ 生成数据的分布；蓝色虚线为判别器 $D$ 的分布，区分黑色散点与绿色实线。最下面的直线为均匀采样 $z$ 的域；其上面的直线是 $x$ 域的一部分。向上的箭头表示 $x G(z)$ 的映射关系。* 图 (a)：对抗接近收敛，$p_g$ 接近 $p_{data}$，判别器部分分类正确（能否分辨出真实数据和生成数据）。 图 (b)：在算法的内部循环中，判别器 D 向着分类数据训练，收敛在 $D^*(x) \\frac {p_{data}(x)}{p_{data}(x) + p_g(x)}$。 图 (c)：更新生成器 $G$ 后，判别器 $D$ 的梯度引导 $G(z)$ 偏向更有可能被归类为真实数据的区域。 图 (d)：经过若干步训练后，如果 $G$ 和 $D$ 有足够的容量，它们会收敛到 $p_g p_{data}$，此时 $D$ 无法区分出真实数据和生成数据，即 $D(x) \\frac {1}{2}$。 ### 解释全局最优解 $p_g p_{data}$ **命题 1.** 对于固定的生成器 $G$，最优判别器 $D$ 为： $$ D^*_G(x) \\frac {p_{data}(x)}{p_{data}(x) + p_g(x)} $$ **证明：** 给定任何生成器 $G$ 的判别器 $D$ 的训练标准是最大化 $V(G, D)$。 $$ V(G,D) \\int_x p_{data}(x) \\log(D(x))dx + \\int_z p_z(z) \\log(1 D(g(z)))dz\\tag{4} $$ 通过变量替换 $G(z) x \\sim p_g(x)$，式(4)改写为： $$ V(G,D) \\int_x \\left[p_{data}(x) \\log(D(x)) + p_g(x) \\log(1 D(x))\\right] dx $$ > *生成器 $G$ 将噪声输入 $z \\sim p_z(z)$ 映射为样本 $x G(z)$，隐式定义了生成样本的分布 $p_g(x)$。当 $z$ 服从噪声先验 $p_z(z)$ 分布时，$x G(z)$ 的分布即为 $p_g(x)$；若 $z \\sim p_z(z)$，则 $x G(z)\\sim p_g(x)$。* > *对于变量替换定理，对于任意函数 $h(x)$，若 $x$ 是随机变量 $z$ 的映射 $x G(z)$，则关于 $z$ 的期望可以转换为关于 $x$ 的期望：$\\mathbb{E}_{z \\sim p_z(z)}[h(G(z))] \\mathbb{E}_{x \\sim p_g(x)}[h(x)]$。* 对于每个样本 $x$，求 $D(x)$ 使得 $V(D,G)$ 最大化。这是一个单变量优化问题，最优解为： $$ D^*_G(x) \\frac {p_{data}(x)}{p_{data}(x) + p_{g}(x)} $$ 将最优判别器代入到目标函数有： $$ \\begin{aligned} C(G) & \\max_D V(G,D)\\\\ & \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[\\log D^*_G(x) \\right] + \\mathbb{E}_{z \\sim p_z} \\left[\\log(1 D^*_G(G(z))) \\right] \\\\ & \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[\\log D^*_G(x) \\right] + \\mathbb{E}_{x \\sim p_g} \\left[\\log(1 D^*_G(x)) \\right]\\\\ & \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[\\log \\frac {p_{data}(x)}{p_{data}(x) + p_{g}(x)} \\right] + \\mathbb{E}_{x \\sim p_g} \\left[\\log \\frac {p_{g}(x)}{p_{data}(x) + p_{g}(x)} \\right] \\end{aligned}\\tag{5} $$ **定理 1.** 当且仅当 $p_g p_{data}$ 时，$C(G)$ 达到的全局最小值 $ \\log 4$。 **证明：** 对于 $p_g p_{data}$，有 $D^*_G(x) \\frac{1}{2}$。因此： $$ C(G) \\mathbb{E}_{x\\sim p_{data}} \\frac {p_{data}(x)}{p_{data}(x) + p_{g}(x)} + \\mathbb{E}_{x\\sim p_{g}} \\frac {p_{g}(x)}{p_{data}(x) + p_{g}(x)} \\log \\frac {1}{2} + \\log \\frac {1}{2} \\log 4 $$ GAN 的优化目标是最小化生成模型分布 $p_g$ 和真实数据分布 $p_{data}$ 之间的差异，将目标函数中的对数概率表达为 KL 散度的形式。引入变形以关联 KL 散度： $$ \\begin{aligned} \\log \\frac {p_{data}(x)}{p_{data}(x) + p_{g}(x)} \\log \\frac {p_{data}(x)}{\\frac {p_{data}(x)+p_g(x)}{2}} \\log 2 \\\\ \\log \\frac {p_{g}(x)}{p_{data}(x) + p_{g}(x)} \\log \\frac {p_{g}(x)}{\\frac {p_{data}(x)+p_g(x)}{2}} \\log 2 \\end{aligned} $$ 所以， $$ \\begin{aligned} C(G) & \\log 4 + \\mathbb{E}_{x\\sim p_{data}} \\left[\\log \\frac {p_{data}(x)}{\\frac {p_{data}(x)+p_g(x)}{2}} \\right] + \\mathbb{E}_{x\\sim p_{g}} \\left[\\log \\frac {p_{g}(x)}{\\frac {p_{data}(x)+p_g(x)}{2}} \\right] \\\\ & \\log 4 + \\text{KL}\\left(p_{data}\\frac{p_{data}+p_g}{2}\\right) + \\text{KL}\\left(p_g\\frac{p_{data}+p_g}{2}\\right) \\\\ & \\log 4 + 2 \\cdot \\text{JSD}(p_{data} p_g) \\end{aligned} $$ >其中，KL 散度是衡量两个分布差异的常见方法，定义为：$\\text{KL}(pq) \\mathbb{E}_{x\\sim p} \\log \\frac {p(x)}{q(x)}$。Jensen Shannon 散度（JSD）是 KL 散度的一个对称版本，是衡量两个分布差异的对称性指标，定义为：$\\text{JSD}(pq) \\frac {1}{2} \\text{KL}(p \\frac {p + q}{2}) + \\frac {1}{2} \\text{KL}(q \\frac {p + q}{2})$。 由于 JSD 非负，当且仅当 $p q$ 时为零，因此： $$ C(G) \\geq \\log 4, 且等号成立当且仅当 p_g p_{data} $$ 证毕。 ### 解释算法收敛性 **命题 2.** 如果生成器 $G$ 和判别器 $D$ 都有足够的容量，并且算法的每一步都允许判别器在给定 $G$ 的情况下达到最优，并更新 $p_g$ 以改进标准 $\\mathbb{E}_{x\\sim p_{data}}[\\log D^*_G(x)] + \\mathbb{E}_{x\\sim p_{g}}[\\log (1 D^*_G(x))]$，则 $p_g$ 收敛到 $p_{data}$。 **证明：** 考虑 $V(G, D) U(p_g, D)$ 作为 $p_g$ 的函数。$U(p_g, D)$ 是凸函数，且全局最优解唯一。通过梯度下降更新 $p_g$，其参数更新方向始终朝向 JSD 减小的方向，从而保证收敛。 ## 代码实验 实验代码如下，详细代码位于[Github](https://github.com/Fingsinz/StyleTransfer/blob/main/src/01.ref_and_note/01.GAN.py)： <details> <summary>GAN MNIST 实验代码</summary> ```python ''' Created on 2025.02.01 @Author: Fingsinz (fingsinz@foxmail.com) @Reference: 1. https://arxiv.org/abs/1406.2661 ''' import time import os import torch import numpy as np import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader from torchvision import datasets, transforms import matplotlib.pyplot as plt # 配置参数 class Config(): data_folder: str './data' # 数据集路径, 此处用 MNIST 做测试 batch_size: int 128 # batch 大小 epochs: int 10 # 训练轮数 lr: float 0.0002 # 学习率 betas: tuple (0.5, 0.999) # Adam 的超参数 k_steps: int 5 # k 值 latent_dim: int 100 # 隐变量维度 device: str 'cuda' if torch.cuda.is_available() else 'cpu' # 生成器 class Generator(nn.Module): def __init__(self, latent_dim): super().__init__() self.model nn.Sequential( nn.Linear(latent_dim, 256), nn.LeakyReLU(0.2), nn.Linear(256, 512), nn.LeakyReLU(0.2), nn.Linear(512, 1024), nn.LeakyReLU(0.2), nn.Linear(1024, 28 * 28), nn.Tanh() ) def forward(self, x): return self.model(x).view( 1, 1, 28, 28) # 判别器 class Discriminator(nn.Module): def __init__(self): super().__init__() self.model nn.Sequential( nn.Flatten(), nn.Linear(28 * 28, 1024), nn.LeakyReLU(0.2, inplace True), nn.Linear(1024, 512), nn.LeakyReLU(0.2, inplace True), nn.Linear(512, 256), nn.LeakyReLU(0.2, inplace True), nn.Linear(256, 1), nn.Sigmoid() ) def forward(self, x): return self.model(x) # GAN 模型 class GAN(): def __init__(self, config): self.config config self.generator Generator(config.latent_dim).to(config.device) self.discriminator Discriminator().to(config.device) self.criterion nn.BCELoss() self.g_optimizer optim.Adam(self.generator.parameters(), lr config.lr, betas config.betas) self.d_optimizer optim.Adam(self.discriminator.parameters(), lr config.lr, betas config.betas) self.real_label 1 self.fake_label 0 def get_data(self): transform transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) train_dataset datasets.MNIST(root self.config.data_folder, train True, download True, transform transform) train_loader DataLoader(train_dataset, batch_size self.config.batch_size, shuffle True) return train_loader def train(self): train_loader self.get_data() epochs self.config.epochs g_loss 0 d_real_loss 0 d_fake_loss 0 for epoch in range(epochs): for i, (images, _) in enumerate(train_loader): batch_size images.size(0) images images.to(self.config.device) # 判别器训练 if (i + 1) % self.config.k_steps ! 0: self.d_optimizer.zero_grad() # 训练真实数据 labels torch.full((batch_size,), self.real_label, device self.config.device).float() output self.discriminator(images) loss_real self.criterion(output.view( 1), labels) loss_real.backward() # 训练假数据 z torch.randn(batch_size, self.config.latent_dim, device self.config.device) fake_images self.generator(z) labels.fill_(self.fake_label).float() output self.discriminator(fake_images.detach()) loss_fake self.criterion(output.view( 1), labels) loss_fake.backward() self.d_optimizer.step() d_real_loss loss_real.item() d_fake_loss loss_fake.item() # 判别器训练 # 生成器训练 else: self.g_optimizer.zero_grad() labels.fill_(self.real_label).float() output self.discriminator(fake_images) loss_g self.criterion(output.view( 1), labels) loss_g.backward() self.g_optimizer.step() g_loss loss_g.item() # 生成器训练 if i % 100 0: print(f\"[{time.strftime('%Y %m %d %H:%M:%S', time.localtime())}] \" + f\"Epoch [{epoch}/{epochs}], Step [{i}/{len(train_loader)}], \" f\"D Loss: {d_real_loss:.4f} + {d_fake_loss:.4f}, G Loss: {g_loss:.4f}\") self.save_generated_images(epoch + 1) def save_generated_images(self, epoch): \"\"\" 保存训练效果图片 参数: epoch (int): 当前轮数 \"\"\" z torch.randn(64, self.config.latent_dim, device self.config.device) fake_images self.generator(z) fake_images fake_images.cpu().detach().numpy() fake_images np.transpose(fake_images, (0, 2, 3, 1)) fig, axes plt.subplots(8, 8, figsize (8, 8)) for i in range(8): for j in range(8): axes[i, j].imshow(fake_images[i * 8 + j, :, :, 0], cmap 'gray') axes[i, j].axis('off') if not os.path.exists('gan_generated_images'): os.makedirs('gan_generated_images') plt.savefig(f'./gan_generated_images/epoch_{epoch}.png') plt.close() if __name__ '__main__': config Config() gan GAN(config) gan.train() ``` </details> GAN 对于 MNIST 数据集效果可如下所示： <table> <th>第一轮生成的图片</th> <th>第十轮生成的图片</th> <tr> <td><img src \"../static/images/GAN/epoch_1.png\"></td> <td><img src \"../static/images/GAN/epoch_10.png\"></td> </tr> </table> ## 论文优缺点及未来工作 优点（DeepSeek总结）： 生成样本质量高：GAN生成的图像、音频等数据具有极高的逼真度，尤其在图像生成任务中表现出色（如人脸、艺术作品生成）。生成器通过对抗训练不断优化，以欺骗判别器，最终生成的样本细节丰富、接近真实数据分布。 无需显式建模数据分布：GAN通过对抗过程直接学习数据分布，无需预先定义概率密度函数（如VAE需要假设潜在变量的分布），适用于复杂高维数据（如自然图像）。 生成多样性：在理想情况下，GAN能够覆盖真实数据的所有模式，生成多样化样本。相比之下，某些模型（如朴素自回归模型）可能因逐像素生成导致模式单一化。 无监督学习能力：GAN仅需未标注数据即可训练，适合缺乏标签的场景（如艺术创作、数据增强）。 灵活的应用扩展：GAN框架可轻松扩展为条件生成（cGAN）、图像翻译（CycleGAN）、超分辨率（SRGAN）等任务，适应多种生成需求。 缺点（DeepSeek总结）： 训练不稳定 模式坍缩（Mode Collapse）：生成器可能仅生成少数几种样本，忽略数据多样性。 梯度问题：若判别器过强，生成器梯度消失；若生成器过强，判别器无法提供有效反馈。 评估困难 缺乏显式似然函数，难以直接计算生成样本的概率。 常用指标（如Inception Score、FID）依赖预训练模型，可能无法全面反映生成质量。 超参数敏感：学习率、网络结构、正则化方法等对训练结果影响显著，需反复调参。 理论分析复杂 收敛性难以保证，实际训练可能陷入局部最优。 均衡状态（纳什均衡）在有限模型容量下难以达到。 生成不可控性 生成过程缺乏显式约束，可能产生不合理样本（如人脸扭曲）。 对离散数据（如文本）生成效果较差，因梯度无法通过离散变量传递。 计算资源消耗大：训练高质量GAN需要大量数据和计算资源（如GPU），耗时较长。 未来工作： 更好地理解这种框架，例如在高维空间中的表现； 探索其他算法和数据结构，例如使用卷积神经网络作为掩码器； 探索其他训练方法，例如使用梯度反向传递训练模型； 探索其他损失函数，例如使用KL损失函数； 探索其他训练方法，例如使用GAN训练模型。"},"/StyleTransfer/ref_and_notes/patchgan.html":{"title":"PatchGAN 到 Multi-Scale PatchGAN","content":" title: PatchGAN 到 Multi Scale PatchGAN keywords: PatchGAN desc: PatchGAN 到 Multi Scale PatchGAN date: 2025 03 11 id: PatchGAN [Image to Image Translation with Conditional Adversarial Networks](https://ieeexplore.ieee.org/document/8100115) *Isola P , Zhu J Y , Zhou T ,et al.Image to Image Translation with Conditional Adversarial Networks[C]//IEEE Conference on Computer Vision & Pattern Recognition.IEEE, 2016.DOI:10.1109/CVPR.2017.632.* [High Resolution Image Synthesis and Semantic Manipulation with Conditional GANs](https://arxiv.org/abs/1711.11585) *Wang T C , Liu M Y , Zhu J Y ,et al.High Resolution Image Synthesis and Semantic Manipulation with Conditional GANs[J]. 2017.DOI:10.48550/arXiv.1711.11585.* ## 基础 PatchGAN *来自《Image to Image Translation with Conditional Adversarial Networks》（Phillip Isola 等，CVPR 2017，即 pix2pix）* ### PatchGAN 原理 PatchGAN 的判别器不再对整张图像进行全局真伪判断，而是**将输入图像分割为多个局部小块（Patch），每个 Patch 独立判断真伪，最终取所有 Patch 的平均结果作为整体判别输出**。 判别器通过多层卷积操作，逐步下采样输入图像，最终输出一个 **N×N 的特征图**。 每个特征图上的点对应输入图像的一个局部块，用于判断该局部区域是否真实。 ### 卷积参数 卷积参数： 卷积核大小：4 × 4，步长（stride） 2，填充（padding） 1。 归一化层：使用 `BatchNorm2d` 或 `InstanceNorm2d`（根据输入条件）。 激活函数：LeakyReLU（负斜率为 0.2）。 输出层：最后一层卷积输出 1 通道的特征图，通过 Sigmoid 函数得到每个局部块的真假概率。 ### PatchGAN 感受野 每层卷积的感受野大小可通过公式递推计算： $$ R_{n} R_{n 1} + (k_n 1) \\times \\prod_{i 1}^{n 1} s_i $$ 其中 $k_n$ 为第 $n$ 层卷积核大小，$s_i$ 为第 $i$ 层步长。 以一个简单的三层卷积网络为例，来展示如何计算每个输出单元对应的输入图像的局部感受野大小。假设网络参数如下： > 输入的单个像素的感受野 $R_0 1$。 > > 第一层卷积：卷积核大小 $k_1 4$，步长 $s_1 2$。 > $R_1 R_0 + (k_1 1) \\times \\prod_{i 1}^{0} s_i 1+(4 1)\\times 1 1 + 3 4$。 > 这表示第一层中，每个神经元对应输入图像上的 $4 \\times 4$ 区域。 > > 第二层卷积：卷积核大小 $k_2 4 $，步长 $s_2 2$，前一层步长的累计乘积为 $s_1 2$。 > $R_2 R_1 + (k_2 1) \\times s_1 4 + 3 \\times 2 4 + 6 10$ > 说明第二层的每个神经元对应输入图像上的 $10 \\times 10$ 区域。 > > 第三层卷积：卷积核大小 $k_3 4$，步长 $s_3 1$，前两层步长的累计乘积为 $s_1 \\times s_2 2 \\times 2 4$。 > $R_3 R_2 + (k_3 1) \\times (s_1 \\times s_2) 10 + 3 \\times 4 10 + 12 22$ > 说明第三层中，每个神经元的感受野大小为 $22 \\times 22$ 像素。 这个例子说明，通过逐层计算，最终网络的最后一层中的每个判别器输出单元实际上“看到了”输入图像上的一个 $22 \\times 22$ 的局部区域。PatchGAN 的设计思想就是利用这样的局部感受野，使得判别器能够专注于图像中细小局部区域的真实感，从而更有效地区分真实图像和生成图像。 ### PatchGAN 感受野计算原理 局部连接与覆盖范围： 每个卷积核具有固定大小 $k$，它在输入图像上滑动时，每次都会覆盖 $k$ 个像素（在一维情况中，二维类似）。 第一个卷积层中，每个神经元直接看到大小为 $k$ 的区域。如果我们把输入的单个像素的感受野记为 $R_0 1$，那么第一层每个神经元的感受野就是：$R_1 1 + (k_1 1)$。这里的 $(k_1 1)$ 代表了额外扩展的部分（因为中心像素已经在 $R_0$ 内）。 步长的影响： 当卷积层使用步长 $s$ 时，输出特征图的每个神经元在输入上移动的间隔不再是 1，而是 $s$。这意味着，后续层的卷积操作相当于在上一层感受野的基础上，以 $s$ 的“倍率”去扩展。 例如，第二层卷积时，假设核大小为 $k_2$ 和步长 $s_2$，它增加的感受野大小不是直接的 $k_2 1$，而是乘上了上一层的步长 $s_1$（因为上一层的每个步长移动在原始输入上代表了 $s_1$ 个像素）：$R_2 R_1 + (k_2 1) \\times s_1$ 层层叠加： 当网络层数增多时，每一层的卷积操作都在前一层的基础上进一步扩展感受野，而且这种扩展会受前面所有层步长的累计影响。 所以对于第 $l$ 层，用前面所有层的步长乘积来缩放当前层的扩展量：$R_l R_{l 1} + (k_l 1) \\times \\prod_{i 1}^{l 1} s_i$，其中： $R_{l 1}$ 是上一层的感受野； $(k_l 1)$ 是当前层除中心外能“看到”的额外像素数量； $\\prod_{i 1}^{l 1} s_i$ 则是前面所有层步长的累乘，表明每一层的位移在原始输入上对应的像素数。 直观地说，每一层卷积操作会在原有感受野的基础上“增加”一圈额外的像素，这一圈的宽度为 $k 1$（不考虑步长时），而实际增加的像素数量会因为前面层的步长而被放大。于是就得到了上述递归公式，用来精确计算任意层的感受野。 ### 论文中的 PatchGAN [@NLayerDiscriminator](https://github.com/junyanz/pytorch CycleGAN and pix2pix/blob/master/models/networks.py#L539) ```python class NLayerDiscriminator(nn.Module): \"\"\"Defines a PatchGAN discriminator\"\"\" def __init__(self, input_nc, ndf 64, n_layers 3, norm_layer nn.BatchNorm2d): \"\"\" Parameters: input_nc (int) 输入图像的通道数 ndf (int) 最后一个转换层中卷积核的数量 n_layers (int) 判别器中的转换层数 norm_layer 归一化层 \"\"\" super(NLayerDiscriminator, self).__init__() if type(norm_layer) functools.partial: # 不需要使用偏差，因为BatchNorm2d有仿射参数 use_bias norm_layer.func nn.InstanceNorm2d else: use_bias norm_layer nn.InstanceNorm2d kw 4 padw 1 sequence [nn.Conv2d(input_nc, ndf, kernel_size kw, stride 2, padding padw), nn.LeakyReLU(0.2, True)] nf_mult 1 nf_mult_prev 1 for n in range(1, n_layers): # 逐渐增加过滤器的数量 nf_mult_prev nf_mult nf_mult min(2 ** n, 8) sequence + [ nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size kw, stride 2, padding padw, bias use_bias), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True) ] nf_mult_prev nf_mult nf_mult min(2 ** n_layers, 8) sequence + [ nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size kw, stride 1, padding padw, bias use_bias), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True) ] sequence + [nn.Conv2d(ndf * nf_mult, 1, kernel_size kw, stride 1, padding padw)] # 输出 1 通道预测图 self.model nn.Sequential(*sequence) def forward(self, input): \"\"\"Standard forward.\"\"\" return self.model(input) ``` 这个 PatchGAN 判别器总共包含 5 个卷积层，其参数分别为： *padding 1* 层级 卷积核大小 步长 通道 效果 : :: :: :: :: : 第一层 4 2 input_nc → ndf 下采样 第二层（第一次迭代） 4 2 ndf → 2 * ndf 下采样 第三层（第二次迭代） 4 2 2 * ndf → 4 * ndf 下采样 第四层（循环结束后） 4 1 4 * ndf → 8 * ndf 尺寸不变 第五层 4 1 8 * ndf → 1 每个位置对应一个局部区域的真实性概率 假设输入图像为 256×256，那么按照代码中的卷积参数（核尺寸 4，padding 1）以及各层的步幅，特征图在每一层的尺寸变化如下： > 1. 第一层卷积（Conv2d(input_nc, 64, kernel_size 4, stride 2, padding 1)），输入尺寸 256×256，输出特征图大小为 128×128，通道数为 64。 > > $$ > \\text{output size} \\left\\lfloor\\frac{256+2\\times1 4}{2}\\right\\rfloor + 1 128 > $$ > > 2. 第二层卷积（第一个 for 循环迭代 n 1，对应 Conv2d(64, 128, kernel_size 4, stride 2, padding 1)），输入尺寸 128×128，输出大小 64×64，通道数由 64 增加到 128。 > > $$ > \\text{output size} \\left\\lfloor\\frac{128+2 4}{2}\\right\\rfloor + 1 64 > $$ > > 3. 第三层卷积（for 循环中 n 2，对应 Conv2d(128, 256, kernel_size 4, stride 2, padding 1)），输入尺寸 64×64，输出大小 32×32，通道数由 128 增加到 256。 > > $$ > \\text{output size} \\left\\lfloor\\frac{64+2 4}{2}\\right\\rfloor + 1 32 > $$ > > 4. 第四层卷积（循环外的第一层卷积，Conv2d(256, 512, kernel_size 4, stride 1, padding 1)），此层不再下采样，输入尺寸 32×32，输出大小为 31×31，通道数从 256 变为 512。 > > $$ > \\text{output size} \\left\\lfloor\\frac{32+2 4}{1}\\right\\rfloor + 1 31 > $$ > > 5. 第五层卷积（最后一层，Conv2d(512, 1, kernel_size 4, stride 1, padding 1)），输入尺寸 31×31，最终输出特征图大小为 30×30，且通道数为 1，对应每个位置的 Patch 判别结果。 > > $$ > \\text{output size} \\left\\lfloor\\frac{31+2 4}{1}\\right\\rfloor + 1 30 > $$ 示意图如下（输入 `[256, 256, 3]`）： ![](../static/images/PatchGAN/fig1.png) 感受野逐层计算如下： $$ R_{n} R_{n 1} + (k_n 1) \\times \\prod_{i 1}^{n 1} s_i $$ 层级 参数 跳跃（前面各层步长的乘积） 感受野 : :: :: :: : 1 $k_1 4$，$s_1 2$ $j_0 1$ $R_1 1 + (4 1) \\times 1 1 + 3 4$ 2 $k_2 4$，$s_2 2$ $j_1 j_0 \\times s_1$ $ 1 \\times 2 2$ $R_2 R_1 + (4 1) \\times j_1$ $ 4 + 3 \\times 2 4 + 6 10$ 3 $k_3 4$，$s_3 2$ $j_2 j_1 \\times s_2$ $ 2 \\times 2 4$ $R_3 R_2 + (4 1) \\times j_2$ $ 10 + 3 \\times 4 10 + 12 22$ 4 $k_4 4$，$s_4 1$ $j_3 j_2 \\times s_3$ $ 4 \\times 2 8$ $R_4 R_3 + (4 1) \\times j_3$ $ 22 + 3 \\times 8 22 + 24 46$ 5 $k_5 4$，$s_5 1$ $j_4 j_3 \\times s_4$ $ 8 \\times 1 8$ $R_5 R_4 + (4 1) \\times j_4$ $ 46 + 3 \\times 8 46 + 24 70$ 因此，最终判别器输出的每个单元的感受野为 70×70 像素。 ## 3 级金字塔判别器（Multi Scale PatchGAN） *来自《High Resolution Image Synthesis and Semantic Manipulation with Conditional GANs》（Ting Chun Wang 等，CVPR 2018，即 pix2pixHD）* 解决了两个主要问题： 1. 使用 GAN 生成高分辨率图像的困难； 2. 先前高分辨率结果中缺乏细节和逼真的纹理。 ### Coarse to Fine Generator *《High Resolution Image Synthesis and Semantic Manipulation with cGANs》生成器部分* 将生成器分解为两个子网：$G_1$ 和 $G_2$。生成器 $G \\{G_1, G_2\\}$。 $G_1$ 为全局生成器网络； $G_2$ 为局部增强器网络。 全局生成器网络以 1024 × 512 的分辨率运行，局部增强器网络输出的图像分辨率是前一个图像输出大小的 4 倍（每个图像维度，长、宽为 2 倍）。 例如，生成器 $G \\{G_1, G_2\\}$ 的输出图像分辨率为 2048 × 1024，$G \\{G_1, G_2, G_3\\}$ 的输出图像分辨率为 4096 × 2048。 ![](../static/images/PatchGAN/fig2.png) 全局生成器由 **一个前端卷积 $G_1^{(F)}$**、**一组残差块 $G_1^{(R)}$** 和 **一个后端转置卷积 $G_1^{(B)}$** 组成。 分辨率为 1024 × 512 的语义标签图依次通过上面 3 个组件，以输出分辨率为 1024 × 512 的图像。 局部增强器由 **一个前端卷积 $G_2^{(F)}$**、**一组残差块 $G_2^{(R)}$** 和 **一个后端转置卷积 $G_2^{(B)}$** 组成。 $G_2$ 的输入标签映射的分辨率为 2048 × 1024。 残差块 $G_2^{(R)}$ 的输入是 $G_2^{(F)}$ 的输出特征图和全局生成器网络 $G_1^{(B)}$ 后端的最后一个特征图的总和。这有助于将全局信息从 $G_1$ 集成到 $G_2$ 中。 在训练过程中，首先训练全局生成器，接着按照它们的分辨率顺序训练局部增强器，然后一起微调所有的网络。 #### 全局生成器代码 [pix2pixHD/models/networks.py](https://github.com/NVIDIA/pix2pixHD/blob/master/models/networks.py#L183) ```python class GlobalGenerator(nn.Module): def __init__(self, input_nc, output_nc, ngf 64, n_downsampling 3, n_blocks 9, norm_layer nn.BatchNorm2d, padding_type 'reflect'): ``` > `input_nc`：输入通道数（如语义标签图的通道数）。 > `output_nc`：输出通道数（如生成图像的 RGB 通道数 3）。 > `ngf`：生成器的基础特征通道数（默认 64）。 > `n_downsampling`：下采样次数（默认 3 次，特征图尺寸缩小为输入的 1/8）。 > `n_blocks`：残差块数量（默认 9 个）。 > `norm_layer`：归一化层（默认批量归一化 BatchNorm2d）。 > `padding_type`：填充类型（默认反射填充 ReflectionPad2d，减少边缘伪影）。 ```python activation nn.ReLU(True) model [ nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size 7, padding 0), norm_layer(ngf), activation ] ``` > 反射填充和卷积层提取初始特征：`(input_nc, H, W) > (ngf, H, W)`。 ```python for i in range(n_downsampling): mult 2**i model + [ nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size 3, stride 2, padding 1), norm_layer(ngf * mult * 2), activation ] ``` > 下采样：每次通道数翻倍，特征图尺寸缩小为 1/2。 ```python mult 2**n_downsampling for i in range(n_blocks): model + [ ResnetBlock(ngf * mult, padding_type padding_type, activation activation, norm_layer norm_layer) ] ``` > 残差块，保持特征图尺寸不变，通过跳跃连接缓解梯度消失。 > 通道数固定为 `ngf * 2^n_downsampling`。 ```python for i in range(n_downsampling): mult 2**(n_downsampling i) model + [ nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size 3, stride 2, padding 1, output_padding 1), norm_layer(int(ngf * mult / 2)), activation ] ``` > 上采样：逐步恢复空间分辨率并减少通道数。 > 每次通道数减半，特征图尺寸增加为 2 倍 ```python model + [ nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size 7, padding 0), nn.Tanh() ] self.model nn.Sequential(*model) ``` > 反射填充和卷积层将通道数映射到 output_nc，并使用 Tanh 激活 #### 局部增强器 [pix2pixHD/models/networks.py](https://github.com/NVIDIA/pix2pixHD/blob/master/models/networks.py#L129) ```python class LocalEnhancer(nn.Module): def __init__(self, input_nc, output_nc, ngf 32, n_downsample_global 3, n_blocks_global 9, n_local_enhancers 1, n_blocks_local 3, norm_layer nn.BatchNorm2d, padding_type 'reflect'): super(LocalEnhancer, self).__init__() self.n_local_enhancers n_local_enhancers ``` > `input_nc`：输入通道数（如语义标签图的通道数）。 > `output_nc`：输出通道数（如RGB图像的3通道）。 > `ngf`：基础特征通道数（默认 32）。 > `n_downsample_global`：全局生成器的下采样次数（默认 3 次）。 > `n_blocks_global`：全局生成器的残差块数量（默认 9 个）。 > `n_local_enhancers`：局部增强器的数量（默认 1 个）。 > `n_blocks_local`：每个局部增强器的残差块数量（默认 3 个）。 > `norm_layer`：归一化层（默认批量归一化 BatchNorm2d）。 > `padding_type`：填充类型（默认反射填充 ReflectionPad2d）。 ```python # 全局生成器模型 ngf_global ngf * (2**n_local_enhancers) model_global GlobalGenerator(input_nc, output_nc, ngf_global, n_downsample_global, n_blocks_global, norm_layer).model # 去掉最后的卷积层 model_global [model_global[i] for i in range(len(model_global) 3)] self.model nn.Sequential(*model_global) ``` > 生成低分辨率基础图象（如 1024 * 512）。 > 截断全局生成器的最后 3 层（如最终卷积层），保留编码器和残差块部分。 > 特征通道：`ngf_global ngf * 2^n_local_enhancers`（如 `n_local_enhancers 1` 时，`ngf_global 64`）。 ```python # 局部增强层 for n in range(1, n_local_enhancers+1): # 下采样分支 ngf_global ngf * (2**(n_local_enhancers n)) model_downsample [ # 1 nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf_global, kernel_size 7, padding 0), norm_layer(ngf_global), nn.ReLU(True), # 2 nn.Conv2d(ngf_global, ngf_global * 2, kernel_size 3, stride 2, padding 1), norm_layer(ngf_global * 2), nn.ReLU(True) ] ``` > 1. 反射填充 → 卷积（7×7，步长 1） → 归一化 → ReLU。 > 2. 卷积（3×3，步长 2） → 归一化 → ReLU（通道数翻倍）。 ```python # 残差块 model_upsample [] for i in range(n_blocks_local): model_upsample + [ ResnetBlock(ngf_global * 2, padding_type padding_type, norm_layer norm_layer) ] ``` > `n_blocks_local` 个残差块（默认3个），保持特征图尺寸不变。 ```python # 上采样分支 model_upsample + [ nn.ConvTranspose2d(ngf_global * 2, ngf_global, kernel_size 3, stride 2, padding 1, output_padding 1), norm_layer(ngf_global), nn.ReLU(True) ] ``` > 转置卷积（3×3，步长2） → 归一化 → ReLU（通道数减半）。 ```python # 最终输出层 if n n_local_enhancers: model_upsample + [ nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size 7, padding 0), nn.Tanh() ] ``` > 反射填充 → 卷积（7×7） → Tanh 激活函数。 ```python setattr(self, 'model'+str(n)+'_1', nn.Sequential(*model_downsample)) setattr(self, 'model'+str(n)+'_2', nn.Sequential(*model_upsample)) self.downsample nn.AvgPool2d(3, stride 2, padding [1, 1], count_include_pad False) ``` > 使用平均池化（AvgPool2d）逐步下采样输入，生成多尺度输入金字塔。 > 例如，若 `n_local_enhancers 2`，则输入金字塔包含原始输入、1/2 分辨率、1/4 分辨率。 ```python def forward(self, input): # 构建输入金字塔 input_downsampled [input] for i in range(self.n_local_enhancers): input_downsampled.append(self.downsample(input_downsampled[ 1])) ``` > 原始输入逐步下采样，生成 `n_local_enhancers+1` 个不同尺度的输入。 ```python # output at coarest level output_prev self.model(input_downsampled[ 1]) ``` > 将输入金字塔中最低分辨率的图像输入全局生成器，得到基础特征。 ```python for n_local_enhancers in range(1, self.n_local_enhancers+1): model_downsample getattr(self, 'model'+str(n_local_enhancers)+'_1') model_upsample getattr(self, 'model'+str(n_local_enhancers)+'_2') input_i input_downsampled[self.n_local_enhancers n_local_enhancers] output_prev model_upsample(model_downsample(input_i) + output_prev) return output_prev ``` > 从最粗糙尺度开始，依次通过下采样分支提取局部特征。 > 将局部特征与上一层的输出相加（特征融合），再通过残差块和上采样。 > 最后一层输出最终高分辨率图像（如 2048×1024）。 ### Multi Scale PatchGAN *《High Resolution Image Synthesis and Semantic Manipulation with cGANs》判别器部分* 为了区分高分辨率的真实图像和合成图像，鉴别器需要有一个大的接受野。 通过图像金字塔构建多尺度判别器，包含 **3 个不同分辨率的判别器**，它们具有相同的网络结构： 1. 原始分辨率图像（如 1024×1024）。 2. 下采样 1/2 的图像（如 512×512）。 3. 下采样 1/4 的图像（如 256×256）。 每个判别器均为 PatchGAN 结构，但感受野随分辨率变化，分别捕捉 **局部细节**（高分辨率）、**中等结构**（中分辨率）和 **全局布局**（低分辨率）。 最粗糙的尺度上工作的鉴别器具有最大的接受域。它具有更全局的图像视图，可以引导生成器生成全局一致的图像。 最精细的尺度上的鉴别器鼓励生成器产生更精细的细节。 这也使得训练从粗到精的生成器更容易，因为将低分辨率模型扩展到更高分辨率只需要在最精细的级别添加鉴别器，而不是从头开始重新训练。在没有多尺度鉴别器的情况下，我们观察到生成的图像中经常出现许多重复的图案。 因此，原来的最大最小博弈变成： $$ \\min_G \\max_{D_1,D_2,D_3}\\sum_{k 1,2,3}\\mathcal{L}_{GAN}(G, D_k) $$ #### 判别器代码 ```python class MultiscaleDiscriminator(nn.Module): def __init__(self, input_nc, ndf 64, n_layers 3, norm_layer nn.BatchNorm2d, use_sigmoid False, num_D 3, getIntermFeat False): super(MultiscaleDiscriminator, self).__init__() self.num_D num_D self.n_layers n_layers self.getIntermFeat getIntermFeat ``` > `input_nc`：输入图像的通道数（如 RGB 图像为 3）。 > `ndf`：基础特征通道数（默认 64）。 > `n_layers`：每个判别器的卷积层数（默认 3）。 > `norm_layer`：归一化层（默认批量归一化 BatchNorm2d）。 > `use_sigmoid`：是否在输出层使用 Sigmoid 激活函数（默认否）。 > `num_D`：判别器的数量（默认 3 个，对应 3 个不同尺度）。 > `getIntermFeat`：是否返回中间层特征（默认否，仅返回最终输出）。 ```python for i in range(num_D): netD NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat) if getIntermFeat: for j in range(n_layers+2): setattr(self, 'scale'+str(i)+'_layer'+str(j), getattr(netD, 'model'+str(j))) else: setattr(self, 'layer'+str(i), netD.model) self.downsample nn.AvgPool2d(3, stride 2, padding [1, 1], count_include_pad False) ``` > 1. 构建多尺度判别器：循环创建 `num_D` 个判别器（每个对应一个尺度） > 每个判别器是 `NLayerDiscriminator`（PatchGAN 的判别器） 的实例，其核心结构为多层卷积。 > 若 `getIntermFeat True`：将每个判别器的每一层单独注册为属性（如 `scale0_layer0`，`scale0_layer1`），以便提取中间特征。 > 若 `getIntermFeat False`：直接将整个判别器模型注册为属性（如 `layer0`，`layer1`）。 > 2. 下采样：使用平均池化（AvgPool2d）对输入图像进行下采样，生成多尺度输入金字塔。池化核大小 3×3，步长 2，padding 1，逐步将图像分辨率减半。 ```python def singleD_forward(self, model, input): if self.getIntermFeat: result [input] for i in range(len(model)): result.append(model[i](result[ 1])) return result[1:] else: return [model(input)] def forward(self, input): num_D self.num_D result [] input_downsampled input for i in range(num_D): if self.getIntermFeat: model [getattr(self,'scale'+str(num_D 1 i)+'_layer'+str(j)) for j in range(self.n_layers+2)] else: model getattr(self, 'layer'+str(num_D 1 i)) result.append(self.singleD_forward(model, input_downsampled)) if i ! (num_D 1): input_downsampled self.downsample(input_downsampled) return result ``` > 原始输入图像通过逐层下采样生成 `num_D` 个不同尺度的图像。例如，若 `num_D 3` ，则生成原始分辨率、1/2 分辨率、1/4 分辨率三个尺度。 > 最粗糙尺度（最低分辨率）开始，依次用对应尺度的判别器处理输入。 > 若 `getIntermFeat True`：提取每个判别器各层的中间特征。 > 若 `getIntermFeat False`：仅返回每个判别器的最终输出。 > 所有判别器的输出（或中间特征）被收集到 `result` 列表中并返回。 ### 改进后的对抗损失 判别器目标：最大化此损失以区分真实图像 $x$ 与生成图像 $G(s)$。 生成器目标：最小化此损失以生成能欺骗判别器的图像。 从多层鉴别器中提取特征，并从真实图像和合成图像中学习匹配这些中间表示。提出特征匹配损失 $\\mathcal{L}_{FM}(G, D_k)$ 计算为： $$ \\mathcal{L}_{FM}(G, D_k) \\mathbb{E}_{\\text{s}, \\text{x}} \\sum_{i 1}^T \\frac {1}{N_i}[\\vert\\vert D_k^{(i)}(\\text{s}, \\text{x}) D_k^{(i)}(\\text{s}, G(\\text{s}))\\vert\\vert _1] $$ $D_k^{(i)}$：鉴别器 $D_k$ 的第 $i$ 层特征提取器。 $\\text{s}$：输入的语义标签图，每个像素值表示对应物体的类别。 $\\text{x}$：输入的真实图像，与语义标签图对应，用于监督。 $T$：总层数。 $N_i$：每层中元素的数量。 L1 范数：计算真实与生成图像在每层特征上的平均差异。 将 GAN 损失和特征匹配损失结合起来，最终的对抗损失为： $$ \\min_G \\left(\\left( \\max_{\\text{D}_1,\\text{D}_2,\\text{D}_3} \\sum_{k 1,2,3}\\mathcal{L}_{GAN}(G, \\text{D}_k) \\right) + \\lambda \\sum_{k 1,2,3}\\mathcal{L}_{FM}(G, \\text{D}_k) \\right) $$ 其中，$\\lambda$ 控制两项的重要性。 注意，对于特征匹配损失 $\\mathcal{L}_{FM}$，$D_k$ 仅作为特征提取器，不会最大化损失 $\\mathcal{L}_{FM}$。 ### 实例级特征 #### 实例边界图 传统的语义标签仅标注像素类别（如“车辆”“行人”），但无法区分同类对象的不同实例（如相邻的两辆车）。所以导致生成图像中同类相邻对象的边界模糊，例如并排的车辆可能被错误地合并成一块区域。 考虑实例边界图（Instance Boundary Map）：解决语义标签无法区分同类相邻对象的问题（如并排的车辆），显著提升对象边界的清晰度。 每个对象实例具有唯一 ID。 若某像素的实例 ID 与其 4 邻域像素不同，则标记为边界（值为1），否则为 0。 实例边界图的输入设计： 生成器输入：将语义标签图（one hot编码）与实例边界图拼接（通道维度），作为生成器的输入。 判别器输入：将语义标签图、实例边界图和真实/生成图像拼接，供判别器区分真假。 #### 实例级特征嵌入 传统方法（如 pix2pix）从语义标签到图像的映射是确定性的（一对一），无法生成多样化的结果。 为了生成不同的图像并允许实例级控制，添加额外的低维特征通道作为生成器网络的输入。论文表明，通过操纵这些特征，可以对图像合成过程进行灵活的控制。此外，由于特征通道是连续的量，提出的模型原则上能够生成无限多的图像。 为了生成低维特征，训练一个编码器网络来为图像中的每个实例找到一个对应于地面真实目标的低维特征向量。编码器网络设计： 架构层面上：编码器网络（E）采用标准的编码器 解码器结构，输入为真实图像（x），输出为每个实例的低维特征向量。 特征一致性：通过实例级平均池化（instance wise average pooling），将同一实例内所有像素的特征取平均，确保实例内部特征一致。 ![](../static/images/PatchGAN/fig3.png) 使用 $G(\\text{s}, E(\\text{x}))$ 替换 $G(\\text{s})$： $$ \\min_{G,E} \\left(\\left( \\max_{\\text{D}_1,\\text{D}_2,\\text{D}_3} \\sum_{k 1,2,3}\\mathcal{L}_{GAN}(G(\\text{s}, E(\\text{x})), \\text{D}_k) \\right) + \\lambda \\sum_{k 1,2,3}\\mathcal{L}_{FM}(G(\\text{s}, E(\\text{x})), \\text{D}_k) \\right) $$ 训练时采用联合训练，编码器与生成器和鉴别器共同训练。 编码器训练完成后，在训练图像中的所有实例上运行编码器，并记录得到的特征。然后对每个语义类别的这些特征执行 K means 聚类。因此，每个簇对特定样式的特征进行编码，例如，道路的沥青或鹅卵石纹理。在推理时，随机选取一个聚类中心作为编码特征。 生成器的输入扩展为语义标签图 + 实例边界图 + 编码器提取的特征，即 $G(\\text{s},E(\\text{x}))$。 #### 编码器代码 [pix2pixHD/models/pix2pixHD_model.py](https://github.com/NVIDIA/pix2pixHD/blob/master/models/pix2pixHD_model.py#L111) ```python def encode_input(self, label_map, inst_map None, real_image None, feat_map None, infer False): if self.opt.label_nc 0: input_label label_map.data.cuda() else: # 为标签图创建 one hot 向量 size label_map.size() oneHot_size (size[0], self.opt.label_nc, size[2], size[3]) input_label torch.cuda.FloatTensor( torch.Size(oneHot_size) ).zero_() input_label input_label.scatter_(1, label_map.data.long().cuda(), 1.0) if self.opt.data_type 16: input_label input_label.half() # 通过 get_edges 方法检测实例 ID 的边界，生成二值边缘图（edge_map） if not self.opt.no_instance: inst_map inst_map.data.cuda() edge_map self.get_edges(inst_map) input_label torch.cat((input_label, edge_map), dim 1) input_label Variable(input_label, volatile infer) # real images for training / 真实图像用于训练 if real_image is not None: real_image Variable(real_image.data.cuda()) # instance map for feature encoding / 实例图用于特征编码 if self.use_features: # 获取预先计算的特征图 if self.opt.load_features: feat_map Variable(feat_map.data.cuda()) if self.opt.label_feat: inst_map label_map.cuda() return input_label, inst_map, real_image, feat_map ``` ### 论文实验结果 实验数据集： Cityscapes：城市场景数据集（2048×1024分辨率），包含2975张训练图像和500张测试图像。 NYU Indoor：室内场景数据集（561×427分辨率），1200张训练图像，249张测试图像。 ADE20K：通用场景数据集（图像宽度统一为512），20210张训练图像，2000张测试图像。 Helen Face：人脸数据集（1024×1024分辨率），2000张训练图像，330张测试图像。 实验参数： LSGANs：最小二乘 GAN。 $\\lambda 10$。 K Means 中 $K 10$。 每个实例特征用 3 维向量编码。 尝试在目标中添加感知损失： $$ \\lambda \\sum_{i 1}^N \\frac{1}{M_i} \\left[ \\vert\\vert F^{(i)}(\\text{x}) F^{(i)}(G(\\text{s})) \\vert\\vert _1 \\right] $$ $\\lambda 10$。 $F^{(i)}$：VGG 中有 $M_i$ 个元素的第 $i$ 层。 感知损失可以作为变量进行对照试验。 Ours(w/o VGG loss): 不使用 VGG 感知损失。 Ours(w/ VGG loss): 使用了 VGG 感知损失来优化生成图像的感知质量。 #### 定量评估 为了量化结果的质量，对合成图像进行语义分割，并比较预测片段与输入的匹配程度。 如果可以生成与输入标签图相对应的真实图像，现成的语义分割模型（例如，使用的 PSPNet）应该能够预测真实标签。 评估指标： 像素准确率（Pixel Accuracy）：生成图像与真实图像在像素级别的一致性。 平均交并比（Mean IoU）：衡量生成图像中对象区域与真实标注的重合度。 人类偏好率（Human Preference Rate）：通过Amazon Mechanical Turk（AMT）平台进行 A/B 测试，统计用户对生成图像的偏好比例。 结果对比 Cityscapes数据集： 像素准确率：本文方法达到 83.78%，显著优于 pix2pix（78.34%）和 CRN（70.55%）。 平均 IoU：本文方法为 0.6389，远高于 pix2pix（0.3948）和 CRN（0.3483），接近真实图像（Oracle：0.6857）。 结论：本文方法在保持高分辨率的同时，显著提升了语义一致性。 NYU Indoor数据集： 在AMT测试中，本文方法以 86.7% 的偏好率优于 pix2pix，以 63.7% 优于 CRN（图8）。 生成图像色彩更丰富，细节更真实（如家具纹理、灯光反射）。 #### 人类主观研究 任务类型分为： 1. 无时间限制（Unlimited Time）：用户同时观看两幅生成图像（不同方法），选择更自然的结果。 2. 有限时间（Limited Time）：图像展示时间随机（1/8秒至8秒），测试快速感知差异。 实验结果如下： 无时间限制测试： 对比 pix2pix：93.8% 用户偏好本文结果；对比 CRN：86.2% 用户偏好本文结果。 即使不使用 VGG 感知损失（仅对抗训练），本文方法仍以 94.6% 和 85.2% 的偏好率领先。 有限时间测试： 随着展示时间延长，用户更易区分本文方法与 CRN 的差异（如细节纹理、边界清晰度）。 在极短时间（1/8秒）内，本文方法仍以显著优势被选中，证明其生成的全局一致性更优。 消融实验： 损失函数分析： 仅用 GAN 损失：偏好率 58.90%。 GAN + 特征匹配损失：偏好率提升至 68.55%。 加入 VGG 感知损失：进一步提升细节质量，但非必需。 实例边界图的影： 用户对车辆边界的偏好率为 64.34%，证明实例信息显著提升边界真实性。 #### 生成结果可视化 场景合成： Cityscapes： 车辆、树木、建筑等对象的细节（如车窗反射、树叶纹理）更接近真实照片。 CRN 生成结果模糊且重复（如车轮形状不清晰），而本文方法避免了此类问题。 ADE20K： 复杂室内外场景（如厨房、街道）的生成质量与真实图像接近，尤其在光照和材质表现上。 多样化生成与交互编辑： 人脸编辑： 通过修改实例特征（如肤色、发型、妆容），实时生成多样化人脸（如添加胡须、调整唇色）。 用户可基于语义标签（如“眼睛”“嘴巴”）进行局部调整。 边缘转照片： 输入边缘图（如人脸轮廓、猫的草图），生成高分辨率自然图像（如逼真肖像、猫的毛发细节）。 ### 论文代码分析 #### Pix2PixHDModel.forward() [pix2pixHD_model](https://github.com/NVIDIA/pix2pixHD/blob/master/models/pix2pixHD_model.py#L152) ```python def forward(self, label, inst, image, feat, infer False): input_label, inst_map, real_image, feat_map self.encode_input(label, inst, image, feat) ``` > 将输入的标签、实例图、真实图像和特征图进行编码 ```python # 生成假的图像 if self.use_features: if not self.opt.load_features: feat_map self.netE.forward(real_image, inst_map) input_concat torch.cat((input_label, feat_map), dim 1) else: input_concat input_label fake_image self.netG.forward(input_concat) ``` > 如果启用实例特征：若未加载预存特征（`load_features False`），通过编码器 `netE` 从真实图像提取特征 `feat_map`。将语义标签 `input_label` 和特征图 `feat_map` 沿通道维度拼接（`dim 1`）。 > 如果不使用特征，则直接以语义标签 `input_label` 为输入。 > 生成器 `netG` 根据输入生成假图像 `fake_image`。 ```python # Fake Detection and Loss pred_fake_pool self.discriminate(input_label, fake_image, use_pool True) loss_D_fake self.criterionGAN(pred_fake_pool, False) # Real Detection and Loss pred_real self.discriminate(input_label, real_image) loss_D_real self.criterionGAN(pred_real, True) # GAN loss (Fake Passability Loss) pred_fake self.netD.forward(torch.cat((input_label, fake_image), dim 1)) loss_G_GAN self.criterionGAN(pred_fake, True) ``` > 1. 通过 `discriminate` 方法检测假图像 `fake_image` 的真实性，并计算假图像的对抗损失 `loss_D_fake`。 > 2. 类似地计算真实图像的对抗损失 `loss_D_real`。 > 3. 直接通过判别器对假图像的输出 `pred_fake` 计算假图像的对抗损失 `loss_G_GAN`，生成器需要欺骗判别器。 ```python # GAN 特征匹配损失 loss_G_GAN_Feat 0 if not self.opt.no_ganFeat_loss: feat_weights 4.0 / (self.opt.n_layers_D + 1) D_weights 1.0 / self.opt.num_D for i in range(self.opt.num_D): for j in range(len(pred_fake[i]) 1): loss_G_GAN_Feat + D_weights * feat_weights * \\ self.criterionFeat(pred_fake[i][j], pred_real[i][j].detach()) * self.opt.lambda_feat ``` > 遍历每个判别器及其各层，对生成图像和真实图像在判别器各层的特征图计算 L1 损失。 > 通过 `feat_weights`（层权重）和` D_weights`（判别器权重）平衡不同层和判别器的贡献，最终乘以全局权重 `lambda_feat`。 ```python # VGG 特征匹配损失 loss_G_VGG 0 if not self.opt.no_vgg_loss: loss_G_VGG self.criterionVGG(fake_image, real_image) * self.opt.lambda_feat ``` > 如果启用 VGG 特征匹配损失，计算假图像和真实图像的 VGG 特征图之间的损失，乘以全局权重 `lambda_feat`。 #### train [pix2pixHD/train.py](https://github.com/NVIDIA/pix2pixHD/blob/master/train.py) ```python ############## Forward Pass ###################### losses, generated model(Variable(data['label']), Variable(data['inst']), Variable(data['image']), Variable(data['feat']), infer save_fake) # sum per device losses losses [ torch.mean(x) if not isinstance(x, int) else x for x in losses ] loss_dict dict(zip(model.module.loss_names, losses)) # calculate final loss scalar loss_D (loss_dict['D_fake'] + loss_dict['D_real']) * 0.5 loss_G loss_dict['G_GAN'] + loss_dict.get('G_GAN_Feat',0) + \\ loss_dict.get('G_VGG',0) ``` > 前向传播：从 `data` 中提取标签（label）、实例图（inst）、真实图像（image）和特征（feat），并调用 `model.forward()` 生成假图像并计算损失。 > 计算损失： > 判别器损失：`loss_D` 为真假图像损失的平均值。 > 生成器损失：`loss_G` 包含 GAN 损失、特征匹配损失（若启用）和 VGG 损失（若启用）。 ```python ############### Backward Pass #################### # 更新生成器权重 optimizer_G.zero_grad() if opt.fp16: # 若启用 FP16，使用 amp.scale_loss 缩放损失以避免梯度下溢。 with amp.scale_loss(loss_G, optimizer_G) as scaled_loss: scaled_loss.backward() else: loss_G.backward() optimizer_G.step() # 更新鉴别器权重 optimizer_D.zero_grad() if opt.fp16: with amp.scale_loss(loss_D, optimizer_D) as scaled_loss: scaled_loss.backward() else: loss_D.backward() optimizer_D.step() ``` > 1. 梯度清零 > 2. 混合精度训练 > 3. 参数更新 ## 对比 方法 提出论文 核心思想 贡献 : :: :: :: : 基础 PatchGAN pix2pix (2017) 局部感受野的真伪判断 高效局部优化，提升细节真实性 3 级金字塔判别器 pix2pixHD (2018) 多尺度图像金字塔 + PatchGAN 高分辨率生成，多尺度一致性优化"},"/StyleTransfer/ref_and_notes/pytorch_basic_workflow.html":{"title":"PyTorch 基本工作流","content":"Reference:[PyTorchWorkflowFundamentals](https://www.learnpytorch.io/01_pytorch_workflow/)*该页面由JupyterNotebook生成，原文件于[Github](https://github.com/Fingsinz/StyleTransfer/tree/main/src/02.pytorch_learning/pytorch_basic_workflow.ipynb)*#先导入包importtorchfromtorchimportnnimportmatplotlib.pyplotasplttorch.__version__##准备数据集数据可以是很多东西，如一个表格、任何类型的图像、视频、歌曲或播客等音频文件，蛋白质结构，文本等等。机器学习是一个由两部分组成：1.把你的数据转换成数字表示。2.选择或构建一个模型来尽可能地学习数据的表征。获得数据之后，需要将数据划分为训练集、验证集和测试集。类型目的占比使用情况: :: :: :: :训练集模型从这些数据里面学习（比如学习的课程材料）\\~60\\~80%必须有验证集模型会根据这些数据进行调整（比如期末考试前的练习）\\~10\\~20%不必有测试集模型根据这些数据进行评估，以测试它学到了什么（比如期末考试）\\~10\\~20%必须有#创建一个y weight*x+bias的数据集weight 0.7bias 0.3X torch.arange(0,1,0.02).unsqueeze(dim 1)y weight*X+bias#划分训练集和测试集train_split int(0.8*len(X))X_train,y_train X[:train_split],y[:train_split]X_test,y_test X[train_split:],y[train_split:]defplot_predictions(train_data X_train,train_labels y_train,test_data X_test,test_labels y_test,predictions None):plt.figure(figsize (5,3))plt.scatter(train_data,train_labels,c \"b\",s 4,label \"Trainingdata\")plt.scatter(test_data,test_labels,c \"g\",s 4,label \"Testingdata\")ifpredictionsisnotNone:plt.scatter(test_data,predictions,c \"r\",s 4,label \"Prediction\")plt.legend(prop {\"size\":8})plot_predictions()##构建模型###PyTorch模型构建要点PyTorch有四个基本模块，可以用它来创建神经网络： [torch.nn](https://pytorch.org/docs/stable/nn.html)； [torch.optim](https://pytorch.org/docs/stable/optim.html)； [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)； [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html)；模块作用: :: :`torch.nn`包含计算图的所有构建块`torch.nn.Parameter`存储可用于`nn.Module`的张量。如果`requires_grad True`则自动计算梯度（用于通过梯度下降更新模型参数），这通常被称为“autograd”`torch.nn.Module`所有神经网络模块的基类，神经网络的所有构建块都是子类。在PyTorch中构建一个神经网络，模型应该继承`nn.Module`，需要实现`forward()`方法`torch.optim`包含各种优化算法（这些算法告诉存储在`nn.Parameter`中的模型参数。如何最好地改变，以改善梯度下降，从而减少损失)`defforward()`所有的`nn.Module`子类都需要一个`forward()`方法，定义传递给特定`nn.Module`的数据进行的计算（例如线性回归公式）简而言之： `nn.Module`包括大的构建块，如神经网络中的层； `nn.Parameter`包括小的参数，比如权重和偏置，众多参数构成`nn.Module`； `forward()`定义了在`nn.Module`中对输入的计算； `torch.optim`包含如何改进`nn.Parameter`中的参数的算法，以更好地表征数据一个简单的神经网络例子：classLinearRegressionModel(nn.Module):def__init__(self):super().__init__()self.weights nn.Parameter(torch.randn(1,dtype torch.float),requires_grad True)self.bias nn.Parameter(torch.randn(1,dtype torch.float),requires_grad True)defforward(self,x:torch.Tensor) >torch.Tensor:returnself.weights*x+self.bias###检视模型中的内容使用`model.parameters()`检查参数：torch.manual_seed(42)model_0 LinearRegressionModel()list(model_0.parameters())使用`model.state_dict()`获得模型状态（包含什么）：model_0.state_dict()###使用`torch.inference_mode()`进行预测`torch.inference_mode()`关闭了一些东西，比如梯度跟踪（训练所必需的，但不是推理所必需的），所以`forward`传递更快。withtorch.inference_mode():y_preds model_0(X_test)plot_predictions(predictions y_preds)由图可知模型预测距离真实值仍有一段距离，所以需要训练模型以达到更好的效果。##训练模型###建立损失函数和优化器为了让模型自己更新参数，需要添加损失函数和优化器。功能作用PyTorch的形式常用值: :: :: :: :损失函数测量模型的预测（`y_preds`）与真值标签（`y_test`）相比的错误程度。越低越好`torch.nn`中有很多内置的损失函数回归问题的平均绝对误差（MAE，`torch.nn.L1Loss()`）；二元分类问题的二元交叉熵（`torch.nn.BCELoss()`）优化器告诉模型如何更新其内部参数以最大程度地降低损失`torch.optim`中的各种优化函数实现随机梯度下降（`torch.optim.SGD()`）；Adam优化器（`torch.optim.Adam()`）为模型添加损失函数和优化器：###训练循环训练过程有以下步骤：序号步骤做法代码: :: :: :: :1前向传播模型将所有训练数据遍历一次，执行其`forward()`函数计算`model(x_train)`2计算损失将模型的输出（预测）与真实数据进行评估，计算损失值`loss loss_fn(y_pred,y_train)`3梯度归零优化器的梯度被设置为零（默认情况下它们是累积的），因此它们可以为特定的训练步骤重新计算`optimizer.zero_grad()`4反向传播损失计算每个要更新的模型参数的损失梯度（每个参数`requires_grad True`）`loss.backward()`5更新参数使用`requires_grad True`更新损耗梯度的参数`optimizer.step()`###测试循环测试过程有以下步骤：序号步骤做法代码: :: :: :: :1前向传播模型将所有训练数据遍历一次，执行其`forward()`函数计算`model(x_train)`2计算损失将模型的输出（预测）与真实数据进行评估，计算损失值`loss loss_fn(y_pred,y_train)`3计算评估指标（可选）计算其他评估指标，例如测试集上的准确性自定义函数###训练并测试代码torch.manual_seed(42)device \"cuda\"iftorch.cuda.is_available()else\"cpu\"X_train X_train.to(device)y_train y_train.to(device)X_test X_test.to(device)y_test y_test.to(device)model_0 model_0.to(device)epochs 200train_loss_values []test_loss_values []epoch_cnt []loss_fn nn.L1Loss()#损失函数optimizer torch.optim.SGD(params model_0.parameters(),lr 0.01)#优化器forepochinrange(epochs):model_0.train()#设置为训练模式#1.前向传播y_pred model_0(X_train)#2.计算损失loss loss_fn(y_pred,y_train)#3.梯度归零、反向传播、更新参数optimizer.zero_grad()loss.backward()optimizer.step()model_0.eval()#设置为评估模式withtorch.inference_mode():test_pred model_0(X_test)test_loss loss_fn(test_pred,y_test.type(torch.float))ifepoch%20 0:epoch_cnt.append(epoch)train_loss_values.append(loss.cpu().detach().numpy())test_loss_values.append(test_loss.cpu().detach().numpy())print(f\"{epoch}/{epochs}MAE训练损失:{loss}MAE测试损失:{test_loss}\")0/200 MAE 训练损失: 0.31288135051727295 MAE 测试损失: 0.48106518387794495 20/200 MAE 训练损失: 0.08908725529909134 MAE 测试损失: 0.21729658544063568 40/200 MAE 训练损失: 0.04543796926736832 MAE 测试损失: 0.11360953003168106 60/200 MAE 训练损失: 0.03818932920694351 MAE 测试损失: 0.08886633068323135 80/200 MAE 训练损失: 0.03132382780313492 MAE 测试损失: 0.07232122868299484 100/200 MAE 训练损失: 0.024458957836031914 MAE 测试损失: 0.05646304413676262 120/200 MAE 训练损失: 0.01758546754717827 MAE 测试损失: 0.04060482606291771 140/200 MAE 训练损失: 0.010716589167714119 MAE 测试损失: 0.024059748277068138 160/200 MAE 训练损失: 0.003851776709780097 MAE 测试损失: 0.008201557211577892 180/200 MAE 训练损失: 0.008932482451200485 MAE 测试损失: 0.005023092031478882损失随着时间的推移而下降，绘图：plt.figure(figsize (5,3))plt.plot(epoch_cnt,train_loss_values,label \"TrainLoss\")plt.plot(epoch_cnt,test_loss_values,label \"TestLoss\")plt.title(\"LossCurves\")plt.ylabel(\"Loss\")plt.xlabel(\"Epochs\")plt.legend()输出一下训练得到的`weight`和`bias`：print(model_0.state_dict())print(f\"{weight},{bias}\")OrderedDict([('weights', tensor([0.6990], device 'cuda:0')), ('bias', tensor([0.3093], device 'cuda:0'))]) 0.7, 0.3由此可知训练得到的参数与实际的参数已经相差很小了。##使用训练后的PyTorch模型进行推理在使用PyTorch模型进行预测（也称为执行推理）时，需要记住三件事：1.将模型设置为评估模式（`model.eval()`）。2.使用推理模式上下文管理器（使用`torch.inference_mode()`）进行预测。3.所有的预测都应该在同一设备上进行（仅在GPU上的数据和模型或仅在CPU上的数据和模型）。前两项确保关闭PyTorch在训练期间在幕后使用的所有有用的计算和设置，但这些计算和设置对于推理是不必要的（这导致更快的计算）。第三个确保你不会遇到跨设备错误。最后查看整体的分布：model_0.eval()withtorch.inference_mode():y_preds model_0(X_test)plot_predictions(predictions y_preds.cpu())##保存和加载模型比如在服务器上训练模型后，需要转移到本地或其他地方进行使用。方法作用: :: :`torch.save`使用Python的`pickle`实用程序将序列化的对象保存到磁盘。模型、张量和其他各种Python对象（如字典）都可以使用`torch.save`保存`torch.load`使用`pickle`的unpickling特性来反序列化并将文件（如模型，张量或字典）加载到内存中。也可以设置加载对象到哪个设备（CPU，GPU等）`torch.nn.Module.load_state_dict`使用已保存的`state_dict()`对象加载模型的参数字典（`model.state_dict()`）###保存模型的参数字典torch.save(obj model_0.state_dict(),f \"model_0.pth\")###加载参数字典到模型中loaded_model_0 LinearRegressionModel()loaded_model_0.load_state_dict(torch.load(f \"model_0.pth\",weights_only True))#测试看看预测结果是否相等loaded_model_0.to(device)loaded_model_0.eval()withtorch.inference_mode():loaded_model_preds loaded_model_0(X_test)loaded_model_preds y_preds"},"/StyleTransfer/ref_and_notes/pytorch_classification.html":{"title":"PyTorch 神经网络分类","content":"Reference:[PyTorchNeuralNetworkClassification](https://www.learnpytorch.io/02_pytorch_classification/)*该页面由JupyterNotebook生成，原文件于[Github](https://github.com/Fingsinz/StyleTransfer/tree/main/src/02.pytorch_learning/pytorch_classification.ipynb)*#先导入包importtorchfromtorchimportnnimportmatplotlib.pyplotasplttorch.__version__分类问题有二分类、多分类、多标签等情况。二分类问题则是或不是；多分类问题具有多个类别区分；多标签问题则一个目标可以被分配多个选项。##分类神经网络的结构分类神经网络的一般架构：项目二分类多分类: :: :: :输入层Shape（`in_features`）与特征数相同与特征数相同隐藏层特定问题特定分析特定问题特定分析每个隐藏层的神经元数量特定问题特定分析，一般从10到512特定问题特定分析，一般从10到512输出层Shape（`out_features`）1（一个类别）每个类1个输出隐藏层激活函数通常是ReLU通常是ReLU输出层激活函数Sigmoid（`torch.sigmoid`）Softmax（`torch.softmax`）损失函数二元交叉熵（`torch.nn.BCELoss`）交叉熵（`torch.nn.CrossEntropyLoss`）优化器SGD，AdamSGD，Adam##准备二分类数据集###输入和输出形状使用Scikit Learn中的`make_circles()`方法生成两个带有不同颜色圆点的圆。*需要安装Scikit Learn：`pipinstallscikit learn`*fromsklearn.datasetsimportmake_circlesn_samples 1000X,y make_circles(n_samples,noise 0.03,random_state 42)plt.scatter(x X[:,0],y X[:,1],c y,cmap plt.cm.RdYlBu)看看输入Shape和输出Shape，然后弄清楚输入层Shape（特征数）和输出层Shape。X.shape,y.shape#输入Shape和输出ShapeX[0].shape,y[0].shape#输入层Shape和输出层Shape这说明X的一个样本有两个特征（向量），而对应的y只有一个特征（标量）。 有两个输入对应一个输出。###划分数据集具体来说：1.将数据转换为张量。2.将数据分成训练集和测试集。X torch.from_numpy(X).type(torch.float)y torch.from_numpy(y).type(torch.float)X.dtype,y.dtype使用Scikit Learn中的函数`train_test_split()`。`test_size 0.2`（80%训练，20%测试），因为分割是随机发生的，所以使用`random_state 42`，使得随机可复现。fromsklearn.model_selectionimporttrain_test_splitX_train,X_test,y_train,y_test train_test_split(X,y,test_size 0.2,random_state 42)len(X_train),len(y_train),len(X_test),len(y_test)##构建分类模型构建模型的步骤：1.设置与设备相关的代码。2.通过继承`nn.module`来构造一个模型。3.定义损失函数和优化器。4.创建一个训练循环。###设置设备#1.设置设备device \"cuda\"iftorch.cuda.is_available()else\"cpu\"device###构建模型对象模型类的操作：1.继承`nn.Module`。2.在构造函数中创建2层`nn.Linear`线性层，能够处理X和y的形状。3.定义一个`forward()`方法，该方法包含模型的前向传递计算。4.实例化模型类并将其发送到目标设备。classCircleModelV0(nn.Module):def__init__(self):super().__init__()self.layer_1 nn.Linear(in_features 2,out_features 5)self.layer_2 nn.Linear(in_features 5,out_features 1)defforward(self,x):returnself.layer_2(self.layer_1)model_0 CircleModelV0().to(device)model_0由上面代码可知该模型类的结构为：`2(输入层) >5(隐藏层) >1(输出层)`也可以使用`nn.Sequential`执行与上面相同的操作。`nn.Sequential`按层出现的顺序对输入数据执行前向传递计算。model_0 nn.Sequential(nn.Linear(in_features 2,out_features 5),nn.Linear(in_features 5,out_features 1)).to(device)model_0自定义模型类可以自定义更多细节，而`nn.Sequential()`则更方便。###定义损失函数和优化器常见损失函数：损失函数适用类型代码: :: :: :交叉熵损失函数多分类`torch.nn.CrossEntropyLoss`平均绝对误差MAE，L1Loss回归问题`torch.nn.L1Loss`均方误差MSE，L2Loss回归问题`torch.nn.MSELoss`常见优化器：优化器适用类型代码: :: :: :随机梯度下降（SGD）分类问题、回归问题等`torch.optim.SGD()`Adam分类问题、回归问题等`torch.optim.Adam()`此处讨论二分类问题，使用一个二元交叉熵损失函数。>注意：损失函数是衡量模型预测错误程度的函数，损失越高，模型越差。>>此外，PyTorch文档经常将损失函数称为“损失准则（losscriterion）”或“准则（criterion）”，这些都是描述同一事物的不同方式。二元交叉熵函数有`torch.nn.BCELoss()`和`torch.nn.BCEWithLogitsLoss()`。 `torch.nn.BCELoss()`：创建一个损失函数，用于测量目标（标签）和输入（特征）之间的二进制交叉熵。 `torch.nn.BCEWithLogitsLoss()`：它内置了一个sigmoid层，其他这与上面的相同。`torch.nn.BCEWithLogitsLoss()`的文档指出，它比在`nn.Sigmoid`层之后使用`torch.nn.BCELoss()`在数值上更稳定。对于优化器，将使用`torch.optim.SGD()`以0.1的学习率优化模型参数。loss_fn nn.BCEWithLogitsLoss()optimizer torch.optim.SGD(params model_0.parameters(),lr 0.1)评估指标可用于提供关于模型运行情况的另一个视角。如果一个损失函数衡量模型的错误程度，那么也有评估指标衡量他的正确程度。defaccuracy_fn(y_true,y_pred):correct torch.eq(y_true,y_pred).sum().item()acc (correct/len(y_pred))*100returnacc##训练分类模型###将原始输出变成标签线性层的公式为：$$y x\\cdot\\text{Weights}^T+bias$$模型的原始输出通常被称为logits。使用激活函数将logits转换成与真值标签相比较的数字。###构建训练和测试循环torch.manual_seed(42)epochs 100X_train,y_train X_train.to(device),y_train.to(device)X_test,y_test X_test.to(device),y_test.to(device)model_0 model_0.to(device)forepochinrange(epochs):model_0.train()y_logits model_0(X_train).squeeze()y_pred torch.round(torch.sigmoid(y_logits))loss loss_fn(y_logits,y_train)acc accuracy_fn(y_true y_train,y_pred y_pred)optimizer.zero_grad()loss.backward()optimizer.step()model_0.eval()withtorch.inference_mode():test_logits model_0(X_test).squeeze()test_pred torch.round(torch.sigmoid(test_logits))test_loss loss_fn(test_logits,y_test)test_acc accuracy_fn(y_true y_test,y_pred test_pred)ifepoch%10 0:print(f\"{epoch}/{epochs}Loss:{loss:.5f},Accu:{acc:.2f}%TestLoss:{test_loss:.5f},TestAccu:{test_acc:.2f}%\")0/100 Loss: 0.70365, Accu: 49.88% Test Loss: 0.71542, Test Accu: 45.50% 10/100 Loss: 0.70059, Accu: 50.25% Test Loss: 0.71142, Test Accu: 45.00% 20/100 Loss: 0.69869, Accu: 50.38% Test Loss: 0.70858, Test Accu: 46.00% 30/100 Loss: 0.69740, Accu: 50.75% Test Loss: 0.70641, Test Accu: 46.00% 40/100 Loss: 0.69648, Accu: 50.75% Test Loss: 0.70469, Test Accu: 45.50% 50/100 Loss: 0.69580, Accu: 50.50% Test Loss: 0.70329, Test Accu: 46.50% 60/100 Loss: 0.69527, Accu: 50.75% Test Loss: 0.70214, Test Accu: 46.00% 70/100 Loss: 0.69487, Accu: 50.88% Test Loss: 0.70117, Test Accu: 45.50% 80/100 Loss: 0.69455, Accu: 50.75% Test Loss: 0.70036, Test Accu: 45.00% 90/100 Loss: 0.69429, Accu: 50.75% Test Loss: 0.69966, Test Accu: 45.00%模型看起来它很好地完成了训练和测试步骤，但结果似乎并没有太大的变化。每次数据分割时，准确率在50%左右。这是一个平衡的二进制分类问题，这意味着模型的性能与随机猜测差不多。##预测评估分类模型从指标来看，模型似乎是随机猜测。绘制一个模型预测的图，它试图预测的数据以及它为某个东西是类0还是类1创建的决策边界。为此，编写一些代码，一个名为`plot_decision_boundary()`的有用函数，该函数创建一个NumPymeshgrid，以可视化地绘制我们的模型预测某些类的不同点。importnumpyasnpdefplot_decision_boundary(model:torch.nn.Module,X:torch.Tensor,y:torch.Tensor):\"\"\"PlotsdecisionboundariesofmodelpredictingonXincomparisontoy.Source https://madewithml.com/courses/foundations/neural networks/(withmodifications)\"\"\"#PuteverythingtoCPU(worksbetterwithNumPy+Matplotlib)model.to(\"cpu\")X,y X.to(\"cpu\"),y.to(\"cpu\")#Setuppredictionboundariesandgridx_min,x_max X[:,0].min() 0.1,X[:,0].max()+0.1y_min,y_max X[:,1].min() 0.1,X[:,1].max()+0.1xx,yy np.meshgrid(np.linspace(x_min,x_max,101),np.linspace(y_min,y_max,101))#MakefeaturesX_to_pred_on torch.from_numpy(np.column_stack((xx.ravel(),yy.ravel()))).float()#Makepredictionsmodel.eval()withtorch.inference_mode():y_logits model(X_to_pred_on)#Testformulti classorbinaryandadjustlogitstopredictionlabelsiflen(torch.unique(y))>2:y_pred torch.softmax(y_logits,dim 1).argmax(dim 1)#mutli classelse:y_pred torch.round(torch.sigmoid(y_logits))#binary#Reshapepredsandploty_pred y_pred.reshape(xx.shape).detach().numpy()plt.contourf(xx,yy,y_pred,cmap plt.cm.RdYlBu,alpha 0.7)plt.scatter(X[:,0],X[:,1],c y,s 40,cmap plt.cm.RdYlBu)plt.xlim(xx.min(),xx.max())plt.ylim(yy.min(),yy.max())plt.figure(figsize (12,6))plt.subplot(1,2,1)plt.title(\"Train\")plot_decision_boundary(model_0,X_train,y_train)plt.subplot(1,2,2)plt.title(\"Test\")plot_decision_boundary(model_0,X_test,y_test)由图可知，模型目前正在尝试用直线分割红点和蓝点。由于我们的数据是圆形的，所以画一条直线最多只能把它从中间切开。在机器学习方面，模型是欠拟合的，这意味着它没有从数据中学习预测模式。##改进模型尝试解决模型的拟合不足问题。 专注于模型（而不是数据）。技巧作用: :: :增加更多隐藏层每一层都可能增加模型的学习能力，每一层都能够学习数据中的某种新模式。更多的层通常被称为使神经网络更深增加更多隐藏神经元与上面类似，每层更多的隐藏单元意味着模型学习能力的潜在增加。更多的隐藏单元通常被称为使你的神经网络更宽增加训练轮数如果模型训练更久，它可能会学到更多改变激活函数有些数据无法仅用直线拟合，使用非线性激活函数可以帮助解决这个问题改变学习率优化器的学习率决定了模型每一步应该改变多少参数，太多了模型会过度校正，太少了模型学习不足改变损失函数不同的问题需要不同的损失函数迁移学习从一个与问题领域相似的问题领域中提取一个预先训练好的模型，并根据问题进行调整###添加非线性classCircleModelV1(nn.Module):def__init__(self):super().__init__()self.layer_1 nn.Linear(in_features 2,out_features 10)self.layer_2 nn.Linear(in_features 10,out_features 10)self.layer_3 nn.Linear(in_features 10,out_features 1)self.relu nn.ReLU()defforward(self,x):returnself.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))model_1 CircleModelV1().to(device)#model_1 nn.Sequential(#nn.Linear(2,10),#nn.ReLU(),#nn.Linear(10,10),#nn.ReLU(),#nn.Linear(10,1),#).to(device)model_1loss_fn nn.BCEWithLogitsLoss()optimizer torch.optim.SGD(params model_1.parameters(),lr 0.1)重新训练：torch.manual_seed(42)epochs 1500X_train,y_train X_train.to(device),y_train.to(device)X_test,y_test X_test.to(device),y_test.to(device)model_1.to(device)forepochinrange(epochs):model_1.train()y_logits model_1(X_train).squeeze()y_pred torch.round(torch.sigmoid(y_logits))loss loss_fn(y_logits,y_train)acc accuracy_fn(y_true y_train,y_pred y_pred)optimizer.zero_grad()loss.backward()optimizer.step()model_1.eval()withtorch.inference_mode():test_logits model_1(X_test).squeeze()test_pred torch.round(torch.sigmoid(test_logits))test_loss loss_fn(test_logits,y_test)test_acc accuracy_fn(y_true y_test,y_pred test_pred)ifepoch%100 0:print(f\"{epoch}/{epochs}Loss:{loss:.5f},Accu:{acc:.2f}%TestLoss:{test_loss:.5f},TestAccu:{test_acc:.2f}%\")0/1500 Loss: 0.69295, Accu: 50.00% Test Loss: 0.69319, Test Accu: 50.00% 100/1500 Loss: 0.69115, Accu: 52.88% Test Loss: 0.69102, Test Accu: 52.50% 200/1500 Loss: 0.68977, Accu: 53.37% Test Loss: 0.68940, Test Accu: 55.00% 300/1500 Loss: 0.68795, Accu: 53.00% Test Loss: 0.68723, Test Accu: 56.00% 400/1500 Loss: 0.68517, Accu: 52.75% Test Loss: 0.68411, Test Accu: 56.50% 500/1500 Loss: 0.68102, Accu: 52.75% Test Loss: 0.67941, Test Accu: 56.50% 600/1500 Loss: 0.67515, Accu: 54.50% Test Loss: 0.67285, Test Accu: 56.00% 700/1500 Loss: 0.66659, Accu: 58.38% Test Loss: 0.66322, Test Accu: 59.00% 800/1500 Loss: 0.65160, Accu: 64.00% Test Loss: 0.64757, Test Accu: 67.50% 900/1500 Loss: 0.62362, Accu: 74.00% Test Loss: 0.62145, Test Accu: 79.00% 1000/1500 Loss: 0.56818, Accu: 87.75% Test Loss: 0.57378, Test Accu: 86.50% 1100/1500 Loss: 0.48153, Accu: 93.50% Test Loss: 0.49935, Test Accu: 90.50% 1200/1500 Loss: 0.37056, Accu: 97.75% Test Loss: 0.40595, Test Accu: 92.00% 1300/1500 Loss: 0.25458, Accu: 99.00% Test Loss: 0.30333, Test Accu: 96.50% 1400/1500 Loss: 0.17180, Accu: 99.50% Test Loss: 0.22108, Test Accu: 97.50%可视化一下：plt.figure(figsize (12,6))plt.subplot(1,2,1)plt.title(\"Train\")plot_decision_boundary(model_1,X_train,y_train)plt.subplot(1,2,2)plt.title(\"Test\")plot_decision_boundary(model_1,X_test,y_test)现在模型的分类就有了显著的效果。##多分类问题###构建多分类数据集利用Scikit Learn的`make_blobs()`方法。这个方法将创建任意数量的类（使用`centers`参数）。1.使用`make_blobs()`创建一些多类数据。2.将数据转换为张量（`make_blobs()`的默认值是使用NumPy数组）。3.使用`train_test_split()`将数据拆分为训练集和测试集。4.可视化数据。importtorchimportmatplotlib.pyplotaspltfromsklearn.datasetsimportmake_blobsfromsklearn.model_selectionimporttrain_test_splitNUM_CLASSES 4NUM_FEATURES 2RANDOM_SEED 42X_blob,y_blob make_blobs(n_samples 1000,n_features NUM_FEATURES,centers NUM_CLASSES,cluster_std 1.5,random_state RANDOM_SEED)X_blob torch.from_numpy(X_blob).type(torch.float)y_blob torch.from_numpy(y_blob).type(torch.LongTensor)X_blob_train,X_blob_test,y_blob_train,y_blob_test train_test_split(X_blob,y_blob,test_size 0.2,random_state RANDOM_SEED)plt.figure(figsize (10,6))plt.scatter(X_blob[:,0],X_blob[:,1],c y_blob,cmap plt.cm.RdYlBu)###构建多元分类模型创建一个`nn.Module`的子类，接受三个超参数： `input_features`：输入特征的数量。 `output_features`：输出特征数（等效于`NUM_CLASSES`或多类分类问题中的类数）。 `hidden_units`：每个隐藏层使用的隐藏神经元的数量。device \"cuda\"iftorch.cuda.is_available()else\"cpu\"fromtorchimportnnclassBlobModel(nn.Module):def__init__(self,input_features,output_features,hidden_units 8):super().__init__()self.model nn.Sequential(nn.Linear(in_features input_features,out_features hidden_units),nn.ReLU(),nn.Linear(in_features hidden_units,out_features hidden_units),nn.ReLU(),nn.Linear(in_features hidden_units,out_features output_features))defforward(self,x):returnself.model(x)model_2 BlobModel(input_features NUM_FEATURES,output_features NUM_CLASSES,hidden_units 8).to(device)model_2###构建多元分类损失函数和优化器loss_fn nn.CrossEntropyLoss()optimizer torch.optim.SGD(model_2.parameters(),lr 0.1)试着看看模型前向输出：y_logits model_2(X_blob_train.to(device))[:5]y_logits再看看经过激活函数Softmax之后的结果：y_pred_probs torch.softmax(y_logits,dim 1)y_pred_probs经过Softmax函数之后，先前的数字变成预测到某类的概率。这些预测概率本质上是说模型认为目标样本（输入）映射到每个类的程度。由于y_pred_probs中的每个类都有一个值，因此最高值的索引是模型认为特定数据样本最属于的类。可以使用`torch.argmax()`检查哪个索引具有最高值。torch.argmax(y_pred_probs[0])###构建多分类训练和测试循环torch.manual_seed(42)epochs 100X_blob_train,y_blob_train X_blob_train.to(device),y_blob_train.to(device)X_blob_test,y_blob_test X_blob_test.to(device),y_blob_test.to(device)model_2.to(device)forepochinrange(epochs):model_2.train()y_logits model_2(X_blob_train)y_pred torch.softmax(y_logits,dim 1).argmax(dim 1)loss loss_fn(y_logits,y_blob_train)acc accuracy_fn(y_true y_blob_train,y_pred y_pred)optimizer.zero_grad()loss.backward()optimizer.step()model_2.eval()withtorch.inference_mode():test_logits model_2(X_blob_test)test_pred torch.softmax(test_logits,dim 1).argmax(dim 1)test_loss loss_fn(test_logits,y_blob_test)tess_acc accuracy_fn(y_true y_blob_test,y_pred test_pred)ifepoch%10 0:print(f\"{epoch}/{epochs}Loss:{loss:.5f},Acc:{acc:.2f}%TestLoss:{test_loss:.5f},TestAcc:{test_acc:.2f}%\")0/100 Loss: 1.15883, Acc: 40.38% Test Loss: 1.07554, Test Acc: 99.00% 10/100 Loss: 0.64476, Acc: 96.75% Test Loss: 0.66069, Test Acc: 99.00% 20/100 Loss: 0.42535, Acc: 98.50% Test Loss: 0.43074, Test Acc: 99.00% 30/100 Loss: 0.25294, Acc: 99.12% Test Loss: 0.24508, Test Acc: 99.00% 40/100 Loss: 0.11232, Acc: 99.25% Test Loss: 0.10229, Test Acc: 99.00% 50/100 Loss: 0.06627, Acc: 99.25% Test Loss: 0.05848, Test Acc: 99.00% 60/100 Loss: 0.05068, Acc: 99.25% Test Loss: 0.04293, Test Acc: 99.00% 70/100 Loss: 0.04300, Acc: 99.25% Test Loss: 0.03491, Test Acc: 99.00% 80/100 Loss: 0.03836, Acc: 99.25% Test Loss: 0.02988, Test Acc: 99.00% 90/100 Loss: 0.03525, Acc: 99.25% Test Loss: 0.02663, Test Acc: 99.00%###评估多分类模型使用准确率评估：model_2.eval()withtorch.inference_mode():y_logits model_2(X_blob_test)y_preds torch.softmax(y_logits,dim 1).argmax(dim 1)print(f\"Testaccuracy:{accuracy_fn(y_true y_blob_test,y_pred y_preds)}%\")Test accuracy: 99.5%可视化评估：plt.figure(figsize (12,6))plt.subplot(1,2,1)plt.title(\"Train\")plot_decision_boundary(model_2,X_blob_train,y_blob_train)plt.subplot(1,2,2)plt.title(\"Test\")plot_decision_boundary(model_2,X_blob_test,y_blob_test)##更多分类评估指标评估指标定义代码: :: :: :正确率模型预测正确的占比`torchmetrics.Accuracy()`或`sklearn.metrics.accuracy_score()`精确率$\\text{Precision} \\frac{TP}{TP+FP}$`torchmetrics.Precision()`或`sklearn.metrics.precision_score()`召回率$\\text{Recall} \\frac{TP}{TP+FN}$`torchmetrics.Recall()`或`sklearn.metrics.recall_score()`F1 Score将查准率和查全率合并为一个指标。1是最好的，0是最坏的`torchmetrics.F1Score()`或`sklearn.metrics.f1_score()`混淆矩阵以表格方式将预测值与真实值进行比较，如果100%正确，矩阵中的所有值将从左上角到右下角（对角线）`torchmetrics.ConfusionMatrix`或`sklearn.metrics.plot_confusion_matrix()`分类报告一些主要分类指标的集合，如精度，召回率和f1分数`sklearn.metrics.classification_report()`"},"/StyleTransfer/ref_and_notes/pytorch_computer_vision.html":{"title":"计算机视觉基础","content":"Reference:[PyTorchComputerVision](https://www.learnpytorch.io/03_pytorch_computer_vision/)*该页面由JupyterNotebook生成，原文件于[Github](https://github.com/Fingsinz/StyleTransfer/tree/main/src/02.pytorch_learning/pytorch_computer_vision.ipynb)*##PyTorch的计算机视觉库PyTorch中关于计算机视觉的库有：模块作用: :: :[torchvision](https://pytorch.org/vision/stable/index.html)包含通常用于计算机视觉问题的数据集、模型架构和图像转换[torchvision.datasets](https://pytorch.org/vision/stable/datasets.html)包含许多计算机视觉数据集，用于解决图像分类、对象检测、图像字幕、视频分类等一系列问题，还包含一系列用于创建自定义数据集的基类[torchvision.models](https://pytorch.org/vision/stable/models.html)包含在PyTorch中实现的性能良好且常用的计算机视觉模型架构，可以将其用于解决问题[torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)图像需要在与模型一起使用之前进行预处理（转换为数字/处理/增强），包含常见的图像转换[torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)PyTorch基础数据集类[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#module torch.utils.data)创建一个可在数据集上迭代的对象（`torch.utils.data.Dataset`）#PyTorchimporttorchfromtorchimportnn#torchvisionimporttorchvisionfromtorchvisionimportdatasetsfromtorchvision.transformsimportToTensor#matplotlibimportmatplotlib.pyplotaspltprint(f\"{torch.__version__}\\n{torchvision.__version__}\")2.5.1+cu124 0.20.1+cu124device \"cuda\"iftorch.cuda.is_available()else\"cpu\"device##准备数据集`torchvision.datasets`包含了大量的示例数据集，可以使用它们来练习编写计算机视觉代码。 MNIST：手写数字数据集，包含数千个手写数字（从0到9）的示例。 FashionMNIST：有10个不同的图像类（不同类型的衣服），这是一个多类分类问题。FashionMNIST可以通过`torchvision.datasets.FashionMNIST`获得。提供以下参数： `root:str`，数据下载到哪个文件夹； `train:Bool`，训练还是测试分割； `download:Bool`，是否下载数据； `transform:torchvision.transforms`，对数据的转换； `target_transform`，对目标（标签）的转换。train_data datasets.FashionMNIST(root \"data\",train True,download True,transform ToTensor(),#图像以PIL格式出现，将其转换为Torch张量target_transform None)test_data datasets.FashionMNIST(root \"data\",train False,#表示测试download True,transform ToTensor())len(train_data),len(test_data)###了解数据集得到数据后，需要了解数据的Shape：image,label train_data[0]image.shape图像张量的形状是`[1，28，28]`或更具体地：`[color_channels 1，height 28，width 28]`。不同的问题会有不同的输入和输出形式。但前提仍然是将数据编码成数字，建立一个模型来找到这些数字中的模式，将这些模式表达成有意义的东西。>除了CHW（通道，高度，宽度）表示，后续还会见到NCHW和NHWC格式，其中N代表图像数量。例如，当batch_size 32，则张量形状可能是[32，1，28，28]。>>PyTorch通常接受NCHW（通道优先）作为许多操作的默认设置。然而，PyTorch还解释说，NHWC（通道最后）性能更好，被认为是最佳实践。>>由于目前的数据集和模型相对较小，这不会有太大的区别。了解数据集的数量后，还可以了解一下类别属性：class_names train_data.classesclass_names可以可视化一下数据：image,label train_data[0]plt.figure(figsize (4,4))plt.imshow(image.squeeze(),cmap \"gray\")#Shape为[1,28,28]，将通道挤压一下plt.title(label)查看更多：torch.manual_seed(42)fig plt.figure(figsize (7,7))rows,cols 4,4foriinrange(1,rows*cols+1):random_idx torch.randint(0,len(train_data),size [1]).item()img,label train_data[random_idx]fig.add_subplot(rows,cols,i)plt.imshow(img.squeeze(),cmap \"gray\")plt.title(class_names[label])plt.axis(False)###构建DataLoader现在已经有了一个数据集，下一步是用`torch.utils.data.DataLoader`准备它。DataLoader有助于将数据加载到模型中。为了训练和推理，它将一个大的数据集转换成一个可迭代的小块。 这些较小的块称为批处理或小批处理，可以通过`batch_size`参数进行设置，这样做可以让计算效率更高。对于小批量（数据的一小部分）而言，每个epoch执行梯度下降的频率更高。 每个batch一次，而不是每个epoch一次。`batch_size`是一个超参数，视情况而定。通常使用2的幂（例如32、64、128、256、512）。fromtorch.utils.dataimportDataLoaderBATCH_SIZE 32train_dataloader DataLoader(train_data,batch_size BATCH_SIZE,shuffle True#每一个epoch都洗牌数据)test_dataloader DataLoader(test_data,batch_size BATCH_SIZE,shuffle False)print(f\"Train:{len(train_dataloader)}\")print(f\"Test:{len(test_dataloader)}\")Train: 1875 Test: 313看看每个batch的shape：train_features_batch,train_labels_batch next(iter(train_dataloader))train_features_batch.shape,train_labels_batch.shape##模型0：构建基线模型数据已加载并准备就绪。基线模型是最简单的模型之一。使用基线作为起点，并尝试使用后续的更复杂的模型对其进行改进。目前基线模型将由两个`nn.Linear()`层组成。因为这是图像数据，所以将使用`nn.Flatten()`层开始。 `nn.Flatten()`将张量的维度压缩为单个向量。classFashionMNISTModelV0(nn.Module):def__init__(self,input_shape:int,hidden_units:int,output_shape:int):super().__init__()self.model nn.Sequential(nn.Flatten(),nn.Linear(in_features input_shape,out_features hidden_units),nn.Linear(in_features hidden_units,out_features output_shape))defforward(self,x):returnself.model(x)设置以下参数： `input_shape 784`：此例中，目标图像中的每个像素都有一个特征（28像素高×28像素宽 784个特征）。 `hidden_units 10`：隐藏层中的神经元数量。 `output_shape len(class_names)`：多分类问题，需要为数据集中的每个类提供一个输出神经元。torch.manual_seed(42)model_0 FashionMNISTModelV0(input_shape 784,hidden_units 10,output_shape len(class_names))设置上损失函数和优化器：loss_fn nn.CrossEntropyLoss()optimizer torch.optim.SGD(params model_0.parameters(),lr 0.1)自定义计算准确率的函数和计时函数：defaccuracy_fn(y_true,y_pred):correct torch.eq(y_true,y_pred).sum().item()acc (correct/len(y_pred))*100returnaccfromtimeitimportdefault_timerastimerdefprint_train_time(start:float,end:float,device:torch.device None):total_time end startprint(f\"{device}:{total_time:.3f}seconds\")returntotal_time构建训练和测试循环（CPU）：torch.manual_seed(42)train_time_start timer()epochs 3forepochinrange(epochs):print(f\"Epoch:{epoch}\\n \")train_loss 0forbatch,(X,y)inenumerate(train_dataloader):model_0.train()y_pred model_0(X)loss loss_fn(y_pred,y)train_loss+ lossoptimizer.zero_grad()loss.backward()optimizer.step()train_loss/ len(train_dataloader)test_loss,test_acc 0,0model_0.eval()withtorch.inference_mode():forX,yintest_dataloader:test_pred model_0(X)test_loss+ loss_fn(test_pred,y)test_acc+ accuracy_fn(y_true y,y_pred test_pred.argmax(dim 1))test_loss/ len(test_dataloader)test_acc/ len(test_dataloader)print(f\"\\n训练loss:{train_loss:.5f}测试loss:{test_loss:.5f},测试acc:{test_acc:.2f}%\\n\")train_time_end timer()total_train_time_model_0 print_train_time(start train_time_start,end train_time_end,device str(next(model_0.parameters()).device))Epoch: 0 训练 loss: 0.59039 测试 loss: 0.50954, 测试 acc: 82.04% Epoch: 1 训练 loss: 0.47633 测试 loss: 0.47989, 测试 acc: 83.20% Epoch: 2 训练 loss: 0.45503 测试 loss: 0.47664, 测试 acc: 83.43% cpu: 37.968 seconds##模型0：预测与评估创建一个函数，接受一个训练过的模型，一个DataLoader，一个损失函数和一个精度函数，使用模型对DataLoader中的数据进行预测，然后使用损失函数和精度函数来评估这些预测。torch.manual_seed(42)defeval_model(model:torch.nn.Module,data_loader:torch.utils.data.DataLoader,loss_fn:torch.nn.Module,accuracy_fn,device:torch.device device):\"\"\"Evaluatesagivenmodelonagivendataset.Args:model(torch.nn.Module):APyTorchmodelcapableofmakingpredictionsondata_loader.data_loader(torch.utils.data.DataLoader):Thetargetdatasettopredicton.loss_fn(torch.nn.Module):Thelossfunctionofmodel.accuracy_fn:Anaccuracyfunctiontocomparethemodelspredictionstothetruthlabels.device(str,optional):Targetdevicetocomputeon.Defaultstodevice.Returns:(dict):Resultsofmodelmakingpredictionsondata_loader.\"\"\"loss,acc 0,0model.eval()withtorch.inference_mode():forX,yindata_loader:#SenddatatothetargetdeviceX,y X.to(device),y.to(device)y_pred model(X)loss+ loss_fn(y_pred,y)acc+ accuracy_fn(y_true y,y_pred y_pred.argmax(dim 1))#Scalelossandaccloss/ len(data_loader)acc/ len(data_loader)return{\"model_name\":model.__class__.__name__,#onlyworkswhenmodelwascreatedwithaclass\"model_loss\":loss.item(),\"model_acc\":acc}#Calculatemodel0resultsontestdatasetmodel_0_results eval_model(model model_0,data_loader test_dataloader,loss_fn loss_fn,accuracy_fn accuracy_fn,device str(next(model_0.parameters()).device))model_0_results##模型1：添加非线性classFashionMNISTModelV1(nn.Module):def__init__(self,input_shape:int,hidden_units:int,output_shape:int):super().__init__()self.model nn.Sequential(nn.Flatten(),nn.Linear(in_features input_shape,out_features hidden_units),nn.ReLU(),nn.Linear(in_features hidden_units,out_features output_shape),nn.ReLU())defforward(self,x:torch.Tensor):returnself.model(x)接着实例化：torch.manual_seed(42)model_1 FashionMNISTModelV1(input_shape 784,hidden_units 10,output_shape len(class_names)).to(device)next(model_1.parameters()).device再次设置损失函数和优化器：loss_fn nn.CrossEntropyLoss()optimizer torch.optim.SGD(params model_1.parameters(),lr 0.1)将训练过程封装成函数：deftrain_step(model:torch.nn.Module,data_loader:torch.utils.data.DataLoader,loss_fn:torch.nn.Module,optimizer:torch.optim.Optimizer,accuracy_fn,device:torch.device device):train_loss,train_acc 0,0model.to(device)forbatch,(X,y)inenumerate(data_loader):#SenddatatoGPUX,y X.to(device),y.to(device)y_pred model(X)loss loss_fn(y_pred,y)train_loss+ losstrain_acc+ accuracy_fn(y_true y,y_pred y_pred.argmax(dim 1))optimizer.zero_grad()loss.backward()optimizer.step()train_loss/ len(data_loader)train_acc/ len(data_loader)print(f\"训练loss:{train_loss:.5f}训练accuracy:{train_acc:.2f}%\")deftest_step(data_loader:torch.utils.data.DataLoader,model:torch.nn.Module,loss_fn:torch.nn.Module,accuracy_fn,device:torch.device device):test_loss,test_acc 0,0model.to(device)model.eval()withtorch.inference_mode():forX,yindata_loader:X,y X.to(device),y.to(device)test_pred model(X)test_loss+ loss_fn(test_pred,y)test_acc+ accuracy_fn(y_true y,y_pred test_pred.argmax(dim 1))test_loss/ len(data_loader)test_acc/ len(data_loader)print(f\"Testloss:{test_loss:.5f}Testaccuracy:{test_acc:.2f}%\\n\")然后调用函数：torch.manual_seed(42)train_time_start timer()epochs 3forepochinrange(epochs):print(f\"Epoch:{epoch}\\n \")train_step(data_loader train_dataloader,model model_1,loss_fn loss_fn,optimizer optimizer,accuracy_fn accuracy_fn)test_step(data_loader test_dataloader,model model_1,loss_fn loss_fn,accuracy_fn accuracy_fn)train_time_end timer()total_train_time_model_1 print_train_time(start train_time_start,end train_time_end,device device)Epoch: 0 训练 loss: 1.09199 训练 accuracy: 61.34% Test loss: 0.95636 Test accuracy: 65.00% Epoch: 1 训练 loss: 0.78101 训练 accuracy: 71.93% Test loss: 0.72227 Test accuracy: 73.91% Epoch: 2 训练 loss: 0.67027 训练 accuracy: 75.94% Test loss: 0.68500 Test accuracy: 75.02% cuda: 40.312 seconds>注意：CUDA与CPU的训练时间在很大程度上取决于您使用的CPU/GPU的质量。>问题：“我使用了GPU，但我的模型没有更快地训练，为什么会这样？\">>答：一个原因可能是因为你的数据集和模型都很小（就像此例子正在使用的数据集和模型一样），使用GPU的好处被传输数据所花费的时间所抵消。在将数据从CPU内存（默认）复制到GPU内存之间存在一个小瓶颈。因此，对于较小的模型和数据集，CPU实际上可能是计算的最佳位置。对于较大的数据集和模型，GPU可以提供的计算速度通常远远超过获取数据的成本。但是，这在很大程度上取决于您使用的硬件。来评估一下模型：torch.manual_seed(42)model_1_results eval_model(model model_1,data_loader test_dataloader,loss_fn loss_fn,accuracy_fn accuracy_fn,device device)model_0_results,model_1_results在这种情况下，看起来向模型添加非线性使它的性能比基线模型更差。从事物的外观来看，模型似乎对训练数据过度拟合。过度拟合意味着我们的模型很好地学习了训练数据，但这些模式并没有推广到测试数据。修复过拟合的两种主要方法包括： 使用较小或不同的模型（某些模型比其他模型更适合某些类型的数据）。 使用更大的数据集（数据越多，模型学习可推广模式的机会就越大）。##模型2：CNN卷积神经网络的典型结构：`输入层 >[卷积层 >激活层 >池化层] >输出层` 其中，`[卷积层 >激活层 >池化层]`的内容可以根据要求放大和重复多次。下表是一个很好的通用指南，可以指导使用哪种模型（尽管也有例外）。类型一般使用模型示例: :: :: :结构化数据（如表格、行、列数据）梯度提升模型，随机森林，XGBoost`sklearn.ensemble`，`XGBoostlibrary`非结构化数据（如图像、音频、语言）卷积神经网络，Transformers`torchvision.models`，`HuggingFaceTransformers`使用`torch.nn`中的`nn.Conv2d()`和`nn.MaxPool2d()`层：classFashionMNISTModelV2(nn.Module):\"\"\"ModelarchitecturecopyingTinyVGGfrom:https://poloclub.github.io/cnn explainer/\"\"\"def__init__(self,input_shape:int,hidden_units:int,output_shape:int):super().__init__()self.block_1 nn.Sequential(nn.Conv2d(in_channels input_shape,out_channels hidden_units,kernel_size 3,#卷积核大小stride 1,#defaultpadding 1),#\"valid\"（无填充），\"same\"（输出与输入具有相同的形状），int表示特定的数字nn.ReLU(),nn.Conv2d(in_channels hidden_units,out_channels hidden_units,kernel_size 3,stride 1,padding 1),nn.ReLU(),nn.MaxPool2d(kernel_size 2,stride 2)#默认stride与池化窗口大小一致)self.block_2 nn.Sequential(nn.Conv2d(hidden_units,hidden_units,3,padding 1),nn.ReLU(),nn.Conv2d(hidden_units,hidden_units,3,padding 1),nn.ReLU(),nn.MaxPool2d(2))self.classifier nn.Sequential(nn.Flatten(),#这个in_features形状是是因为网络的每一层都会压缩和改变我们输入数据的形状。#此例中28*28池化一次变成14*14，再池化变成7*7nn.Linear(in_features hidden_units*7*7,out_features output_shape))defforward(self,x:torch.Tensor):x self.block_1(x)x self.block_2(x)x self.classifier(x)returnxtorch.manual_seed(42)model_2 FashionMNISTModelV2(input_shape 1,hidden_units 10,output_shape len(class_names)).to(device)model_2*该笔记只讲代码，不讲原理。*设置损失函数与优化器：loss_fn nn.CrossEntropyLoss()optimizer torch.optim.SGD(params model_2.parameters(),lr 0.1)进行训练：torch.manual_seed(42)train_time_start_model_2 timer()epochs 3forepochinrange(epochs):print(f\"Epoch:{epoch}\\n \")train_step(data_loader train_dataloader,model model_2,loss_fn loss_fn,optimizer optimizer,accuracy_fn accuracy_fn,device device)test_step(data_loader test_dataloader,model model_2,loss_fn loss_fn,accuracy_fn accuracy_fn,device device)train_time_end_model_2 timer()total_train_time_model_2 print_train_time(start train_time_start_model_2,end train_time_end_model_2,device device)Epoch: 0 训练 loss: 0.59430 训练 accuracy: 78.44% Test loss: 0.40483 Test accuracy: 85.33% Epoch: 1 训练 loss: 0.36487 训练 accuracy: 86.81% Test loss: 0.35013 Test accuracy: 87.23% Epoch: 2 训练 loss: 0.32794 训练 accuracy: 88.12% Test loss: 0.31522 Test accuracy: 88.63% cuda: 52.375 seconds看起来效果甚好。model_2_results eval_model(model model_2,data_loader test_dataloader,loss_fn loss_fn,accuracy_fn accuracy_fn,device device)model_2_results##比较并评估模型这里训练了三个模型： `model_0`：具有两个`nn.Linear()`层的基线模型。 `model_1`：与基线模型相同，除了在`nn.Linear()`层之间有`nn.ReLU()`层。 `model_2`：CNN模型，模仿CNNExplainer网站上的TinyVGG架构。使用`pandas`的`DataFrame`将数据展示：importpandasaspdcompare_results pd.DataFrame([model_0_results,model_1_results,model_2_results])compare_results[\"training_time\"] [total_train_time_model_0,total_train_time_model_1,total_train_time_model_2]compare_results结论： CNN（FashionMNISTModelV2）模型表现最好（损失最低，准确率最高），但训练时间最长。 基线模型（FashionMNISTModelV0）的性能优于model_1（FashionMNISTModelV1）。比较可视化：plt.figure(figsize (4,4))compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind \"barh\")plt.xlabel(\"accuracy(%)\")plt.ylabel(\"model\")可以使用许多不同的评估指标来解决分类问题，其中最直观的是混淆矩阵。混淆矩阵显示了分类模型在预测和真实标签之间混淆的地方。制作混淆矩阵：1.使用训练模型进行预测（混淆矩阵将预测与真实标签进行比较）。2.使用`torchmetrics.ConfusionMatrix`制作混淆矩阵。3.使用`mlxtend.plotting.plot_confusion_matrix()`绘制混淆矩阵。#1.使用训练模型进行预测y_preds []model_2.eval()withtorch.inference_mode():forX,yintest_dataloader:X,y X.to(device),y.to(device)y_logit model_2(X)y_pred torch.softmax(y_logit,dim 1).argmax(dim 1)y_preds.append(y_pred.cpu())y_pred_tensor torch.cat(y_preds)下载并导入`torchmetrics`和`mlxtend`：fromtorchmetricsimportConfusionMatrixfrommlxtend.plottingimportplot_confusion_matrix#2.设置混淆矩阵实例并将预测与目标进行比较confmat ConfusionMatrix(num_classes len(class_names),task 'multiclass')confmat_tensor confmat(preds y_pred_tensor,target test_data.targets)#3.绘图fig,ax plot_confusion_matrix(conf_mat confmat_tensor.numpy(),#matplotlib就像NumPy一样class_names class_names,#将行和列标签转换为类名figsize (4,4))可以看到模型表现相当好，因为大多数黑色方块对角线（理想模型将只在这些方块中有值，其他地方都是0）。混淆矩阵的信息能够进一步检查模型和数据，看看如何改进。"},"/StyleTransfer/ref_and_notes/pytorch_custom_datasets.html":{"title":"自定义数据集","content":"Reference:[PyTorchCustomDatasets](https://www.learnpytorch.io/04_pytorch_custom_datasets/)*该页面由JupyterNotebook生成，原文件于[Github](https://github.com/Fingsinz/StyleTransfer/tree/main/src/02.pytorch_learning/pytorch_custom_datasets.ipynb)*#导入包和设置设备importtorchfromtorchimportnndevice \"cuda\"iftorch.cuda.is_available()else\"cpu\"torch.__version__,device##获取并处理数据###获取数据首先，需要一些数据。这里使用的数据是[Food101数据集](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food 101/)的一个子集。 Food101包含101种不同食物的1000张图像，总计101000张图像（75750张训练图像和25250张测试图像）。为了自定义数据集，选取将3种食物开始：披萨、牛排和寿司。同时每个类并不是1000个图像，而是从随机的10%开始（从小处开始，必要时增加）。可以以下步骤下载数据集： 原始Food101数据集和论文网站。 笔记本（https://www.learnpytorch.io）提供。importrequestsimportzipfilefrompathlibimportPath#Setuppathtodatafolderdata_path Path(\"data/\")image_path data_path/\"pizza_steak_sushi\"#Iftheimagefolderdoesn'texist,downloaditandprepareit...ifimage_path.is_dir():print(f\"{image_path}directoryexists.\")else:print(f\"Didnotfind{image_path}directory,creatingone...\")image_path.mkdir(parents True,exist_ok True)#Downloadpizza,steak,sushidatawithopen(data_path/\"pizza_steak_sushi.zip\",\"wb\")asf:request requests.get(\"https://github.com/mrdbourke/pytorch deep learning/raw/main/data/pizza_steak_sushi.zip\")print(\"Downloadingpizza,steak,sushidata...\")f.write(request.content)#Unzippizza,steak,sushidatawithzipfile.ZipFile(data_path/\"pizza_steak_sushi.zip\",\"r\")aszip_ref:print(\"Unzippingpizza,steak,sushidata...\")zip_ref.extractall(image_path)data\\pizza_steak_sushi directory exists.*也可以自行下载数据集，并划分为train和test。*在此例中，有标准图像分类格式的披萨、牛排和寿司图像。图像分类格式在单独的目录中包含单独的图像类，标题为特定的类名。例如，pizza的所有图像都包含在pizza/目录中。```pizza_steak_sushi/train/pizza/steak/sushi/test/pizza/steak/sushi/```目标是将这个数据存储结构转化为PyTorch可用的数据集。现在试着打开几张图片看看：1.使用`pathlib.Path.glob()`获取所有图像路径，以查找所有以`.jpg`结尾的文件。2.使用Python的`random.choice()`选择一个随机的图像路径。3.使用`pathlib.Path.parent.stem`获取图像类名。4.使用`PIL.image.open()`（PIL代表PythonimageLibrary）打开随机图像路径。5.显示图像并打印一些元数据。importrandomfromPILimportImagerandom.seed(42)image_path_list list(image_path.glob(\"*/*/*.jpg\"))random_image_path random.choice(image_path_list)image_class random_image_path.parent.stemimg Image.open(random_image_path)print(f\"Randomimagepath:{random_image_path}\")print(f\"Imageclass:{image_class}\")print(f\"Imageheight:{img.height}\")print(f\"Imagewidth:{img.width}\")imgRandom image path: data\\pizza_steak_sushi\\test\\sushi\\2394442.jpg Image class: sushi Image height: 408 Image width: 512同样可以使用`matplotlib`：importnumpyasnpimportmatplotlib.pyplotaspltimg_as_array np.asarray(img)plt.figure(figsize (5,5))plt.imshow(img_as_array)plt.title(f\"Imageclass:{image_class}Imageshape:{img_as_array.shape} >[height,width,color_channels]\")plt.axis(False);###转化数据集表示现在希望将图像数据加载到PyTorch中。在PyTorch中使用图像数据之前，需要：1.把它变成张量（图像的数值表示）。2.将其转换为`torch.utils.data.dataset`，随后再转换为`torch.utils.data.DataLoader`，简称它们为Dataset和DataLoader。PyTorch有几种不同类型的预构建数据集和数据集加载器，具体取决于处理的问题。 视觉类：`torchvision.datasets`； 音频类：`torchaudio.datasets`； 文本类：`torchtext.datasets`； 推荐系统：`torchrec.datasets`。#导入包importtorchfromtorch.utils.dataimportDataLoaderfromtorchvisionimportdatasets,transforms使用`torchvision.transforms`转化数据：1.使用`transform.Resize()`调整图像的大小。2.使用`transform.RandomHorizontalFlip()`在水平方向上随机翻转图像（这可以被认为是一种数据增强形式，因为它会人为地改变我们的图像数据）。3.使用`transform.ToTensor()`将图像从PIL图像转换为PyTorch张量。可以使用`torchvision.transforms.Compose()`编译所有这些步骤。data_transform transforms.Compose([transforms.Resize(size (64,64)),transforms.RandomHorizontalFlip(p 0.5),#p为翻转的概率transforms.ToTensor()])接下来试试转换的效果：defplot_transformed_images(image_paths,transform,n 3,seed 42):random.seed(seed)random_image_paths random.sample(image_paths,k n)forimage_pathinrandom_image_paths:withImage.open(image_path)asf:fig,ax plt.subplots(1,2)ax[0].imshow(f)ax[0].set_title(f\"Original\\nSize:{f.size}\")ax[0].axis(\"off\")#permute()会改变图像的形状以适应matplotlib#(PyTorchdefaultis[C,H,W]butMatplotlibis[H,W,C])transformed_image transform(f).permute(1,2,0)ax[1].imshow(transformed_image)ax[1].set_title(f\"Transformed\\nSize:{transformed_image.shape}\")ax[1].axis(\"off\")fig.suptitle(f\"Class:{image_path.parent.stem}\",fontsize 16)plot_transformed_images(image_path_list,transform data_transform,n 3)##使用ImageFolder加载数据目前数据是标准的图像分类格式，所以可以使用`torchvision.datasets.ImageFolder`类。将目标图像目录的文件路径以及我们想要对图像执行的一系列转换传递给它。fromtorchvisionimportdatasetstrain_dir image_path/\"train\"test_dir image_path/\"test\"train_data datasets.ImageFolder(root train_dir,transform data_transform,target_transform None)#转换在标签上执行test_data datasets.ImageFolder(root test_dir,transform data_transform)print(f\"Traindata:\\n{train_data}\\nTestdata:\\n{test_data}\")Train data: Dataset ImageFolder Number of datapoints: 225 Root location: data\\pizza_steak_sushi\\train StandardTransform Transform: Compose( Resize(size (64, 64), interpolation bilinear, max_size None, antialias True) RandomHorizontalFlip(p 0.5) ToTensor() ) Test data: Dataset ImageFolder Number of datapoints: 75 Root location: data\\pizza_steak_sushi\\test StandardTransform Transform: Compose( Resize(size (64, 64), interpolation bilinear, max_size None, antialias True) RandomHorizontalFlip(p 0.5) ToTensor() )现在PyTorch已经注册了数据集。通过检查`classes`和`class_to_idx`属性以及训练集和测试集的长度来检查一下：class_names train_data.classesclass_dict train_data.class_to_idxclass_names,class_dict,len(train_data),len(test_data)再检查一下训练数据和测试数据：img,label train_data[0][0],train_data[0][1]img.shape,img.dtype,label,type(label)图像现在是张量的形式（形状为`[3,64,64] >[通道,高度,宽度]`），标签是与特定类相关的整数形式（由class_to_idx属性引用）。还需要将数据转换为DataLoader。将Dataset转换为DataLoader，模型可以遍历并学习样本和目标（特征和标签）之间的关系。为了简单起见，将使用`batch_size 1`和`num_workers 1`。 `batch_size`已经解释过，批量大小。 `num_workers`定义将创建多少个子进程来加载数据，`num_workers`设置的值越高，PyTorch在加载数据时使用的计算能力就越强。通常通过Python的`os.cpu_count()`将其设置为CPU总数，确保DataLoader使用尽可能多的内核来加载数据。fromtorch.utils.dataimportDataLoadertrain_dataloader DataLoader(dataset train_data,batch_size 1,num_workers 1,shuffle True)test_dataloader DataLoader(dataset test_data,batch_size 1,num_workers 1,shuffle False)最后获取`train_dataloader`中每个可迭代项的Shape信息：img,label next(iter(train_dataloader))img.shape,label.shape##使用自定义DataSet类加载数据如果像`torchvision.datasets.ImageFolder()`这样的预构建数据集创建器不存在，或者针对具体问题的解决方案根本不存在，那么可以自定义一个。创建自定义方式来加载Dataset的优缺点： 优点：可以用几乎任何东西创建数据集，不限于PyTorch预构建的Dataset函数。 缺点：尽管可以用几乎任何东西创建一个数据集，但这并不意味着它就有效；同时会导致编写更多代码，这可能容易出现错误或性能问题。实际操作是继承`torch.utils.data.Dataset`（PyTorch中所有Dataset的基类）来复制`torchvision.datasets.ImageFolder()`。从导入需要的模块开始： Python处理目录的`os`（数据存储在目录中）。 Python处理文件路径的`pathlib`（每个图像都有一个唯一的文件路径）。 PyTorch的所有的东西。 用于加载图像的PIL的Image类。 继承`torch.utils.data.Dataset`创建自定义数据集。 `torchvision.transforms`把图像变成张量。 来自Python的typing模块的各种类型，为代码添加类型提示。importosimportpathlibimporttorchfromPILimportImagefromtorch.utils.dataimportDatasetfromtorchvisionimporttransformsfromtypingimportTuple,Dict,List###获取数据类名首先实现获取数据类名的函数，获取如`['pizza','steak','sushi'],{'pizza':0,'steak':1,'sushi':2}`的信息：deffind_classes(directory:str) >Tuple[List[str],Dict[str,int]]:\"\"\"Findstheclassfoldernamesinatargetdirectory.Assumestargetdirectoryisinstandardimageclassificationformat.Args:directory(str):targetdirectorytoloadclassnamesfrom.Returns:Tuple[List[str],Dict[str,int]]:(list_of_class_names,dict(class_name:idx...))Example:find_classes(\"food_images/train\")>>>([\"class_1\",\"class_2\"],{\"class_1\":0,...})\"\"\"#1.通过扫描目标目录获取类名classes sorted(entry.nameforentryinos.scandir(directory)ifentry.is_dir())#2.如果找不到类名，则引发错误ifnotclasses:raiseFileNotFoundError(f\"Couldn'tfindanyclassesin{directory}.\")#3.创建索引标签的字典class_to_idx {cls_name:ifori,cls_nameinenumerate(classes)}returnclasses,class_to_idx测试一下：find_classes(train_dir)###构建自定义数据集类将构建一个类来复刻`torchvision.datasets.ImageFolder()`的功能。分析如下：1.继承`torch.utils.data.Dataset`。2.用`targ_dir`参数（目标数据目录）和`transform`参数初始化子类。3.创建属性：目标图像路径、`transform`（可以是`None`），`classes`和`class_to_idx`（来自`find_classes()`函数）。4.创建一个函数从文件中加载图像并返回它们，可以使用`PIL`或`torchvision.io`。5.重写`torch.utils.data.Dataset`的`__len__`方法，返回数据集中的样本数量。（不必需）6.重写`torch.utils.data.Dataset`的`__getitem__`方法以返回数据集中的单个样本。（必需）fromtorch.utils.dataimportDatasetclassCustomImageFolder(Dataset):def__init__(self,targ_dir:str,transform None) >None:self.paths list(pathlib.Path(targ_dir).glob(\"*/*.jpg\"))self.transform transformself.classes,self.class_to_idx find_classes(targ_dir)defload_image(self,index:int) >Image.Image:image_path self.paths[index]returnImage.open(image_path)def__len__(self) >int:returnlen(self.paths)def__getitem__(self,index:int) >Tuple[torch.Tensor,int]:img self.load_image(index)class_name self.paths[index].parent.name#要求data_folder/class_name/image.jpegclass_idx self.class_to_idx[class_name]ifself.transform:returnself.transform(img),class_idx#(X,y)else:returnimg,class_idx#(X,y)重新设置数据转变器：train_transforms transforms.Compose([transforms.Resize((64,64)),transforms.RandomHorizontalFlip(p 0.5),transforms.ToTensor()])test_transforms transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor()])接着实例化数据：train_data_custom CustomImageFolder(targ_dir train_dir,transform train_transforms)test_data_custom CustomImageFolder(targ_dir test_dir,transform test_transforms)train_data_custom.classes,train_data_custom.class_to_idx,len(train_data_custom),len(test_data_custom)###测试`__getitem__`直接上函数：#1.TakeinaDatasetaswellasalistofclassnamesdefdisplay_random_images(dataset:torch.utils.data.dataset.Dataset,classes:List[str] None,n:int 10,display_shape:bool True,seed:int None):#2.Adjustdisplayifntoohighifn>10:n 10display_shape Falseprint(f\"Fordisplaypurposes,nshouldn'tbelargerthan10,settingto10andremovingshapedisplay.\")#3.Setrandomseedifseed:random.seed(seed)#4.Getrandomsampleindexesrandom_samples_idx random.sample(range(len(dataset)),k n)#5.Setupplotplt.figure(figsize (16,5))#6.Loopthroughsamplesanddisplayrandomsamplesfori,targ_sampleinenumerate(random_samples_idx):targ_image,targ_label dataset[targ_sample][0],dataset[targ_sample][1]#7.Adjustimagetensorshapeforplotting:[color_channels,height,width] >[color_channels,height,width]targ_image_adjust targ_image.permute(1,2,0)#Plotadjustedsamplesplt.subplot(1,n,i+1)plt.imshow(targ_image_adjust)plt.axis(\"off\")ifclasses:title f\"class:{classes[targ_label]}\"ifdisplay_shape:title title+f\"\\nshape:{targ_image_adjust.shape}\"plt.title(title)调用测试：#DisplayrandomimagesfromImageFoldercreatedDatasetdisplay_random_images(train_data,n 5,classes class_names,seed None)display_random_images(train_data_custom,n 5,classes class_names,seed None)#Trysettingtheseedforreproducibleimages看起来生效。###把自定义数据类变成DataLoader通过`CustomImageFolder`类，可以将原始图像转换为数据集（特征映射到标签或X映射到y）。因为自定义数据集的继承`torch.utils.data`，所以可以通过`torch.utils.data.DataLoader()`直接使用它们。fromtorch.utils.dataimportDataLoadertrain_dataloader_custom DataLoader(dataset train_data_custom,batch_size 1,num_workers 0,shuffle True)test_dataloader_custom DataLoader(dataset test_data_custom,batch_size 1,num_workers 0,shuffle False)最后获取`train_dataloader_custom`中每个可迭代项的Shape信息：img_custom,label_custom next(iter(train_dataloader_custom))img_custom.shape,label_custom.shape##其他形式的转换（数据增强）目前已经看到了对数据的一些变换，但还有更多，可以在[torchvision.transforms文档](https://pytorch.org/vision/stable/transforms.html)中查阅。变换的目的是以某种方式改变图像，如裁剪、随机删除部分、随即旋转等等。进行这类转换通常被称为数据增强。数据增强是通过人为地增加训练集的多样性来改变数据的过程。对图像执行数据增强的许多示例在：[https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html)研究表明，随机变换（如`transform.RandAugment()`和`transform.TrivialAugmentWide()`）通常比手工选择的变换表现得更好。在`transforms.TrivialAugmentWide()`中需要注意的主要参数是`num_magnitude_bins 31`。 它定义了将选择多少范围的强度值来应用某个转换，0表示没有范围，31表示最大范围（最高强度的最高机会）。将`transforms.TrivialAugmentWide()`合并到`transforms.Compose()`中：fromtorchvisionimporttransformstrain_transforms transforms.Compose([transforms.Resize((224,224)),transforms.TrivialAugmentWide(num_magnitude_bins 31),transforms.ToTensor()])test_transforms transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])看看效果：image_path_list list(image_path.glob(\"*/*/*.jpg\"))plot_transformed_images(image_paths image_path_list,transform train_transforms,n 3,seed None)##模型0：没有数据增强的TinyVGG定义transform：simple_transform transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor(),])加载数据：importosfromtorchvisionimportdatasetsfromtorch.utils.dataimportDataLoadertrain_data_simple datasets.ImageFolder(root train_dir,transform simple_transform)test_data_simple datasets.ImageFolder(root test_dir,transform simple_transform)BATCH_SIZE 32NUM_WORKERS 3#个人修改，不全部使用print(f\"batchsize:{BATCH_SIZE},workers:{NUM_WORKERS}\")train_dataloader_simple DataLoader(train_data_simple,batch_size BATCH_SIZE,shuffle True,num_workers NUM_WORKERS)test_dataloader_simple DataLoader(test_data_simple,batch_size BATCH_SIZE,shuffle False,num_workers NUM_WORKERS)len(train_dataloader_simple),len(test_dataloader_simple)batch size: 32, workers: 3直接创建模型，同视觉一节：classTinyVGG(nn.Module):\"\"\"ModelarchitecturecopyingTinyVGGfrom:https://poloclub.github.io/cnn explainer/\"\"\"def__init__(self,input_shape:int,hidden_units:int,output_shape:int) >None:super().__init__()self.conv_block_1 nn.Sequential(nn.Conv2d(input_shape,hidden_units,kernel_size 3,stride 1,padding 1),nn.ReLU(),nn.Conv2d(hidden_units,hidden_units,kernel_size 3,stride 1,padding 1),nn.ReLU(),nn.MaxPool2d(kernel_size 2,stride 2))self.conv_block_2 nn.Sequential(nn.Conv2d(hidden_units,hidden_units,kernel_size 3,padding 1),nn.ReLU(),nn.Conv2d(hidden_units,hidden_units,kernel_size 3,padding 1),nn.ReLU(),nn.MaxPool2d(2))self.classifier nn.Sequential(nn.Flatten(),nn.Linear(in_features hidden_units*16*16,out_features output_shape))defforward(self,x:torch.Tensor):returnself.classifier(self.conv_block_2(self.conv_block_1(x)))torch.manual_seed(42)model_0 TinyVGG(input_shape 3,#(3,RGB)hidden_units 10,output_shape len(train_data.classes)).to(device)model_0###使用`torchinfo`了解模型形状需要安装`torchinfo`库：`pipinstalltorchinfo`。使用：`summary(model,input_size (batch_size,model_shape))`fromtorchinfoimportsummarysummary(model_0,input_size [1,3,64,64])#对示例输入大小进行测试传递`torchinfo.summary()`的输出提供了关于模型的大量信息。 `Totalparams`是模型中参数的总数； `EstimatedTotalSize`是估计的总大小（MB）。还可以看到输入和输出形状的变化，因为特定`input_size`的数据在模型中移动。###封装每step的训练和测试函数deftrain_step(model:torch.nn.Module,dataloader:torch.utils.data.DataLoader,loss_fn:torch.nn.Module,optimizer:torch.optim.Optimizer):model.train()train_loss,train_acc 0,0forbatch,(X,y)inenumerate(dataloader):X,y X.to(device),y.to(device)y_pred model(X)loss loss_fn(y_pred,y)train_loss+ loss.item()optimizer.zero_grad()loss.backward()optimizer.step()y_pred_class torch.argmax(torch.softmax(y_pred,dim 1),dim 1)train_acc+ (y_pred_class y).sum().item()/len(y_pred)train_loss train_loss/len(dataloader)train_acc train_acc/len(dataloader)returntrain_loss,train_accdeftest_step(model:torch.nn.Module,dataloader:torch.utils.data.DataLoader,loss_fn:torch.nn.Module):model.eval()test_loss,test_acc 0,0withtorch.inference_mode():forbatch,(X,y)inenumerate(dataloader):X,y X.to(device),y.to(device)test_pred_logits model(X)loss loss_fn(test_pred_logits,y)test_loss+ loss.item()test_pred_labels test_pred_logits.argmax(dim 1)test_acc+ ((test_pred_labels y).sum().item()/len(test_pred_labels))test_loss test_loss/len(dataloader)test_acc test_acc/len(dataloader)returntest_loss,test_acc###封装训练函数deftrain(model:torch.nn.Module,train_dataloader:torch.utils.data.DataLoader,test_dataloader:torch.utils.data.DataLoader,optimizer:torch.optim.Optimizer,loss_fn:torch.nn.Module nn.CrossEntropyLoss(),epochs:int 5):results {\"train_loss\":[],\"train_acc\":[],\"test_loss\":[],\"test_acc\":[]}forepochinrange(epochs):train_loss,train_acc train_step(model model,dataloader train_dataloader,loss_fn loss_fn,optimizer optimizer)test_loss,test_acc test_step(model model,dataloader test_dataloader,loss_fn loss_fn)print(f\"Epoch:{epoch+1}\"f\"train_loss:{train_loss:.4f}\"f\"train_acc:{train_acc:.4f}\"f\"test_loss:{test_loss:.4f}\"f\"test_acc:{test_acc:.4f}\")results[\"train_loss\"].append(train_loss.item()ifisinstance(train_loss,torch.Tensor)elsetrain_loss)results[\"train_acc\"].append(train_acc.item()ifisinstance(train_acc,torch.Tensor)elsetrain_acc)results[\"test_loss\"].append(test_loss.item()ifisinstance(test_loss,torch.Tensor)elsetest_loss)results[\"test_acc\"].append(test_acc.item()ifisinstance(test_acc,torch.Tensor)elsetest_acc)returnresults###构建训练和测试循环torch.manual_seed(42)torch.cuda.manual_seed(42)NUM_EPOCHS 5model_0 TinyVGG(input_shape 3,#3,RGBhidden_units 10,output_shape len(train_data.classes)).to(device)loss_fn nn.CrossEntropyLoss()optimizer torch.optim.Adam(params model_0.parameters(),lr 0.001)fromtimeitimportdefault_timerastimerstart_time timer()model_0_results train(model model_0,train_dataloader train_dataloader_simple,test_dataloader test_dataloader_simple,optimizer optimizer,loss_fn loss_fn,epochs NUM_EPOCHS)end_time timer()print(f\"耗时:{end_time start_time:.3f}seconds\")Epoch: 1 train_loss: 1.1078 train_acc: 0.2578 test_loss: 1.1362 test_acc: 0.2604 Epoch: 2 train_loss: 1.0846 train_acc: 0.4258 test_loss: 1.1622 test_acc: 0.1979 Epoch: 3 train_loss: 1.1153 train_acc: 0.2930 test_loss: 1.1695 test_acc: 0.1979 Epoch: 4 train_loss: 1.0990 train_acc: 0.2891 test_loss: 1.1343 test_acc: 0.1979 Epoch: 5 train_loss: 1.0989 train_acc: 0.2930 test_loss: 1.1435 test_acc: 0.1979 耗时: 46.904 seconds效果很差，试试可视化损失，封装函数：defplot_loss_curves(results:Dict[str,List[float]]):\"\"\"Plotstrainingcurvesofaresultsdictionary.Args:results(dict):dictionarycontaininglistofvalues,e.g.{\"train_loss\":[...],\"train_acc\":[...],\"test_loss\":[...],\"test_acc\":[...]}\"\"\"#Getthelossvaluesoftheresultsdictionary(trainingandtest)loss results['train_loss']test_loss results['test_loss']#Gettheaccuracyvaluesoftheresultsdictionary(trainingandtest)accuracy results['train_acc']test_accuracy results['test_acc']#Figureouthowmanyepochstherewereepochs range(len(results['train_loss']))#Setupaplotplt.figure(figsize (10,3))#Plotlossplt.subplot(1,2,1)plt.plot(epochs,loss,label 'train_loss')plt.plot(epochs,test_loss,label 'test_loss')plt.title('Loss')plt.xlabel('Epochs')plt.legend()#Plotaccuracyplt.subplot(1,2,2)plt.plot(epochs,accuracy,label 'train_accuracy')plt.plot(epochs,test_accuracy,label 'test_accuracy')plt.title('Accuracy')plt.xlabel('Epochs')plt.legend();调用：plot_loss_curves(model_0_results)##探究损失函数查看训练和测试损失曲线是查看模型是否过拟合的好方法。 过拟合模型是在训练集上比在验证/测试集上表现更好，训练损失远低于测试损失。 当训练和测试损失没有想要的那么低时，这被认为是欠拟合。训练和测试损失曲线的理想位置是它们彼此紧密排列。###处理过拟合由于过拟合的主要问题是模型太好地拟合训练数据，防止过拟合的一种常见技术称为正则化。预防过拟合的操作： 使用更多数据：拥有更多的数据使模型有更多的机会学习样式，这些样式可能更容易推广到新的示例。 简化模型：如果当前模型已经过拟合训练数据，则模型可能过于复杂。这意味着它对数据的模式学习得太好，无法很好地推广到看不见的数据。简化模型的一种方法是减少它使用的层数或减少每层中隐藏单元的数量。 数据增强：人为地为数据添加了更多的多样性。如果模型能够学习增强数据中的模式，则模型可能能够更好地概括看不见的数据。 迁移学习：迁移学习涉及利用一个模型已经学会使用的模式（也称为预训练权重）作为您自己任务的基础。在此例子中，可以使用一个在各种图像上预训练的计算机视觉模型，然后稍微调整它，使其更专门用于食物图像。 使用dropout层：dropout层随机删除神经网络中隐藏层之间的连接，有效地简化了模型，也使剩余的连接更好。 使用衰减的学习率：在模型训练时慢慢降低学习率。越接近收敛，越希望权重更新越小。 使用早停：早期停止在模型训练开始过度拟合之前停止。例如，假设模型的损失在过去10（这个数字是任意的）个epoch中停止下降，可能希望在这里停止模型训练，并使用损失最低的模型权重（10epoch之前）。###处理欠拟合当模型拟合不足时，它被认为对训练集和测试集的预测能力较差。从本质上讲，欠拟合模型将无法将损失值降低到期望的水平。目前的损失曲线，认为TinyVGG模型model_0对数据拟合不足。处理欠拟合背后的主要思想是提高模型的预测能力。处理欠拟合的操作： 增加模型隐藏层或隐层神经元：如果模型拟合不足，可能没有足够的能力来学习所需的模式/权重/数据表示来进行预测。为模型添加更多预测能力的一种方法是增加这些层中隐藏层/单元的数量。 调整学习率：也许模型的学习率太高了。而且它试图在每个时期更新权重太多，从而无法学习任何东西。在这种情况下，可以降低学习率。 使用迁移学习：迁移学习能够防止过拟合和欠拟合。它涉及到使用以前工作模型中的模式，并根据当前问题进行调整。 训练更长时间：模型可能需要更多的时间来学习数据的表示。如果你在小型实验中发模型没有学习到任何东西，也许让它训练更多的epoch可能会带来更好的性能。 减少正则化：也许因为试图防止过度拟合导致模型是欠拟合的。##模型1：数据增强后的TinyVGG修改数据transform：train_transform_trivial_augment transforms.Compose([transforms.Resize((64,64)),transforms.TrivialAugmentWide(num_magnitude_bins 31),transforms.ToTensor()])test_transform transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor()])再次处理数据集：train_data_augmented datasets.ImageFolder(train_dir,transform train_transform_trivial_augment)test_data_simple datasets.ImageFolder(test_dir,transform test_transform)train_data_augmented,test_data_simple转成DataLoader：BATCH_SIZE 32NUM_WORKERS 3torch.manual_seed(42)train_dataloader_augmented DataLoader(train_data_augmented,batch_size BATCH_SIZE,shuffle True,num_workers NUM_WORKERS)test_dataloader_simple DataLoader(test_data_simple,batch_size BATCH_SIZE,shuffle False,num_workers NUM_WORKERS)重新实例化模型：torch.manual_seed(42)model_1 TinyVGG(input_shape 3,hidden_units 10,output_shape len(train_data_augmented.classes)).to(device)model_1开始训练：torch.manual_seed(42)torch.cuda.manual_seed(42)NUM_EPOCHS 5loss_fn nn.CrossEntropyLoss()optimizer torch.optim.Adam(params model_1.parameters(),lr 0.001)start_time timer()model_1_results train(model model_1,train_dataloader train_dataloader_augmented,test_dataloader test_dataloader_simple,optimizer optimizer,loss_fn loss_fn,epochs NUM_EPOCHS)end_time timer()print(f\"耗时:{end_time start_time:.3f}seconds\")Epoch: 1 train_loss: 1.1073 train_acc: 0.2500 test_loss: 1.1060 test_acc: 0.2604 Epoch: 2 train_loss: 1.0793 train_acc: 0.4258 test_loss: 1.1380 test_acc: 0.2604 Epoch: 3 train_loss: 1.0805 train_acc: 0.4258 test_loss: 1.1684 test_acc: 0.2604 Epoch: 4 train_loss: 1.1287 train_acc: 0.3047 test_loss: 1.1618 test_acc: 0.2604 Epoch: 5 train_loss: 1.0895 train_acc: 0.4258 test_loss: 1.1470 test_acc: 0.2604 耗时: 47.582 seconds看起来效果也不好，绘制损失趋势图：plot_loss_curves(model_1_results)##比较并评估模型使用`pandas`并绘图：importpandasaspdmodel_0_df pd.DataFrame(model_0_results)model_1_df pd.DataFrame(model_1_results)#Setupaplotplt.figure(figsize (15,8))#Getnumberofepochsepochs range(len(model_0_df))#Plottrainlossplt.subplot(2,2,1)plt.plot(epochs,model_0_df[\"train_loss\"],label \"Model0\")plt.plot(epochs,model_1_df[\"train_loss\"],label \"Model1\")plt.title(\"TrainLoss\")plt.xlabel(\"Epochs\")plt.legend()#Plottestlossplt.subplot(2,2,2)plt.plot(epochs,model_0_df[\"test_loss\"],label \"Model0\")plt.plot(epochs,model_1_df[\"test_loss\"],label \"Model1\")plt.title(\"TestLoss\")plt.xlabel(\"Epochs\")plt.legend()#Plottrainaccuracyplt.subplot(2,2,3)plt.plot(epochs,model_0_df[\"train_acc\"],label \"Model0\")plt.plot(epochs,model_1_df[\"train_acc\"],label \"Model1\")plt.title(\"TrainAccuracy\")plt.xlabel(\"Epochs\")plt.legend()#Plottestaccuracyplt.subplot(2,2,4)plt.plot(epochs,model_0_df[\"test_acc\"],label \"Model0\")plt.plot(epochs,model_1_df[\"test_acc\"],label \"Model1\")plt.title(\"TestAccuracy\")plt.xlabel(\"Epochs\")plt.legend();最后封装一个函数，使得可以外部输入图片路径，然后进行预测：importtorchvisiondefpred_and_plot_image(model:torch.nn.Module,image_path:str,class_names:List[str] None,transform None,device:torch.device device):\"\"\"Makesapredictiononatargetimageandplotstheimagewithitsprediction.\"\"\"#1.加载图像并将张量值转换为float32target_image torchvision.io.read_image(str(image_path)).type(torch.float32)#2.将图像像素值除以255，得到[0,1]之间的值target_image target_image/255.#3.作数据转换iftransform:target_image transform(target_image)#4.确保模型在目标设备上model.to(device)#5.打开模型评估模式model.eval()withtorch.inference_mode():#为图像添加额外的维度target_image target_image.unsqueeze(dim 0)#对具有额外维度的图像进行预测，并将其发送到目标设备target_image_pred model(target_image.to(device))#6.转换logits >预测概率target_image_pred_probs torch.softmax(target_image_pred,dim 1)#7.转换预测概率 >预测标签target_image_pred_label torch.argmax(target_image_pred_probs,dim 1)#8.将图像与预测和预测概率一起绘制plt.imshow(target_image.squeeze().permute(1,2,0))#确保它的大小适合matplotlibifclass_names:title f\"Pred:{class_names[target_image_pred_label.cpu()]}Prob:{target_image_pred_probs.max().cpu():.3f}\"else:title f\"Pred:{target_image_pred_label}Prob:{target_image_pred_probs.max().cpu():.3f}\"plt.title(title)plt.axis(False);test_img_path \"data/pizza_steak_sushi/test/pizza/1687143.jpg\"custom_image_transform transforms.Compose([transforms.Resize((64,64))])pred_and_plot_image(model model_1,image_path test_img_path,class_names class_names,transform custom_image_transform,device device)"}}
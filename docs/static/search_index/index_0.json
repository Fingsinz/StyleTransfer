{"/StyleTransfer/paper/abstract.html":{"title":"摘要","content":" title: 摘要 keywords: Abstract desc: 论文摘要部分 date: id: abstract class: heading_no_counter <style> p { font size: 20px; } .text { text indent: 2em; } </style> <div class \"text\"> 随着信息技术的发展，图像风格迁移技术在艺术创作、影视特效、文创设计等领域展现出重要价值。近年来出现了很多基于深度学习的风格迁移方法，它们相比传统方法提高了图像风格迁移的质量，但仍存在风格化不明显以及细节丢失或模糊的问题。 本文基于元学习理念和现有的MetaNet模型展开研究，旨在通过改进算法，提升图像风格迁移的效果与效率。改进方法包括优化特征提取、改良图像转换网络结构，以及引入注意力机制。在特征提取中，采用预训练的VGG 19替代VGG 16，增强对深层风格特征的捕捉能力；在图像转换网络中，改进下采样和上采样的操作确保输入输出图像尺寸一致，并引入实例归一化层以保留图像细节特征。在元学习器中，分别引入通道注意力、自注意力和Transformer 模块，验证对通道间依赖、长距离特征关联的建模能力的提升效果。 实验过程中采用MS COCO 2017测试集与WikiArt数据集的子集展开训练，并通过客观指标和主观的人工打分方式综合评估模型。实验纵向对比分析了多个超参数对迁移效果产生的影响，从而确定最优模型。横向方面，与AdaIN、MSG Net、StyleID等主流风格迁移算法展开对比。本文改进模型在风格相似度评估上优于其他对比模型，得分最高达0.733，并且在推演效率上具有优势，尤其在实时交互场景中表现突出，充分证实了本文方法的有效性。 </div> **关键词**：深度学习；风格迁移；元学习；注意力机制 # Abstract <div class \"text\"> With the development of information technology, image style transfer technology shows important value in the fields of art creation, film and television special effects, and cultural and creative design. In recent years, many deep learning based style transfer methods have appeared, which improve the quality of image style transfer compared with the traditional methods, but still have the problems of inconspicuous stylization and loss or blurring of details. In this paper, based on the concept of meta learning and the existing MetaNet model, we aim to improve the effectiveness and efficiency of image style transfer by improving the algorithm. The improvement methods include optimizing feature extraction, improving the image transformation network structure, and introducing the attention mechanism. In feature extraction, pre trained VGG 19 is used instead of VGG 16 to enhance the ability to capture deep style features; in the image conversion network, the operations of down sampling and up sampling are improved to ensure that the input and output image sizes are the same, and an instance normalization layer is introduced to preserve image detail features. In the meta learner, channel attention, self attention and Transformer modules are introduced respectively to verify the effect of improving the modeling ability for inter channel dependency and long range feature association. A subset of MS COCO 2017 test set and WikiArt dataset is used to start the training during the experiment, and the model is evaluated comprehensively by objective metrics and subjective manual scoring. The experiment vertically compares and analyzes the impact of multiple hyperparameters on the transfer effect to determine the optimal model. Horizontally, the model is compared with mainstream style transfer algorithms such as AdaIN, MSG Net, and StyleID. The improved model in this paper outperforms other comparative models in style similarity evaluation, with a score of up to 0.733, and has an advantage in deduction efficiency, especially in real time interaction scenarios, which fully confirms the effectiveness of this paper's method. </div> **Keywords**：Deep Learning; Style Transfer; Meta Learning; Attention Mechanism"},"/StyleTransfer/paper/conclusion.html":{"title":"总结与展望","content":" title: 总结与展望 keywords: Conclusion desc: 论文总结部分 date: 2025 06 21 id: conclusion class: heading_no_counter <style> .text { text indent: 2em; font size: 20px; } .note { text align: center; font size: 0.8em\" } </style> ## 5.1 研究总结 <div class \"text\"> 早期的图像风格迁移技术主要依赖手工设计特征与数学建模等传统方法实现。这类方法不仅需要研究人员针对不同场景设计复杂处理流程，而且存在开发周期长、人力成本高和迁移效果不佳的问题。在深度学习技术的推动下，基于神经网络的图像风格迁移算法取代了传统方法，能够提升处理效率，并且生成视觉效果更细腻、艺术表现力更强的风格化图像。基于此，本文对基于元学习的深度学习图像风格迁移方法展开研究。具体工作如下： (1)本文首先阐述了研究背景与意义。图像风格迁移技术能够完成在艺术创作、影视特效、文创设计等领域的任务。接着通过梳理传统方法的局限性和深度学习方法的发展脉络，明确了研究的必要性和应用价值。然后本文介绍了图像风格迁移相关的理论基础，为后续模型改进和实验设计提供了理论支撑。 (2)提出了一种基于改进的 MetaNet图像风格迁移算法。在元学习理论的基础上对MetaNet 进行改进，在特征提取方面采用深度更深的预训练VGG 19模型代替原来的预训练VGG 16模型，增强模型对复杂风格图像的纹理和色彩分布的表达能力。同时改进图像转换网络的结构，通过引入最大池化层代替原来的固定步幅卷积下采样，结合双线性插值动态恢复空间尺寸的上采样方法，并且使用实例归一化层增强模型的特征学习能力。此外，本文尝试在元学习器中引入基础通道注意力模块、增强通道注意力模块、自注意力模块和Transformer四种注意力模块，加强训练过程中深层图像空间的表达，减少内容特征和风格特征的丢失，提高图像风格迁移效果。 (3)使用COCO 2017测试数据集和WikiArt数据集子集进行模型训练。通过设计多组对比实验，从注意力机制类型、VGG版本、轮换批次、批次大小等不同的超参数出发进行纵向对比，也与AdaIN、MSG Net和StyleID等主流风格迁移算法进行横向对比。实验中采用SSIM结构相似性指数、PSNR峰值信噪比、风格Gram矩阵余弦相似度等客观指标，结合人工视觉评分，定量分析生成图像的内容保留度和风格匹配度。结果表明，改进后的算法在风格Gram矩阵余弦相似度指标上高于基线模型和其他对比算法，证明其对风格特征的捕捉能力更强；同时，在SSIM和PSNR指标上保持稳定，说明内容结构未因风格迁移而严重失真，综合性能优于部分现有方法，验证了改进策略的有效性和优越性。在人工视觉评分上，基于扩散模型的StyleID方法得分高于本文的改进方法及其他方法，图像风格迁移效果更出色。然而StyleID方法需要较长的推演时间，达到本研究改进方法所需时间的31倍。另外，StyleID方法还需对输入的内容图像和风格图像执行预计算操作，占用大量额外存储空间。与之相比，本研究方法无需预计算步骤，直接通过元学习器动态 生成转换网络参数，在保持高效推理速度的同时避免了额外存储开销，在实际应用中更具部署优势。 </div> ## 5.2 研究展望 <div class \"text\"> 本文以图像风格迁移作为研究领域，深入研究与分析基于深度学习的图像风格迁移方法，并提出基于MetaNet的改进算法。实验结果表明，相较于基线模型与主流方法，本文算法在风格迁移结果图像的质量与效果上均实现显著提升。但未来研究仍存在一些改进的方向： (1)图像风格迁移领域尚未形成统一且标准化的评价指标体系。作为人工智能领域的重要研究方向，图像风格迁移算法的科学性亟需一套严谨、可量化的评估标准予以支撑。然而，现阶段的效果评估主要依赖主观评分与传统客观质量评估指标。传统图像质量评估指标难以较好地衡量风格迁移任务中内容保留与风格重构的复杂特性；另外，主观的审美认知受个体影响较大，使得主观评价结果难以形成普适性结论。 (2)风格迁移效果与基于扩散模型的风格迁移相比仍有明显差距。基于扩散模型的风格迁移能够更细腻地实现风格与内容的融合，生成图像的视觉效果更具真实感和艺术感，而本文改进的风格迁移方法在色彩过渡、纹理细节的呈现以及风格与内容的自然融合度等方面均存在不足，距离基于扩散模型的风格迁移水平尚有一段距离。 (3)本研究缺乏图像风格迁移系统的实现和多模态融合。对算法改进但未落地成实际应用系统，后续研究可尝试构建图像风格迁移系统，整合文本描述、视频序列等多模态输入，以语义导向为核心优化风格迁移过程，进而满足多元场景的应用需求。 </div>"},"/StyleTransfer/paper/experiment.html":{"title":"实验与评估","content":" title: 实验与评估 keywords: Experiment desc: 论文实验评估部分 date: 2025 06 21 id: experiment class: heading_no_counter <style> .text { text indent: 2em; font size: 20px; } .note { text align: center; font size: 0.8em\" } table>caption { margin: 0 0 5px 0; } table>thead>tr>th { border top: 2px solid #000; border bottom: 1px solid #000; } table>tbody>tr:last child>td { border bottom: 2px solid #000; } table>thead>tr>th { padding: 6px 20px; text align: center; } table>tbody>tr>td { padding: 6px 20px; text align: center; } </style> ## 4.1 实验设置 ### 4.1.1 数据集介绍 <div class \"text\"> 本研究的实验中内容图像数据选用Microsoft COCO 2017数据集中的测试集，风格图像数据使用WikiArt数据集的子集。 Microsoft COCO 2017 是微软团队提供的一个用于进行图像识别的数据集，覆盖人、汽车、动物等多个物体类别和天空、草地、建筑等多种背景。COCO全称Common Objects in Context，实验中使用的 COCO 2017 test 数据集由 40670 张高度真实且场景复杂图像组成^[38]^。Microsoft COCO 2017 数据集多样的物体以及丰富的背景可以为图像风格迁移提供有力的内容图像支持。 WikiArt 数据集是一个非营利性的线上艺术博物馆项目。目前为止，数据集已经收录了3293位艺术家的169057件画作，包括61个流派。实验中使用的Wikiart数据集子集包括80000张图片，涉及抽象表现主义、巴罗克风格、流行艺术、现实主义、印象主义、文艺复兴盛期和晚期等20个流派。WikiArt数据集记录了许多艺术家的真实画作，为图像风格迁移提供了有效的风格图像支持。 </div> ### 4.1.2 实验环境配置 <div class \"text\"> 本文中的所有实验均依托超算互联网平台。在训练过程中，硬件层面CPU使用的是Hygon C86 7285 型号的 32 核处理器，浮点计算使用异构加速卡，加速卡类型为DCU，显存16GB；软件方面，配置DTK 25.04开发工具包、PyTorch 2.4.1深度学习框架及Python 3.10编程语言环境。 在整个训练过程中总共执行100次迭代。训练中采用余弦退火动态学习率调整策略，初始学习率设置为0.001，最小学习率设置为0.00001，可以保证训练的稳定性，提高模型的收敛效率。选择Adam优化器，能够提高了训练过程的稳定性和收敛速度。标准情况下，内容损失权重设置为1，风格损失权重设置为150，总变异损失权重设置为0.00001。 除了上述的迭代轮次、学习率、优化器、内容损失权重、风格损失权重之外，本文实验的超参数变量还有VGG模型的版本(16还是19)、批次大小、轮换风格图像的轮次、图像转换网络卷积层的通道基数base、是否引入注意力机制以及注意力机制的种类。 </div> ### 4.1.3 评估指标 <div class \"text\"> 实验的评估主要采用客观的SSIM指标、PSNR指标、风格特征Gram矩阵余弦相似度、主观的人工视觉判别意见得分以及模型推演速度作为图像风格迁移生成图像质量和效率的评价标准。前两者SSIM指标和PSNR指标用于评估风格迁移在内容上的性能，风格特征Gram矩阵余弦相似度和主观人工评分则用于评估风格迁移在风格上的性能。 SSIM(Structural Similarity)指的是结构相似性指数，用于衡量两幅图像在亮度(均值)、色彩对比度(方差)与结构(协方差)三个维度上的相似性，取值范围为 1到1，值越接近1表示两幅图像越相似。对于亮度的计算如式4.1，色彩对比度的计算如式4.2，结构相似计算如式4.3，其中，$I(x,y)$ 表示图像的亮度特征，$c(x,y)$ 表示图像的色彩对比度特征，$s(x,y)$ 表示图像的结构特征，$\\mu_x$ 和 $\\mu_y$ 分别表示图像x和y的均值，$\\sigma_x$ 和 $\\sigma_y$ 分别表示图像x和y的方差，$\\sigma_{xy}$ 表示图像x和y的协方差，$C_1$、$C_2$ 和 $C_3$ 都为常数。 </div> $$ I(x,y) \\frac {2 \\mu_x \\mu_y + C_1} {\\mu_x^2 + \\mu_y^2 + C_1}\\tag{4.1} $$ $$ c(x,y) \\frac {2 \\sigma_{xy} + C_2}{\\sigma_x^2 + \\sigma_y^2 + C_2}\\tag{4.2} $$ $$ s(x,y) \\frac {\\sigma_{xy} + C_3}{\\sigma_x \\sigma_y + C_3}\\tag{4.3} $$ <div class \"text\"> PSNR(Peak Signal to Noise Ratio)指的是峰值信噪比，通过量化生成图像与原图像的像素级差异从而评估生成质量。PSNR的数学定义如公式4.4所示，其中 $I$ 为内容图像，$J$ 为迁移图像，$MAX_I$ 表示图像像素的最大可能值，$MSE$ 表示均方误差。PSNR的值越高，说明像素差异越小，图像的结构则越接近原内容图像。PSNR能够有效捕捉迁移后图像与内容图像在轮廓、物体布局等低频结构上的一致性。例如，当风格迁移模型过度扭曲内容结构时，PSNR会显著下降，从而在内容保留方面提供客观量化依据。 </div> $$ PSNR 10\\log_{10}\\frac{MAX_I^2}{MSE}\\tag{4.4} $$ $$ MSE \\frac {1}{mn}\\sum_{i 1}^{m 1}\\sum_{j 1}^{n 1}(I(i,j) J(i,j))^2\\tag{4.5} $$ <div class \"text\"> 风格Gram矩阵余弦相似度(Style Gram Matrix Cosine Similarity)是一种评估两幅图像风格相似性的指标，本文中简称为SGMCS，下同。该指标的计算流程如下。首先，通过预训练的VGG 19卷积神经网络模型提取风格图像和迁移后图像的某些层输出作为风格特征。接着对提取到的风格特征计算Gram矩阵。Gram矩阵的每个元素表示特征图中不同通道之间的内积，可以反映特征通道之间的相关性，能够有效表示图像的风格信息。然后将Gram矩阵其展平为一维向量，计算风格迁移后图像跟原风格图像的一维向量的余弦相似度。SGMCS的计算表达式如公式4.6，其中Gram的计算如公式4.7，$\\epsilon$ 为极小常数，此处取值为避免分母为0。Gram计算公式中输入为 $I\\in\\mathbb{R}^{C\\times H\\times W}$，重塑后 $I'\\in\\mathbb{R}^{C\\times(H\\times W)}$。SGMCS的计算结果范围在[ 1,1]之间，值越接近1，表示两幅图像的风格相似度越高；值越接近 1，表示风格越是相反；值为0则表示两者风格完全不相关。 </div> $$ \\text{SGMCS}(I_s, I_t) \\frac {Gram(I_s)\\cdot Gram(I_t)}{\\Vert Gram(I_s)\\Vert \\Vert Gram(I_t)\\Vert + \\epsilon}\\tag{4.6} $$ $$ Gram(I) I' \\cdot I'^T\\tag{4.7} $$ <div class \"text\"> 主观的人工视觉判别得分是由人类主观的评价得到。实验中选取若干个风格图像样本和内容图像样本进行图像风格迁移，然后评分员对风格迁移后的图像进行评分。分值范围从0到10分，其中0分表示质量最差，表示可能出现严重的语义扭曲、风格特征完全缺失或存在显著伪影；10分表示质量最好，表示迁移效果理想。在得到一系列评分数据后，人为剔除异常评分以保证数据有效性。最终评估指标通过计算有效评分的算术平均值获得，即某风格迁移方法的最终得分为所有评分员对该方法生成图像评分的均值。 模型推演速度SPI(Second Per Image)指的是单张图像从读取、处理到输出所需要的时间，可以衡量模型从输入内容图像、风格图像到输出风格迁移图像的效率。SPI的计算过程如公式4.7。模型推演速度受模型结构的复杂度、硬件环境、软件环境等因素影响，仅供参考。在实验设计中加入模型推演速度的评估，将模型聚焦于实时交互场景和边缘设备部署等方面，保证风格迁移效果的同时降低图像风格迁移所占用的时间。本章中模型推演速度的测试环境配置如下：处理器为16 vCPU的Intel (R) Xeon (R) Platinum 8474C，显卡采用RTX 4090D(显存24GB)，内存容量80GB，操作系统为Ubuntu22.04，软件环境为Python3.12、PyTorch2.5.1及CUDA12.4。 </div> ## 4.2 实验流程 <div class \"text\"> 本文实验包括网络的训练、网络参数调整以及与其他图像风格迁移算法的比较分析。 本文元网络训练流程如图4.1所示。训练开始时初始化VGG模型、图像转换网络和元学习器。在训练过程中，模型按照设定的轮次进行循环训练。在每一轮中，内容图像数据集被划分为指定批次大小的若干个小批量输入到网络中进行训练。每间隔固定的批次数，随机选取一张新的风格图像提取风格特征，用于训练学习当前阶段风格的迁移。在每个批次中，模型对该批次的内容图像进行风格迁移，并与原内容图像和原风格图像计算内容损失、风格损失，以及与自身计算总变分损失。随后，模型根据计算的损失利用优化器优化模型参数，从而降低模型损失，提升图像风格迁移的效果。 </div> ![图4.1训练流程](../static/images/Paper/TrainFlow.svg) <p class \"note\">图4.1训练流程</p> ## 4.3 纵向对比 <div class \"text\"> 纵向对比主要针对网络自身的不同参数取值所产生的效果进行对比。本节对于MetaNet的实验固定迭代轮次为100，固定使用余弦退火动态调整学习率和Adam优化器，其余参数如风格损失权重、VGG模型的版本(16还是19)、批次大小、轮换风格图像的轮次、图像转换网络卷积层的通道基数base、是否引入注意力机制以及注意力机制的种类都作为变量进行实现对比分析结果。实验使用10张风格图像和500张内容图像，共生成5000张迁移图像。在此基础上计算结构相似性指数、峰值信噪比指数、Gram特征余弦相似度和FID指数。 其中基线模型的参数为：无注意力机制，特征提取网络选用VGG 16模型，风格损失权重设置为50，内容损失权重设置为1，总变分损失权重为1e 6，批次大小设定为8，每20批次内容图像轮换一次风格图像，图像转换网络初始通道基数base设置为8。 </div> ### 4.3.1 风格损失权重影响实验 <div class \"text\"> 针对风格损失权重进行对比实验，分别取值为50、100、150、200。实验中，添加基础通道注意力模块，图像转换网络初始通道基数base设置为32，其余参数同基线模型的参数。各个模型训练时长和所占资源相近，实验指标情况如表4 1所示，其中模型名字简称为风格损失权重的取值。 从表4 1可知，随着风格损失权重逐步提升，SSIM指标从初始的0.441下降至0.338，表明原内容图像与风格迁移后的图像在结构上的相似度越来越低。SSIM指标的下降可能是因为风格损失权重的增加，使得风格在迁移中的占比越来越大，内容细节在迁移过程中可能被忽视。而PSNR指标数值稳定地维持在27.89至27.94区间内，波动幅度较小，图像的整体保真度并未受到显著影响。风格Gram矩阵余弦相似度同样随着风格损失权重的升高而升高，与预期的理论表现高度契合。推演速度无明显差异，说明风格损失权重的调整对计算效率影响较小。 </div> <p class \"note\">表4 1风格损失权重实验结果</p> 模型 SSIM PSNR SGMCS SPI 人工评分 : :: :: :: :: :: : 50 <u>0.441457</u> 27.919930 0.686667 0.155095 4.1 100 0.366497 <u>27.939641</u> 0.697833 0.149371 6.6 150 0.362293 27.889932 0.696696 <u>0.148403</u> 6.5 200 0.337894 27.901066 <u>0.701731</u> 0.152467 <u>7.3</u> ![图4.2风格损失权重不同取值效果对比](../static/images/Paper/StyleWeight.webp) <p class \"note\">图4.2风格损失权重不同取值效果对比</p> ### 4.3.2 注意力机制的种类影响实验 <div class \"text\"> 针对注意力机制模块的种类进行对比实验，分别设置为基础通道注意力模块、增强通道注意力模块、自注意力机模块和Transformer模块。实验中，无注意力机制的模型为基线模型，其余的参数同基线模型的参数。实验指标情况如表4 2，其中模型名字简称为注意力机制的种类。 从表4 2可知，在SSIM指标和PSNR指标上，添加基础通道注意力模块的模型的SSIM指标相比基线模型提升了3.96%，PSNR指标提升了0.21%，表明基础通道注意力模块能够有效捕捉并还原图像中的细节；而添加增强通道注意力的模型的SSIM指标增幅比添加基础通道注意力的模型低，内容相似度有所下降。添加Transformer的模型PSNR指标为所有模型最高，但SSIM指标略高于基线，表明Transformer提升了图像整体亮度和对比度的稳定性，但在局部细节的处理上未能显著优于其他注意力模型。在风格Gram矩阵余弦相似度上，添加增强通道注意力的模型高于其他所有模型，可能是因为增强了通道注意力的强度，使得模型更注重风格的特征，从而提升了风格的匹配度。添加Transformer的模型在该项上得分在所有模型中最低，表明该模型更侧重于图像内容而非风格纹理，风格相似度显著降低。添加自注意力的模型整体表现不佳。究其原因，可能是自注意力机制在建模长距离依赖时，可能因计算参数冗余或参数解析能力不足，反而降低了图像局部结构的还原效果，需进一步优化。综合SSIM指标和风格Gram矩阵余弦相似度指标来看，添加各种注意力机制能够在风格迁移效果大致不变的情况下，有效保留原内容图像中的内容特征。 </div> <p class \"note\">表4 2注意力机制实验结果</p> 模型 SSIM PSNR SGMCS SPI 人工评分 : :: :: :: :: :: : 无注意力(基线模型) 0.408205 27.879501 0.610139 <u>0.148580</u> 5.3 基础通道注意力 <u>0.424355</u> 27.938609 0.609673 0.154184 5.7 增强通道注意力 0.411272 27.961223 <u>0.610794</u> 0.158146 <u>7.1</u> 自注意力 0.391212 27.889798 0.605021 0.154176 6.4 Transformer 0.410037 <u>27.964593</u> 0.593184 0.158225 4.1 ### 4.3.3 VGG模型影响实验 <div class \"text\"> 针对VGG模型的不同选择，分别使用VGG 16和VGG 19进行实验，比较图像风格迁移效果。实验中添加了基础通道注意力模块，风格损失权重设置为100，图像转换网络通道基数base设置为32，其余参数同基线模型的参数。 实验指标情况如表4 3所示。基于预训练的VGG 19模型的SSIM指标略高于基于预训练VGG 16的模型。但是两者指标的数值相差不大，可以认为在保留图像结构细节上的能力接近。另外，这两个模型的PSNR指标相近，可以认为VGG模型并不影响峰值信噪比。在风格Gram矩阵余弦相似度指标上，基于预训练的VGG 19模型的高于基于预训练VGG 16的模型，说明预训练的VGG 19模型在提取风格特征上更具有优势，可能是因为VGG 19具有更深的网络结构，能够提取更深层的风格特征。 </div> <p class \"note\">表4 3VGG模型实验结果</p> 模型 SSIM PSNR SGMCS SPI 人工评分 : :: :: :: :: :: : VGG 16 0.366508 <u>27.939701</u> 0.643284 0.154881 6.4 VGG 19 <u>0.367502</u> 27.939578 <u>0.653839</u> <u>0.153120</u> 6.7</u> ![图4.3不同VGG模型效果对比](../static/images/Paper/VGG.webp) <p class \"note\">图4.3不同VGG模型效果对比</p> ### 4.3.4 图像转换网络通道基数影响实验 <div class \"text\"> 针对图像转换网络通道基数base进行对比实验，分成两组进行实验，表4 5中模型名字为批次大小以及组别，如“8(1)”表示第一组中图像转换网络通道基数base取值为8的情况。第一组无特别参数设置，“8(1)”即基线模型；第二组添加基础通道注意力模块，其余参数同基线模型。 图像转换网络通道基数base直接影响图像转换网络的结构复杂度，也间接影响了元学习器的模型参数量。参数量影响模型的大小。表4 4给出了base与图像转换网络和元学习器之间参数量的关系。当base为32时，元学习器的参数量是base为8时参数量的13倍，图像转换网络的参数量是base为8时参数量的15倍。 实验指标情况如表4 5所示。各个模型的PSNR指标基本保持在较小的区间内浮动。在SSIM指标和风格Gram矩阵余弦相似度上，无论是第一组还是第二组，图像转换网络通道基数base为32的模型得分都高于base为8的模型，说明通道基数base的增加能够有效提高模型保留原内容图像中内容信息的能力以及迁移原风格图像中风格纹理特征的能力。但从推演速度来看，由于base的增加，导致模型复杂度增加，推理运算时长增加。总而言之，增加通道数可能增强模型对图像中复杂结构和特征的捕获能力，进而提升内容保留和风格迁移效果，但同时需要承担推理速度下降的代价，在实际应用中需根据对模型性能和速度的具体需求进行权衡。 虽然base为32的模型的性能较强，但是从模型参数量和模型推演速度来看，应该结合计算资源、模型训练效率及任务性能需求综合权衡，避免因过度追求通道基数导致参数量爆炸而引发的优化困难或部署成本上升。 </div> <p class \"note\">表4 4通道基数影响网络参数量</p> 图像转换网络通道基数 图像转换网络参数量 元学习器参数量 : :: :: : 8 107,971 16,867,720 32 1,676,035 220,797,600 <p class \"note\">表4 5图像转换网络通道基数实验结果</p> 模型 SSIM PSNR SGMCS SPI 人工评分 : :: :: :: :: :: : 8(1) \t0.408205 27.879501 0.610139\t <u>0.148580</u>\t 5.9 32(1) \t<u>0.434521</u> <u>27.904213</u> <u>0.630926</u>\t 0.159106\t <u>6.4</u> 8(2) \t0.424355 <u>27.938609</u> 0.609673\t <u>0.155636</u>\t <u>6.1</u> 32(2) \t<u>0.441471</u> 27.919928 <u>0.627995</u>\t 0.162235\t 5.7 ![图4.4不同base效果对比](../static/images/Paper/base.webp) <p class \"note\">图4.4不同base效果对比</p> ### 4.3.5 轮换批次影响实验 <div class \"text\"> 轮换批次指的是训练中每隔多少批次更换一次风格图像进行学习。针对训练过程中的轮换批次进行对比实验，分别取值为10和20。实验中，添加基础通道注意力模块，风格损失权重设置为150，图像转换网络通道基数base设置为32，其余参数同基线模型的参数。实验指标情况如表4 6所示，其中模型名字简称为轮换批次的取值。 由表4 6可知，每10批次轮换一次风格图像比每20批次轮换一次风格图像在风格Gram矩阵余弦相似度上得分高2.9%。这在数据集总样本数不变的情况下，提高风格图像的轮换频率能够使得模型学习到更多种类的风格样本，从而提高模型的迁移泛化能力。而随着风格迁移的效果的提升，SSIM指标会相应降低，对原内容图像结构细节的保留能力下降。此外，实验中两者的SPI指标没有显著变化，说明风格图像轮换频率的调整仅影响模型训练过程中对风格特征的学习策略，并未改变模型本身的网络结构，因此不影响模型的原来的复杂度。 </div> <p class \"note\">表4 6轮换批次实验结果</p> 模型 SSIM PSNR SGMCS SPI 人工评分 : :: :: :: :: :: : 10 0.353660\t <u>27.893030</u>\t <u>0.660653</u> 0.158067 <u>7.8</u> 20 <u>0.362311</u>\t 27.890001\t 0.641875 <u>0.156689</u> 6.5 ![图4.5不同轮换批次效果对比](../static/images/Paper/interval.webp) <p class \"note\">图4.5不同轮换批次效果对比</p> ### 4.3.6 批次大小影响实验 <div class \"text\"> 批次大小指的是在一次梯度更新中所使用的样本数量。针对批次大小进行对比实验，分成两组取值，表4 7和表4 8中模型名字为批次大小以及组别，如“4(1)”表示第一组中批次大小取值为4的情况。第一组基于VGG 19预训练模型，添加Transformer模块，风格损失权重设置为150；第二组添加基础通道注意力模块，风格损失权重设置为100，其余参数同基线模型。 批次大小参数直接影响训练的过程，在总样本数一致的情况下，更大的批次大小意味着更新梯度频次更少，所以还需考虑训练所需时长。训练时长基于本章4.1小节中实验设置的环境下测量，由表4 7可知批次大小越小，训练时长越长。在第一组中，批次大小为4的模型训练时长甚至比批次大小为8的模型长了11小时。 实验指标情况如表4 8所示。对于SSIM指标，第一组的批次大小从4扩大到8时，指标提升3.9%；第二组的批次大小从8扩大到16时，指标提升了5.2%。这说明更大的批次大小使得模型处理了更多的内容样本，模型能够花费更多精力地学习内容图像的结构特征，从而更好地保留原内容图像的细节信息。但是对于风格Gram矩阵余弦相似度而言，第一组的指标下降了1%，第二组的指标下降了0.8%。这说明批次大小增加会导致模型在风格特征学习上出现一定程度的弱化。这种情况可能是因为在数据集总样本数不变的情况下，批次大小越大，每一轮所迭代的次数越少，能够轮换学习的新风格图像越少，模型对风格特征的学习不够充分。综合可得，增大批次大小能够增强模型对内容结构的学习能力，但因减少迭代次数和风格样本的接触频率，导致风格特征学习的效果下降。 </div> <p class \"note\">表4 7批次大小与训练时长</p> 模型 训练时长 : :: : 4(1) 2天3小时18分钟 8(1) 1天16小时19分钟 8(2) 1天11小时56分钟 16(2) 1天9小时26分钟 <p class \"note\">表4 8批次大小实验结果</p> 模型 SSIM PSNR SGMCS SPI 人工评分 : :: :: :: :: :: : 4(1) 0.343987\t <u>27.881804</u>\t <u>0.681195</u> <u>0.158763</u> <u>8.0</u> 8(1) <u>0.357377</u> 27.879289\t 0.674350 0.161896 7.7 8(2) 0.366508\t 27.939701\t <u>0.643284</u> <u>0.156573</u> <u>6.6</u> 16(2) <u>0.385595</u> <u>27.999234</u> 0.638068\t 0.158555 5.2 ## 4.4 横向对比 <div class \"text\"> 本小节对多个模型展开横向对比分析，进行对比的模型包括基线模型、本文改进模型、AdaIN方法、MSG Net方法以及StyleID方法[39]。 基准模型被定义为未经任何特定参数调整的原始模型，基于Shen的思想实现。经过重复实验后，用于横向对比的本文改进模型的参数设置为：添加Transformer模块，特征提取网络选用VGG 19模型，风格损失权重设置为200，内容损失权重设置为1，总变分损失权重设置为10<sub> 6</sub>，批次大小设定为4，每10批次内容图像轮换一次风格图像，图像转换网络初始通道基数base设置为32。 AdaIN(Adaptive Instance Normalization)方法意为自适应实例归一化，通过将内容图像的特征进行实例归一化处理后，再用风格图像的均值和方差替换内容特征的对应统计量。 MSG Net(Multi style Generative Network)是一种多风格生成网络，通过Siamese网络提取多尺度Gram矩阵，再使用CoMatch层匹配风格Gram矩阵，实现风格迁移。 StyleID(Style Injection in Diffusion)是一种基于扩散模型的方法，不进行训练，直接微调扩散模型进行快速风格迁移。 </div> <p class \"note\">表4 9横向对比评估指标</p> 模型 SSIM PSNR SGMCS SPI 人工评分 : :: :: :: :: :: : 基线模型 <u>0.408205</u> 27.879501 0.610139 <u>0.148580</u> 5.3 本文改进方法 0.330672 27.886268 <u>0.733233</u> 0.171477 8.4 AdaIN\t 0.102123 27.901968 0.720675 0.065824 8.3 MSG Net 0.065857 27.889683 0.629356 0.383283 8.0 StyleID 0.144267 <u>27.911183</u> 0.674784 7.259386 <u>9.3</u> <div class \"text\"> 各模型间的对比结果如表4 9所示。在SSIM指标上，基线模型得分最高，说明它在保留原始内容方面的卓越表现。另一方面，AdaIN、MSG Net、StyleID等方法的得分均低于0.15，这可能是因为风格迁移过程中风格对图像的亮度、对比度产生了明显的变化，与原内容图像的亮度、对比度相差甚远，从而导致SSIM指标得分偏低。同时也存在另一种可能性，即SSIM指标在图像风格迁移任务中存在评估局限性，无法全面精准地衡量风格迁移后的图像质量。在PSNR指标上，所有模型的得分接近，说明各个模型在降噪能力层面的表现差异并不显著，处于同一水平区间。在风格Gram矩阵余弦相似度上，本文提出的改进方法得分最高，说明该模型迁移后的风格与原风格相似度最大，学习到的风格图像特征更贴切全面。其他模型的得分不理想，但是结合表4 9的人工评分和图4.6的结果分析，可能是因为该指标的特征计算方式可能存在片面性，未能合理评估风格迁移任务中的效果，导致一些风格迁移效果良好的模型在该指标中未能得到理想分数。在推演速度上，AdaIN模型速度最快；本文改进方法在提升风格迁移效果的同时也保持可观的推演效率；而StyleID模型的推演速度最慢，说明基于扩散模型的图像风格迁移方法在计算过程中较为复杂。另外StyleID需要大量的预计算空间，如一个由10张内容图像和12张风格图像组成近4MB空间大小的图像风格迁移任务需要70GB左右的存储空间进行存储预计算量，且在推演过程中显存占用高达20GB。这进一步说明基于扩散模型的图像风格迁移方法需要更强大的硬件存储和计算资源。 本文改进方法模型的更多结果如图4.7和图4.8所示。迁移所使用的内容图像为校内建筑景色，风格则选取经典的艺术画作。通过本文的改进风格迁移模型，将校园景色分别呈现出不同风格。总体来看，风格迁移技术通过改变色彩、笔触、纹理等元素，使同一内容图像展现出从抽象到写实、古典到现代等多样艺术风格，具备丰富的视觉表现力与艺术模仿能力。 </div> ![图4.6各模型间迁移对比](../static/images/Paper/HorizontalComp.webp) <p class \"note\">图4.6各模型间迁移对比</p> ![图4.7本研究改进模型迁移效果1](../static/images/Paper/Mine1.webp) <p class \"note\">图4.7本研究改进模型迁移效果1</p> ![图4.8本研究改进模型迁移效果2](../static/images/Paper/Mine2.webp) <p class \"note\">图4.8本研究改进模型迁移效果2</p> ## 4.5 本章小结 <div class \"text\"> 本章介绍了本文改进模型的训练环境、训练流程，并展开评估对比分析。 实验依托超算互联网平台，使用Hygon处理器和DCU加速卡，基于PyTorch框架实现算法。训练采用Microsoft COCO 2017 test数据集作为内容图像，WikiArt数据集子集作为风格图像。在评估环节中，采用SSIM指标和PSNR指标衡量风格迁移前后图像内容的相似性，采用风格Gram矩阵余弦相似度评估风格的相似度，采用推演速度评估模型效率，同时引入人工主观评分对图像质量进行全面评估。 在模型评估对比部分，将风格损失权重、注意力机制、VGG模型版本、图像转换网络通道基数、轮换批次和批次大小作为变量进行实验，分别评估每个变量对风格迁移效果的影响。最后，与AdaIN、MSG Net、StyleID等方法进行横向对比，展示本文改进模型在风格迁移效果上的优越性。 本章通过系统性实验验证了模型设计的有效性，揭示了关键超参数对风格迁移效果的影响效果，为模型优化提供了数据支撑。未来可进一步探索轻量化网络结构、动态调整损失权重策略，或结合更复杂注意力机制，以在边缘设备部署、实时交互等场景中实现更优性能。 </div> [38] Lin T Y, Maire M, Belongie S, et al. Microsoft COCO Common Objects in Context[M/OL]//Computer Vision – ECCV 2014,Lecture Notes in Computer Science. 2014 740 755. [39] Chung J, Hyun S, Heo J P. Style Injection in Diffusion: A Training free Approach for Adapting Large scale Diffusion Models for Style Transfer[C]// roceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024 8795 8805."},"/StyleTransfer/paper/introduction.html":{"title":"绪论","content":" title: 绪论 keywords: Introduction desc: 论文绪论部分 date: 2025 06 21 id: introduction class: heading_no_counter <style> .text { text indent: 2em; font size: 20px; } .note { text align: center; font size: 0.8em\" } </style> ## 1.1 课题研究背景及意义 <div class \"text\"> 随着信息技术的高速发展，当今社会信息的呈现方式日益多样化，图像已经成为一种至关重要的信息载体。人们的手机、相机等设备都可以获取大量的图像数据，自然而然地也产生了对图像数据处理的需求。计算机技术、数字图像处理技术以及深度学习技术的发展为图像处理领域的研究提供了强大的技术支持。 图像风格迁移技术的目的是将一张内容图像从原始风格转换成特定图像的风格。 图像风格是指一幅图像所呈现出的视觉特征，如色彩、纹理、笔触等。不同的色彩搭配能够营造出迥异的氛围与情感基调，如梵高的《星月夜》中浓郁且对比强烈的蓝、黄色彩，赋予画面神秘而奇幻的风格；纹理体现图像表面的细腻质感，像古典油画中厚重的笔触纹理、木质材料的天然木纹纹理等；笔触反映了创作者的绘画手法，细腻流畅或粗犷豪放的笔触能传达出截然不同的艺术韵味，例如中国传统水墨画中灵动多变的笔墨笔触。 图像内容指的是一幅图像所描绘的具体对象、场景及其布局结构。内容包括图像中的人物、物体、风景等实体元素以及这些元素之间的空间关系与组合方式。这些元素共同传达出画面的主题，也是图像语义理解的关键所在。 </div> ![图1.1 风格图像迁移示例](../static/images/Paper/example1.webp) <p class \"note\">图1.1风格图像迁移示例</p> <div class \"text\"> 在图像风格迁移的过程中，图像的风格与内容具有一定程度的相对独立性，所以能够将一种风格图像的风格特征抽取出来，并迁移至具有不同内容的另一幅图像上。如图 1.1所示，将世界名画梵高的《星月夜》画作与校园的景色相融合，既保留原内容中校园建筑和道路主体，同时融入了梵高的风格元素，展现全新风格魅力的视觉效果。 在艺术创作领域上，传统艺术创作受限于艺术家个人的技能、风格和创作工具，图像风格迁移技术的出现打破了艺术风格之间的壁垒，也降低了艺术创作的门槛。人们可以利用这一技术将富含特色的艺术绘画作品的风格应用到自己的照片中，创造出独特的艺术效果和氛围。例如，将自己的自拍照转化成素描画作，或者将现代摄影作品转化为古典油画风格或者山水画风格，或者将抽象画风应用到纪录照片上，拓展艺术创作的可能性，使艺术创作更加普及化、大众化^[1][2][3][4]^。在影视制作中，图像风格迁移可以用于特效制作，例如将现实拍摄下的场景转化为奇幻的动画风格^[5][6]^，或者将拍摄历史场景还原为特定的历史时期风格，增强影视作品的视觉效果和艺术感染力的同时降低制作成本。在医学上，医学图像通常具有高度复杂的结构和丰富的细节信息，通过风格迁移生成的图像既保留了图像的风格，又保留了图像的内容,使得生成的图像更具真实感和可信度^[7]^。此外，图像风格迁移技术还可以应用于文化创意产业领域。以戴娟的相关实践为例^[8]^，她运用图像风格迁移算法，通过将不同艺术风格与大熊猫形象有机融合，不仅为传统的文创设计赋予了崭新的视觉呈现形式，同时也拓展了文化周边产品的文化内涵与艺术感染力，进一步彰显了图像风格迁移算法在各领域应用中的潜力。 </div> ## 1.2 国内外研究现状 <div class \"text\"> 目前，图像风格迁移算法的研究可以分为传统风格迁移和基于神经网络的风格迁移两个发展阶段。从图像风格迁移任务实现的维度上来看，又可以分为三个阶段，分别是固定内容的固定风格迁移、任意内容的固定风格迁移和任意内容的任意风格迁移。 </div> ### 1.2.1 传统风格迁移方法 <div class \"text\"> 在深度学习兴起之前，传统的风格迁移方法主要基于手工设计算法和人工特征工程等计算机图形学知识实现风格迁移，比如通过数学建模或拼接现有图像的小块补丁进行融合内容与风格特征。这个阶段的图像风格迁移技术局限在固定内容图像或任意内容图像迁移 到固定的风格。 固定的内容图像迁移到固定的风格主要通过传统图像处理技术实现。传统的图像处理技术中，基于滤波的方法有利用高斯滤波、拉普拉斯滤波等线性滤波器提取图像的低频轮廓和高频细节，试图通过调整不同频率成分的分布来模拟风格特征；基于纹理合成技术则通过统计风格图像的纹理模式(如灰度共生矩阵、局部二进制模式等)，在内容图像上复制类似的纹理结构。这些传统处理方法显然需要特殊情况特殊处理，要求研究人员针对特定的内容和风格进行定制分析。例如，在处理风景图像的风格迁移时，可能需要设计专门捕捉云层纹理的滤波器；而在处理人物肖像时，则需调整纹理模板以适配面部特征。这种人工设计的方式暴露出两大核心缺陷：一方面，算法的泛化能力非常弱，每一种新的内容或风格组合都需要重新设计处理流程，不能形成通用的解决方案；另一方面，传统方法仅仅解读图像的浅层视觉特征(如颜色分布、边缘方向、纹理重复模式等)，完全缺乏对图像深层语义的理解。 有的研究人员尝试从不同角度挖掘风格图像中的纹理基元，通过拼接重组生成新图像。Efros等人提出了通过对目标图像的风格纹理进行拼接和重组操作，利用纹理元素之间的组合关系，最终合成全新风格的图像^[9]^。他们的方法能较好地复制风格图像的重复纹理(如砖墙、布料)，但在处理非周期性纹理(如绘画笔触)时，容易出现明显的拼接痕迹。 研究人员们还试图通过模拟绘画的物理过程实现风格迁移。Hertzmann等人提出了图像类比框架进行图像纹理合成，通过建立内容图像与风格图像的局部块对应关系，将风格图像的色彩分布、边缘方向等统计特性迁移到内容图像，可以合成各种艺术绘画风格^[10]^。 然而这些传统方法存在显著的缺陷，它们本质上是人工进行设计不同风格的特征表达，只是提取了图像的底层特征，未能表达图像的高层抽象特征。当遇到颜色和纹理复杂的图像时，生成的图像风格效果粗糙，不能满足如今高质量图像的需求。 </div> ### 1.2.2 神经网络风格迁移方法 <div class \"text\"> 随着深度学习技术的逐渐发展以及人们对图像质量要求的不断攀升，这些传统方法所固有的局限性逐渐暴露出来。在特征提取这一关键层面，传统方法局限于运用相对简单的技术手段对图像底层特征的获取，仅聚焦于图像的基本像素信息、颜色直方图等初级特征。这种情况下图像所蕴含的高层抽象特征，如图像所传达的语义信息、物体之间的逻辑关系等深层次内涵，难以精准捕捉。正是因为传统方法缺乏对复杂特征的深度处理能力，所以此类传统方法所生成的图像在风格呈现效果上显得颇为粗糙，图像可能出现纹理模糊、风格杂糅不自然等问题，无法满足在视觉美感、语义准确性等方面的严苛需求。深度学习技术凭借自身强大的自动特征学习本领，依托海量的数据支撑以及复杂的神经网络架构，能够深入挖掘图像中的深层次信息，精准分离图像的内容与风格特征，并通过复杂的模型架构与训练机制，实现两者的有机融合，进而生成高度逼真、风格独特的图像^[11][12]^。 早期的基于神经网络的固定内容的固定风格迁移方法将生成图像作为可优化变量。这样的风格迁移方法局限于较高的计算复杂度，而且固定输入的设定导致其缺乏对动态内容或风格的泛化能力。2015年，Gatys等人提出了基于卷积神经网络的图像风格迁移算法，将生成图像视为优化变量，通过预训练卷积神经网络提取的内容特征与风格特征，构建内容和风格双损失函数推进像素优化，首次证明了神经网络提取的深度特征可以有效分离内容与风格，为后续研究奠定了理论基础^[13]^。 每次生成图像都需要对像素值进行数百次迭代优化的生成方式已经无法满足效率需求，研究人员们着手研究基于神经网络的任意内容的固定风格迁移方法。迁移方法基于深度卷积神经网络，通常采用编码器 解码器架构。编码器负责提取内容图像的语义特征，将图像数据变得抽象；解码器则将编码后的语义特征与目标风格信息结合，恢复图像内容生成风格化图像。 任意内容图像的固定风格迁移将优化目标从图像空间转移到网络参数空间。2016年，斯坦福大学李飞飞团队针对Gatys方法的效率瓶颈，提出了基于感知损失的图像风格迁移方法，使用感知损失代替原损失，设计了一个包含残差连接的深层网络，直接以内容图像到风格图像作为训练数据，通过训练图像变换网络参数生成直接逼近最优解图像^[14]^。基于预训练模型的风格迁移算法在训练时，首先固定预训练网络模型的参数，然后仅对转换网络的权值进行更新。通过随机梯度下降等优化算法，在大规模内容图像数据集上进行迭代训练，使转换网络学习从任意内容图像到目标风格的映射关系。训练完成后，对于新的内容图像，只需将其输入训练好的网络，通过前向传播即可快速生成风格化结果，无需像早期方法那样对单张图像进行耗时的迭代优化。 后续还针对Gatys 提出的方法进行了诸多改进。Li等人并未使用特征映射的Gram矩阵来表征风格，而是证实了其他几种损失函数同样能够有效地完成任务。其中，均值方差表示法尤为突出，相较于Gram矩阵表示，它更为紧凑，且性能表现相近^[15]^。此外，风格还有其他多种表示形式，诸如直方图损失^[16]^、MRF损失^[17]^以及CORAL损失^[18]^。Dumoulin等人提出利用条件实例归一化来适配每种风格。该方法通过调整特征通道的权重，成功实现对多种不同风格的表示^[19]^。Zhu等人提出了循环对抗生成框架，通过两个生成器学习两种图像域的映射，并使用两个判别器尝试区分生成图像和真实图像，在风格转移和季节转移方面彰显了该方法的优越性^[20]^。与此同时，Der Lor Way等人在动漫风格迁移任务的基础上，提出了一种新颖的动漫风格迁移算法，达到了不错的图像风格迁移效果^[21]^。它们的算法针对图像的前景与背景在风格呈现上的差异，进行不同的处理策略，例如，对于动漫人物作为前景，人物的细节和风格需要更细致地刻画，而画面的背景可能更注重整体氛围的营造和与前景的融合。然而，这些方法无法推广应用于新的风格图像。 面对新风格任务时上述的模型需重新训练。这样每一种风格一个模型的情况在实际应用中成本极高。为了进一步提高图像风格迁移的灵活性和通用性，研究人员开始探索任意内容的任意风格迁移技术。 在这个阶段中，Huang等人提出了新颖的归一化思想，他们通过对齐内容图像与风格图像在预训练VGG 16网络多层特征空间的均值与方差统计量，将风格迁移过程转化为特征分布的匹配问题。虽然该方法能够处理任意新风格，但严重依赖VGG 16网络对图像进行编码，同时需要相应网络对特征进行解码，无法利用更先进的主干网络，这给模型的控制带来了困难^[22]^。Chen等人提出基于图像块的局部风格匹配，引入了风格交换来处理任意风格迁移。该方法将内容图像和风格图像分割为重叠的图像块，通过K近邻搜索在风格图像中找到与内容块最相似的补丁，然后将其替换到内容图像中，但是处理的速度非常缓慢^[23]^。Zhang等人提出一个多风格生成网络MSG Net，设计CoMatch层匹配风格图像的Gram矩阵，捕捉特征之间的相关性。该方法整体采用类似U Net的编码器 解码器框架，并且跳跃连接中引入风格调节模块，在风格实时迁移中实现了高质量的输出^[24]^。 再后来，基于 Transformer的风格迁移算法利用自注意力机制和交叉注意力机制进行风格迁移，其中自注意力机制旨在捕捉图像内部的长距离依赖关系，而交叉注意力机制则用于融合内容图像和风格图像的特征，进而实现风格迁移。2022年，Jianbo Wang等人将自然语言处理中的Transformer架构引入风格迁移，提出了一种新颖的STyle TRansformer(STTR)网络，在风格迁移结果上具有令人满意的有效性和效率^[25]^。他们的网络将内容和风格图像分解为视觉标记，以实现细粒度的风格转换。网络中用到了两种注意力机制，其中自注意力机制用于编码内容和内容标记，然后在内容和风格标记之间融入交叉注意力机制。与其他一些基于传统卷积神经网络的方法相比，STTR的创新点在于其采用视觉标记化和双注意力机制，突破了传统全局特征转换的局限。 2023 年，Chiyu Zhang等人提出了一种基于Transformer的新方法用于图像风格迁移，并引入基于Canny算子的边缘检测分支，可以明显增强内容细节。与其他基于Transformer的图像风格迁移方法相比，该方法能够有效避免因过度渲染风格特征而生成模糊结果^[26]^。颜明强等人提出了一个基于自注意力机制的渐进式流形特征映射模块(MFMM AM)，用于协调一致地匹配相关内容和风格流形之间的特征；然后通过在图像特征空间中应用精确直方图匹配来实现风格和内容特征图的高阶分布匹配，减少了图像信息的丢失^[27]^。 到目前，纪宗杏等人提出了一种新型的基于双路视觉 Transformer的图像风格迁移方法Bi Trans，对内容图像域和风格图像域进行独立编码，通过交叉注意力机制与条件实例归一化将内容图像标定至目标域风格，从而生成风格化图像^[28]^。 </div> ## 1.3 论文研究内容 <div class \"text\"> 本文的研究主要针对基于MetaNet网络风格迁移算法存在的局限性进行改进，提出的改进方法提升原来的图像风格迁移效果。主要研究内容和创新点如下： (1)在特征提取方面，本研究采用预训练的VGG卷积神经网络作为特征提取模块，对比VGG 16与VGG 19两种卷积神经网络模型在风格特征提取中的差异，发现深层网络结构对风格迁移的影响机制。 (2)本研究对元学习器与图像转换网络进行双重结构增强，并尝试引入新的超参数以强化模型的能力。元学习器中添加注意力机制增强处理特征信息的能力，同时改进图像转换网络的结构，以优化图像风格迁移的效果。 (3)本研究使用COCO数据集和WikiArt数据集对模型进行训练，联合内容损失函数和风格损失函数以优化模型参数。后续对元网络与现有的图像风格迁移算法进行对比分析，评估模型生成图像的质量和效率，展示本研究方法的优越性。 </div> ## 1.4 论文结构安排 <div class \"text\"> 本文共五章，结构安排如下： 第一章，绪论。本章首先简要介绍论文研究内容的背景及意义，接着阐述了图像风格迁移领域的国内外研究现状，最后介绍本文的研究内容和文章结构。 第二章，相关理论基础。本章简单介绍卷积神经网络和编码器 解码器架构，为本研究模型的搭建打下基础。接着对本研究进行可行性分析，证明本研究的有效性。 第三章，基于改进的MetaNet风格迁移算法研究。本章首先对原始模型MetaNet进行原理分析，接着阐述本章提出的改进方法，包括去除对图像尺寸的限制、改进的图像转换网络结构设计、添加超参数配置和注意力机制等。 第四章，实验与评估。本章主要介绍本文改进方法的训练环境和配置参数，并将改进的算法和现有的一些风格迁移算法进行实验对比，通过定性和定量分析本文改进方法在风格迁移任务上的优越性。 第五章，总结与展望。本章对本文的研究进行总结，并对后续研究方向展开探讨。 </div> [1] 董心悦,傅鹏.基于改进生成对抗网络的人脸图像风格迁移方法[J].鄂州大学学报,2025,32(02) 94 97. [2] 熊文楷.基于深度学习的中国画风格迁移[J].科技与创新,2023(13) 176 178. [3] Liao M, Huang F. Deep Learning‐Based Application of Image Style Transfer[J]. Mathematical roblems in Engineering, 2022 1 10. [4] 胡琦瑶,刘乾,彭先霖,张翔,彭盛霖,范建平.SN CL GAN 基于谱归一化的中国传统山水画风格迁移方法[J].西北大学学报（自然科学版）,2025,55(1) 63 74. [5] 刘欢.基于改进生成对抗网络的图像动漫风格迁移研究[D].哈尔滨师范大学,2023. [6] 蔡建禄.基于深度学习的多风格场景卡通化与图像风格化算法研究[D].浙江工商大学,2024. [7] 赖灿芸.基于风格迁移的图像生成模型及其在医学领域的应用[D].浙江工商大学,2024. [8] 戴娟.风格迁移算法在大熊猫文创设计中的运用[J].鞋类工艺与设计,2024,4(18) 192 194. [9] Alexei A. Efros, William T. Freeman． Image Quilting for Texture Synthesis and Transfer[C]//Computer Graphics.Computer Science Division, University of California, Berkeley, Berkeley, CA 94720 USA, 2001 341 346. [10] Hertzmann A, Jacobs E C, Oliver N, et al. Image Analogies[C]// roceedings of SIGGRA H. 2001 327 340. [11] 镇家慧,罗明俐.基于卷积神经网络的图像风格变换[J].数码设计（下）,2021,10(6) 43. [12] 廉露,田启川,谭润,等.基于神经网络的图像风格迁移研究进展[J].计算机工程与应用,2024,60(09) 30 47. [13] Gatys L, Ecker A, Bethge M. A Neural Algorithm of Artistic Style[J].Journal of Vision, 2016, 16(12) 326 326. [14] Johnson J, Alahi A, Fei Fei L. Perceptual Losses for Real Time Style Transfer and Super Resolution[C]//Computer Vision–ECCV 2016 14th European Conference, Amsterdam, The Netherlands, October 11 14, 2016, roceedings, art II 14. Springer International Publishing, 2016 694 711. [15] Li C, Wand M. Precomputed Real Time Texture Synthesis with Markovian Generative Adversarial Networks[C]//Computer vision–ECCV 2016 14th European conference, amsterdam, the netherlands, October 11 14, 2016, proceedings, part III 14. Springer International ublishing, 2016 702 716. [16] Risser E, Wilmot P, Barnes C. Stable and Controllable Neural Texture Synthesis and Style Transfer Using Histogram Losses[J]. CoRR, 2017. [17] Li C, Wand M. Combining markov random fields and convolutional neural networks for image synthesis[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016 2479 2486. [18] Peng X, Saenko K. Synthetic to real adaptation with generative correlation alignment networks[C]//2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2018 1982 1991. [19] Dumoulin V, Shlens J, Kudlur M. A Learned Representation For Artistic Style[J]. CoRR, 2016. [20] Zhu J Y, Park T, Isola P, et al. Unpaired Image to Image Translation Using Cycle Consistent Adversarial Networks[C]// roceedings of the IEEE international conference on computer vision. 2017 2223 2232. [21] Way D L, Chang W C, Shih Z C. Deep Learning for Anime Style Transfer[C]// Proceedings of the 2019 3rd international conference on advances in image processing. 2019 139 143. [22] Huang X , Belongie S. Arbitrary Style Transfer in Real time with Adaptive Instance Normalization[C]// roceedings of the IEEE international conference on computer vision. 2017 1501 1510. [23] Chen Q T, Schmidt M. Fast atch based Style Transfer of Arbitrary Style[J]. CoRR, 2016. [24] Zhang H, Dana K. Multi style Generative Network for Real time Transfer[M/OL]//Lecture Notes in Computer Science,Computer Vision – ECCV 2018 Workshops. 2019 349 365. [25] JianBo Wang, Huan Yang, JianLong Fu, et al. Fine Grained Image Style Transfer with Visual Transformers[C]//Computer vision – ACCV 2022, art 3 16th Asian conference on computer vision (ACCV), December 4 8, 2022, Macao, China. 2023 427 443. [26] Zhang C, Yang J, Dai Z, et al. Edge Enhanced Image Style Transfer via Transformers[J]. 2023 105 114. [27] 颜明强,余鹏飞,李海燕,等.语义风格一致的任意图像风格迁移[J].计算机科学,2023,50(07) 129 136. [28] 纪宗杏,贝佳,刘润泽,等.基于双路视觉 Transformer 的图像风格迁移[J].北京航空航天大学学报,2024 1 12."},"/StyleTransfer/paper/proposed_network.html":{"title":"基于改进的 MetaNet 风格迁移算法研究","content":" title: 基于改进的 MetaNet 风格迁移算法研究 keywords: Proposed Network desc: 论文提出的网络结构 date: 2025 06 21 id: proposed_network class: heading_no_counter <style> .text { text indent: 2em; font size: 20px; } .note { text align: center; font size: 0.8em\" } </style> ## 3.1 MetaNet 算法原理分析 <div class \"text\"> MetaNet意为元网络，是一种基于元学习的深度学习模型，用于解决图像风格迁移中的速度、灵活性和质量平衡问题。本文讨论的超网络和元网络都是一种为其他网络生成权重参数的网络。在早期，Ha等人提出使用静态的超网络为卷积神经网络生成权重参数，并使用动态的超网络为循环网络生成权重参数^[34]^。 元网络遵循元学习规律，通过分层学习策略实现跨任务知识迁移和单个任务的快速适应，生成的网络更为灵活和全面。元网络的工作采用两级学习，分别是跨任务执行的元级(Meta Level)模型的缓慢学习和每个任务内执行的基本级(Base Level)模型的快速学习。 跨任务执行的元级模型缓慢学习意思是模型处理大规模跨任务数据时进行的长期知识积累。学习过程呈现显著的缓慢变化，称为“缓变性”。而且参数更新周期覆盖多个任务集合，优化目标聚焦于提取跨任务的通用先验知识，例如不同任务共享的特征表示空间、优化器超参数配置或归纳偏置模式。缓变学习机制使得元级模型能够捕捉任务间的共性结构，形成可迁移的元知识，如适用于多种任务类型的初始化参数分布或动态调整策略。 每个任务内执行的基本级学习指的是针对具体任务场景中，网络在元级知识支撑下可以实现快速完成任务。当处理单个新任务时，基本级模型从元级输出的先验知识出发，只需通过少量样本或迭代步骤完成任务特定的参数调整。这种快速学习过程通常表现为在元级提供的初始状态基础上，进行局部参数的梯度更新或结构微调，其学习速率显著高于元级。例如在小样本学习场景中，基本级模型可利用元级预训练的特征提取器，仅通过数轮迭代即可在新类别上达到理想性能。 Munkhdalai等人在小规模标本学习领域的研究中，提出了一种通过快速参数化实现一次性分类的元网络，能够快速泛化完成任务。他们设计的元网络核心在于构建一个能够动态生成特殊任务特定参数的元学习器，通过对支持集的快速编码，直接生成目标任务分类器所需的权重参数。这样的快速参数化过程无需重复迭代训练基本模型，而是通过元网络的前向传播实现一次性参数映射，将传统小样本学习中的模型适应时间从分钟级缩短至毫秒级^[35]^。 在某些情况下，为了得到一个图像变换网络，需要通过SGD在风格图像数据集上多次训练最小化风格图像与输出结果图像的损失。训练的目标是获得一个性能较好的网络，在输入内容图像域与风格图像域之间建立一个映射。元网络的思想与SGD的思想不同，元网络尝试建立一个生成网络的网络，输入风格图像，输出相应的图像变换网络。 基于元网络的图像风格迁移算法构建基于三个逐步递进的条件假设，从传统的参数优化问题过渡到动态网络的生成。令f(x)和h(x)为固定可微函数，记·为正则化，考虑优化问题式3.1。将a看作输入内容图像，b看作输入风格图像，f(x)看作内容感知函数，h(x)看作风格感知函数。 </div> $$ \\Vert f(x) f(a) \\Vert + \\lambda \\Vert h(x) h(b) \\Vert\\tag{3.1} $$ ### 3.1.1 条件假设一 <div class \"text\"> 固定a a<sub>0</sub>，b b<sub>0</sub>。这种条件下，对应固定内容图像的固定风格迁移。a<sub>0</sub>可以表示是苹果、汽车或建筑等某已知的内容图像，而b<sub>0</sub>表示是印象派、巴罗克风格或抽象派等某特定的绘画风格。 为了解决这个问题，假设f(x)和h(x)是凸函数，则公式3.1是一个关于x的凸优化问题。凸优化问题中，不存在局部最小值，任何一个局部最优解就是全局最优解。基于凸优化问题的特性，该假设的解决方法是使用梯度下降法。梯度下降法是机器学习和深度学习领域最常用的迭代优化算法之一。梯度是一个向量，它的方向指的是目标函数在该点处增长最快的方向。梯度下降则根据梯度信息调整参数的更新方向，使得目标函数逼近最优解。在图像不断更新的过程中，梯度的方向指的是损失函数增长最快的方向，所以根据梯度的反向逐步更新合成图像。经过梯度下降迭代后，使得损失函数最小的图像即是目标图像。但是梯度下降法在实际应用中需要经过数百次的优化迭代才能得到每个样本的收敛结果。每次迭代都涉及损失函数和梯度的矩阵计算，消耗大量的计算资源和时间。 </div> ### 3.1.2 条件假设二 <div class \"text\"> 固定b b<sub>0</sub>，让a可变。无论输入什么内容a，整个风格迁移过程只能向着一种特定的风格迁移。 为了解决条件假设二的问题，考虑引入一个具有可学习参数w的从a到输出x的映射 $\\mathcal{N}:a\\rightarrow x$，将输入内容图像a通过参数w进行计算得到结果x。该过程为如公式3.2所示。为了找到这一映射，把学习映射的过程当作训练神经网络的过程，w包含神经网络的权重和偏置等可学习参数。通过大量的风格图像来训练神经网络，不断地调整参数w的值，将风格图像中的特征和风格信息编码到参数w中。 </div> $$ x \\mathcal{N}(a;w)\\tag{3.2} $$ <div class \"text\"> 当神经网络训练至收敛时，参数w就被确定下来。对于每一张新的内容图像，只需要通过图像变换网络的前向传播即可生成对应的图像。一次前向传播的过程大大减少了计算的时间，在条件允许下可实现图像的实时风格迁移。 </div> ### 3.1.3 条件假设三 <div class \"text\"> 当a和b都可变。这种条件下，网络需要处理内容和风格两个变量，自适应生成对应的风格迁移结果，对应图像风格迁移的第三个阶段——任意内容的任意风格迁移。 根据条件假设2，已经存在 $\\mathcal{N}(\\cdot;w)$，引入元学习驱动的每个任务内执行的基本级模型的快速学习。假设存在高层映射 $w 𝑚𝑒𝑡𝑎\\mathcal{N}(b;θ)$，通过输入b和参数θ计算图像转换网络的参数w，如公式3.3所示。对于每一个给定的风格特征b，都可以寻找一个最优的w，然后结合参数w和输入a计算得到输出x。 </div> $$ w meta\\mathcal{N}(b;\\theta)\\tag{3.3} $$ <div class \"text\"> 总而言之，元学习经过海量数据训练学习得到元级知识后，当需要迁移新的风格时，只需三个步骤即可生成迁移后的图像：第一步，将新风格图像输入预处理模型得到风格特征；第二步，将提取的风格特征传入训练后的元学习模型中，得到图像转换网络的权重参数w；第三步，将输入内容输入到由w参数化的神经网络中，经过一次前向传播得到迁移后的图像。这种范式突破了传统深度学习模型每一个模型只能完成每一个小任务的局限，实现了通过元级知识积累的任务级快速适配。 </div> ### 3.1.4 MetaNet网络架构 <div class \"text\"> Shen等人提出的元网络由一个预训练的VGG 16网络、元学习器和图像转换网络组成，思想是元网络两级学习中的每个任务内执行的基本级模型的快速学习。VGG 16网络从风格图像中提取风格特征，然后将纹理特征输入到元学习器中。元学习器经过大量风格图像的特征训练后，将风格特征投影到图像变换网络的参数中。图像转换网络经过元学习器填充权重参数后，便形成了某种风格的迁移转换网络。通过这种方式，他们首次提供了一种新的方法，在一次前馈传播中接受新风格图像并生成对应图像的图像变换网络^[36]^。这种方法的思想是让模型学习如何学习，即学习图像转换网络的生成方式，从而实现对新风格的快速适应。 Shen提出的一个图像转换网络版本结构如图3.1所示，其中每个残差块由2层卷积层组成，5个残差块共10层卷积，整个图像转换网络共14层卷积层。下采样部分首先经过较大(40×40)的反射填充层将边界扩大，然后经第一个9×9卷积层将通道数增加到8，最后依次经过第二第三层3×3卷积层将输出特征图尺寸缩小一半，通道数翻倍；上采样部分首先依次经过第一第二层3×3反卷积层减少通道数和恢复特征图尺寸，然后经过第三层9×9卷积层将通道数降至3，从而输出正常的三通道彩色图像。每一个残差块都是两层3×3不填充卷积层，将输出特征图的长和宽各减少4。经过5个残差块后与下采样块连接。除了第一层卷积层和最后一层卷积层之外，每个卷积层后面接上一个实例批次归一化层和一个激活函数ReLU层，为了简化起见，图中省略。在模型训练阶段，绿色的卷积层卷积核与元网络一同训练。其他卷积层卷积核和残差块的卷积核固定不参与训练，后续不进行更新。在模型推理阶段，所有淡蓝色的卷积层卷积核和残差块卷积核都由元学习器生成。 元网络的整体架构如图3.2所示。左侧将风格图像输入到预训练的VGG 16模型中，将VGG 16模型的第3、8、15、22层输出作为风格特征。元学习器指的是中间部分的全连接层。风格特征经过元学习器的两个全连接层得到对应图像变换网络中每个不参与训练的卷积层卷积核参数。其中第一个全连接层的输入维度为风格特征的维度，输出维度为1972；第二个全连接层将前一层的输出分组映射到图像生成网络各卷积层的卷积核权重参数。上述提到图像转换网络共需生成参数14层，每层的权重参数由128维向量通过全连接层生成，共得维度14×128维，即1972维。同时，通过预训练的VGG 16分别计算风格迁移后的生成图像的风格损失和内容损失。 </div> ![图3.1图像转换网络的结构图](../static/images/Paper/TransformNet.svg) <p class \"note\">图3.1图像转换网络的结构图</p> ![图3.2MetaNet整体架构](../static/images/Paper/MetaNet.svg) <p class \"note\">图3.2MetaNet整体架构</p> <div class \"text\"> 在元学习器的风格图像特征处理方面，假设输入风格图像的大小为256×256，那么通过预训练VGG 16模型提取的特征输出尺寸分别为(64，256，256)、(128，128，128)、(256，64，64)、(512，32，32)。假设取Gram矩阵作为特征进行计算，输出尺寸为(64，64)、(128，128)、(256，256)、(512，512)，依靠这些尺寸特征生成对应的权值，可想而知计算量是非常庞大的。Shen提到，只计算卷积层输出的均值和标准差作为风格特征。通过这个思路进行计算，风格特征的维度变为(64+128+256+512)×2，即1920维。但是直接使用这1920维特征向量生成14层卷积层的卷积核权重参数还是比较困难，所涉及参数量仍然非常庞大，十分占用硬件资源。为了解决这个问题，设定图像转换网络的每层不参与训练的卷积层卷积核权重参数由单独的128维向量通过全连接层生成，14层卷积层共计1972个输出，元学习器将这些输出分组映射到图像转换网络中的每层权重参数，“*”表示图像转换网络中某层卷积层所需的参数数量，具体结构如图3.3所示。 </div> ![图3.3元学习器分组映射](../static/images/Paper/MetaNetGroups.svg) <p class \"note\">图3.3元学习器分组映射</p> ### 3.1.5 损失函数设计 <div class \"text\"> Shen等人设计的损失函数由图像内容损失、图像风格损失和图像全变分损失组成。 图像内容损失定义为图像风格迁移后的图像与原输入内容图像的内容特征均方误差。如图3.2所示，内容特征通过预训练VGG 16模型的relu3_3层输出得到。均方误差是比较常用的误差，通过预测值与真实值之间的差值平方和的均值计算得到。均方误差计算公式如式3.4，其中 $f(x_i)$ 为预测值，n为计算样本总数。均方误差的函数曲线光滑连续且处处可导，随着误差减小，梯度也随之减小，这一特性有利于训练过程中的收敛。 </div> $$ MSE \\frac{\\sum_{i 1}^n(f(x_i) y_i)^2}{n}\\tag{3.4} $$ <div class \"text\"> 图像风格损失定义为图像风格迁移后的图像与原输入风格图像的风格特征均方误差。风格特征由图像经过预训练VGG 16模型的relu1_2层、relu2_2层、relu3_3层和relu4_3层输出特征图的均值和标准差拼接得到。 全变分损失的目的提高风格迁移后图像的质量，保持图像平滑。在图像生成的过程中，图像上的微小噪声会对结果产生比较大的影响，且受噪声污染的图像的总变分比无噪声图像的总变分大。所以将总变分损失作为正则项引入到损失函数中，以此达到一定程度上的降噪处理。 </div> ## 3.2 模型改进设计 ### 3.2.1 图像转换网络结构设计 <div class \"text\"> 内容图像在输入图像转换网络时，原网络设置卷积层的卷积核步幅大小为2，在特征途中每隔2个像素进行卷积操作，以此进行下采样操作。虽然这种下采样方式的计算速度较快，但是容易出现特征信息提取丢失问题。如果一张高频细节比较丰富图像经过每隔2个像素的下采样操作，得到的结果相当于丢弃一半的空间信息。同时，固定的步幅卷积仅仅通过加权求和进行特征聚合，缺乏了对局部特征的选择性提取。所以，在改进方法中，将下采样卷积层的卷积核步幅大小设置为1，同时在后面增加一层2×2的最大池化层，以此进行下采样操作。在上采样操作中，网络通过双线性插值的方法代替原来固定缩放因子的方式，并且借助PyTorch深度学习框架的相关函数，动态尺寸计算实现恢复原来的空间尺寸。双线性插值法通过在两个方向上分别进行线性插值来得到未知点的像素值。 经过改进后的图像转换网络结构如图3.4，整个图像转换网络总卷积层数不变，增加设置base作为卷积层的通道基数，控制卷积通道变化，每经过一个下采样块，输出特征图的尺寸就缩小一半，同时通道数加倍。原来设计中base为8，改进后base为32，增加图像转换网络的通道数以获得更多更全面的特征。与原来的设计一致，仍然保持第一层卷积层和最后一层卷积层（绿色卷积层）卷积核与元学习器一同训练。其他卷积层卷积核和残差块的卷积核参数使用Kaiming Normal正态初始化固定，后续不参与训练不进行更新，在推演时由元学习器生成。另外说明的是，残差块内部和上采样部分仍然包含反射填充层、实例归一化层和ReLU激活层，为简便起见，图中省略。下采样部分中引入最大池化层，通过选取2×2窗口中的最大值作为输出，最大值能够灵敏地捕捉到图像中最强的特征，同时能够抑制噪声信号。池化窗口使用2×2对应原来下采样卷积核步幅为2，以此达到相同的下采样效果。为了尽可能减少不同图像间亮度、对比度差异并且同时保留各图像的纹理风格信息，改进的网络中引入实例归一化层。实例归一化层可以针对单个样本单通道进行归一化，保留图像像素的细节。除了实例归一化，还有同批次所有样本同通道的批次归一化、单样本的单层所有通道的层归一化和单样本通道分组的分组归一化。与其他归一化方法相比，实例归一化更适用于图像风格迁移场景，而且忽略跨样本统计信息，避免风格混合不明确。 </div> ![图3.4改进后的图像转换网络](../static/images/Paper/TransformNetImproved.svg) <p class \"note\">图3.4改进后的图像转换网络</p> ### 3.2.2 注意力模块设计 <div class \"text\"> 原网络结构中没有包含注意力机制。在模型改进中，针对元学习网络的模型结构分别设计了通道注意力模块、自注意力模块和Transformer模块。加入注意力模块后网络流程如图3.5所示，注意力机制模块用于增强元学习器的特征处理能力^[37]^。 </div> ![图3.5网络流程图](../static/images/Paper/flow.svg) <p class \"note\">图3.5网络流程图</p> <div class \"text\"> 通道注意力模块通过对特征图的通道维度进行学习，能够有效学习不同通道特征的重要程度。通道注意力模块首先对输入特征进行全局维度的压缩，获取通道维度上的全局统计量，随后通过全连接层对通道统计信息进行非线性变换，生成与通道数量一致的权重向量，最后将生成的通道权重与原始输入特征进行逐通道相乘，实现对重要通道特征的增强和对次要通道特征的抑制。在本研究模型中，预训练的VGG模型将提取到的1920维风格特征输入到元学习器中并前向传播，经过全连接层生成G×128个输出，然后将一整批次大小的G×128个特征输入到基础通道注意力机制模块的输入层中。添加的基础通道注意力模块如图3.6所示，其中B表示批次大小，G表示分组数，此处G为14，即图像转换网络中不参与训练更新学习的14层卷积层。在基础通道注意力模块中，首先将输入分组重塑为三维的特征。接着基于SENet的思想，通过全局平均池化压缩三维分组信息形成B×128的二维特征，然后将二维特征输入到全连接块学习分组通道间的权重。全连接块包括两层全连接层和两层激活函数层，全连接层由ratio变量控制。二维特征经全连接块压缩激活处理后，再通过维度恢复操作还原为与输入分组维度匹配的特征，得到通道特征的注意力权重向量。最后将三维特征和通道特征的注意力权重通过张量广播相乘，得到添加注意力机制后的G×128个输出。这G×128个输出经过后续G个全连接层分组映射连接到图像转换网络中不参与训练的卷积层卷积核权重参数。 </div> ![图3.6含基础通道注意力模块的元学习器](../static/images/Paper/basic channel att.svg) <p class \"note\">图3.6含基础通道注意力模块的元学习器</p> <div class \"text\"> 基础通道注意力仅关注全局信息。为了进一步考虑局部信息，对基础通道注意力机制进行增强改进，设计增强通道注意力机制模块，如图3.7所示。含增强通道注意力模块的元学习器输入同样为批次大小的1920维风格特征，经过隐藏层映射为G×128的特征，再输入到增强通道注意力模块中。与基础通道注意力模块相比，增强通道注意力模块主要引入全局权重和分组权重两部分，通过调整它们间的比例加权平衡局部与全局的特征信息。分组权重部分参考原来基础通道注意力模块的计算权重方式，学习每组内部的通道信息。另外，分组权重部分使用GELU（Gaussian Error Linear Unit）激活函数代替原来的ReLU激活函数，GELU函数可以近似地表达为公式3.5。GELU函数在x小于0的部分取值并不为零，在一定程度上可以缓解神经元失活的问题，避免梯度消失。分组权重模块还对全连接层的输出进行层归一化操作，提升模型的表达能力。全局权重由一层全连接层和一层Sigmoid激活函数层组成，融合所有分组的全局信息。 </div> $$ \\text{GELU}(x) 0.5x[1+\\tanh(\\sqrt{2/\\pi(x+0.047715x^3)})]\\tag{3.5} $$ ![图3.7含增强通道注意力模块的元学习器](../static/images/Paper/enhanced channel att.svg) <p class \"note\">图3.7含增强通道注意力模块的元学习器</p> <div class \"text\"> 自注意力机制能够捕捉输入特征图中不同位置的长距离依赖关系，将每个元素同时当成查询（Query）、键（Key）和值（Value），通过三者的计算实现对上下文的感知表示。自注意力机制模块设计如图3.8。元学习器在得到提取的风格特征并经过全连接层之后，将批次大小和G×128参数数量二维的特征空间变换为批次大小、通道数、高度和宽度的四维伪空间，其中通道数为参数数量G×128，高度和宽度均为1，以便于后续的投影和矩阵运算。随后将该尺寸的特征输入到自注意力模块的输入层，分别计算查询投影、键投影、值投影。查询投影通过降维生成查询向量，查询向量用于捕捉“需要关注什么”，能够聚焦于关键的语义信息，从而为后续的注意力权重计算提供引导。键投影通过降维生成键向量，键向量表示“被关注的内容”，能够描述输入特征中各个位置的特征表示，与查询向量共同计算位置之间的相似度。值投影保持原来的维度生成值向量，值向量表示“实际传递的信息”，同时也保留了输入特征的完整信息。计算得到查询向量、键向量和值向量后，将查询向量和键向量重塑为对应的尺度，使其适合进行矩阵乘法，计算每对位置的相似度。通过Softmax函数处理后，生成注意力权重矩阵。接着将值投影与注意力权重矩阵进行矩阵乘法，最后用参数γ进行残差连接，对值特征进行加权聚合，增强重要位置的特征。残差链接可以防止深层网络退化，保留原始特征中的重要信息，使得模型的训练较为稳定，参数γ用于调整残差连接的权重。 </div> ![图3.8含自注意力模块的元学习器](../static/images/Paper/self att.svg) <p class \"note\">图3.8含自注意力模块的元学习器</p> <div class \"text\"> Transformer基于自注意力机制实现捕捉序列中元素间的全局依赖关系。在自然语言处理领域，Transformer被广泛应用于机器翻译、文本生成、问答系统等任务，但经过一定修改之后，可以扩展应用在图像处理领域。本设计中简单构建一个应用于视觉处理领域的Vision Transformer模块（以下简称ViT模块），具体流程如图3.9所示。首先预训练的VGG模型对输入风格图像进行特征提取，得到1920维风格特征向量。接着风格特征向量输入到含Transformer的元学习器中，经过全连接层将1920维映射到指定的Embed大小，再通过变换操作得到关于批次大小、通道数、高度和宽度的四维伪空间，其中通道数与 Embed 大小一致。得到四维伪空间特征后进入到ViT模块，ViT模块首先对输入的四维伪空间数据进行1×1卷积的Patch Embedding处理。1×1卷积能够在不改变特征图空间维度的前提下，对通道维度进行线性组合，从而将每个空间位置的特征转换为嵌入向量。接着将嵌入向量进行展平操作，并重塑为关于长度、批次大小和通道数的三维空间特征。该三维空间特征会输入到若干个Transformer块中进行加强处理。其中每个Transformer块由多头注意力模块、残差连接、层归一化、前馈网络等部分组成。三维特征进入多头注意力机制模块，通过多个不同的注意力头并行计算，从不同角度捕获全局上下文依赖关系，获取序列中各元素之间的全局关联信息。与单一的注意力机制相比，多头注意力机制显然扩展了模型对特征的处理和理解能力。随后特征通过残差连接与输入特征相加，再进行层归一化操作，稳定输出并防止梯度消失问题。最后通过前馈网络增强数据的非线性表示能力，使模型能够学习到更复杂的特征表示。 </div> ![图3.9含Transformer的元学习器](../static/images/Paper/Transformer.svg) <p class \"note\">图3.9含Transformer的元学习器</p> ## 3.3 本章小结 <div class \"text\"> 本章围绕基于改进的MetaNet的图像风格迁移算法展开，介绍了MetaNet的核心原理、网络结构以及改进设计。 在MetaNet的算法原理部分，阐述了基本级模型通过元级模型积累的元级知识在单任务上的快速执行。通过三个条件假设，逐步推导从固定内容与风格的迁移问题过渡到任意内容与风格的动态生成场景，介绍了元网络通过生成图像转换网络参数实现快速风格迁移的核心思想。接着介绍了MetaNet的整体架构，包括特征提取器、元学习器和图像转换网络三部分。最后介绍内容损失、风格损失和全变分损失组成的损失函数体系，确保生成图像在内容、风格和结构上的一致性。 在模型改进设计中，针对原网络的不足做出两点改进：一是对图像转换网络进行网络层上的优化；二是对元学习器引入注意力模块。在图像转换网络的改进上，使用池化层代替原来的固定步幅卷积下采样，使用双线性插值代替原来的固定缩放因子，还引入通道基数超参数和实例归一化层，优化网络结构。在元学习器的改进上，引入通道注意力、自注意力和Transformer模块，用于提升元学习器的特征处理能力。 本章通过理论分析以及改进设计，搭建起新的图像风格迁移模型，为后续算法的实验验证奠定基础。 </div> [34] Ha D, Dai A, Le QuocV. HyperNetworks[M/OL]//Hypernetworks in the Science of Complex Systems. 2014 151 176. [35] Munkhdalai T, Yu H. Meta Networks[C]//International conference on machine learning. MLR, 2017 2554 2563. [36] Shen F, Yan S, Zeng G. Neural Style Transfer via Meta Networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018 8061 8069. [37] 王树声,李文书.基于神经网络与注意力的任意图像风格迁移研究综述[J].软件工程,2025,28(02) 27 31."},"/StyleTransfer/paper/index.html":{"title":"基于深度学习的图像风格迁移算法研究","content":" title: 基于深度学习的图像风格迁移算法研究 keywords: desc: 基于深度学习的图像风格迁移算法研究 date: 论文主要内容： 摘要 第 1 章 绪论 第 2 章 相关理论基础 第 3 章 基于改进的 MetaNet 风格迁移算法研究 第 4 章 实验与评估 第 5 章 总结与展望"},"/StyleTransfer/paper/related_work.html":{"title":"相关理论基础","content":" title: 相关理论基础 keywords: Related Work desc: 论文涉及的相关理论 date: 2025 06 21 id: related_work class: heading_no_counter <style> .text { text indent: 2em; font size: 20px; } .note { text align: center; font size: 0.8em\" } </style> ## 2.1 卷积神经网络 <div class \"text\"> 卷积神经网络(Convolutional Neural Network，CNN)是一种前馈神经网络，其核心思想通过局部连接、权值共享和空间下采样等机制，有效降低参数数量并提取数据的层次化特征，在图像识别、目标检测等计算机视觉领域以及自然语言处理、语音识别等领域取得了显著成就。CNN的基本结构由卷积层、激活函数层、池化层和全连接层构成。 卷积层是卷积神经网络的关键组成部分。卷积操作由卷积核完成。每个卷积核相当于 一个特征检测器，通过参数可训练的卷积核在输入特征图上进行卷积操作提取局部特征，例如边缘、纹理、颜色等。卷积核权重参数共享机制使得网络对平移变化具有鲁棒性，同时大幅度减少权重参数数量。一般卷积操作包含三个关键参数：卷积核大小决定感受野大小；步长大小表示卷积核每次移动的大小，影响控制输入特征图降采样速度；填充方式指的是在输入特征矩阵外围进行填充，会影响输出特征图的空间维度。一次卷积操作后的输 出特征图大小如式(2.1)所示： </div> $$ output \\frac {input + 2p k} {s}\\tag{2.1} $$ <div class \"text\"> 其中，input为输入特征矩阵大小，k为卷积核大小，s为步长，p为填充补零数，output为卷积后得到的输出特征矩阵大小。 激活函数层可以引入非线性因素打破线性模型的局限性，使网络能够学习和拟合更复杂的函数映射关系。常用的激活函数有Sigmoid、ReLU、Tanh和Leaky ReLU等。Sigmoid激活函数输出结果范围是(0，1)，适合用于输出预测概率的模型。Tanh激活函数是Sigmoid的变形，其输出结果范围是( 1，1)。ReLU(Rectified Linear Unit)激活函数可以加快模型收敛，并且可以缓解梯度消失问题。ReLU 函数计算过程相对简单，仅需对输入值执行简单的阈值操作，数学表达式如式(2.2)所示。ReLU函数能够有效缓解传统神经网络中普遍存在的梯度消失问题，但缺点是会出现神经元死亡的问题。Leaky ReLU解决了ReLU输入值为负时神经元死亡的问题，同时不需要进行指数运算，计算复杂度低，数学表达式如式(2.3)所示。 </div> $$ \\text{ReLU}(x) \\begin{cases} x, & x > 0 \\\\ 0, & x \\leq 0 \\end{cases}\\tag{2.2} $$ $$ \\text{Leaky ReLU}(x) \\begin{cases} x, & x > 0 \\\\ ax, & x \\leq 0 \\end{cases}\\tag{2.3} $$ <div class \"text\"> 池化层实际上是下采样层，主要通过最大值或平均值操作对特征图进行空间下采样。每次卷积操作后，特征会变得越来越高维，但是特征矩阵的大小却没什么变化。这种情况下会生成大量的参数，增大了网络的计算量，所以一般会加上一个池化层进行降低维度减少参数量。常见的池化方法有最大池化、平均池化和随机池化等。 全连接层位于卷积神经网络的末端，是由多个神经元组成的单层结构。全连接层将经过多次卷积和池化操作后得到的特征图展平，然后与该层的神经元进行连接，对提取到的特征进行分类或回归等任务。 </div> ### 2.1.1 VGG卷积神经网络 <div class \"text\"> VGG(Visual Geometry Group)网络由牛津大学的计算机视觉组(Visual Geometry Group)和Google DeepMind公司的研究人员共同研发，凭借其独特设计与卓越性能，在计算机视觉领域中的图像识别、目标检测、语义分割等多个关键方向具有重要的地位。 VGG网络的架构比较简单，主要通过堆叠一系列的卷积层和池化层来构建深度网络，最后连接全连接层以完成目标任务。VGG网络的卷积层配置的核心特征是使用小尺寸卷积核。与大尺寸卷积核相比，小尺寸卷积核在保持相同感受野的前提下，通过增加卷积层的数量引入更多非线性变换，从而能够学习更复杂的特征表示；同时，小卷积核的参数规模更小，可有效减少网络参数总量，降低模型过拟合的风险。随着网络深度的增加，模型能够逐层提取从低级到高级的抽象特征，其中VGG 16和VGG 19模型因深度与性能的平衡优化表现突出^[29]^。VGG 16的模型结构图如图2.1所示。 </div> ![图2.1VGG模型结构图](../static/images/Paper/VGG16.svg) <p class \"note\">图2.1VGG模型结构图</p> <div class \"text\"> VGG 16网络卷积层共13层，均采用3×3大小的小卷积核，步长为1，填充为1，通道数逐渐从64增大到512；池化层共5层，均采用2×2最大池化，步长为2，每次将输入特征图大小减半；全连接层3层，最后一层对应任务的输出，此处K取1000则为ImageNet的1000类分类任务。VGG 19网络相比VGG 16网络增加了3层卷积层，将神经网络的层次进一步加深，可以捕获更深层抽象的特征。 VGG模型最初是为ImageNet大规模图像分类任务设计。但在ImageNet数据集上预训练后，VGG模型能够很好的提取到多种层次的特征，可以将它的特征提取能力用在风格迁移任务上。 </div> ### 2.1.2 ResNet卷积神经网络 <div class \"text\"> 深度神经网络在图像识别等任务中取得了显著的成果，但随着网络深度的增加，会出现训练困难、性能退化等问题。传统的神经网络在增加深度时，由于梯度消失和梯度爆炸等问题，使得网络难以训练，并且准确率可能会饱和甚至下降。为了解决这个问题，何恺明等人在2015年提出了ResNet^[30]^。ResNet在2015年的ImageNet大规模视觉识别挑战赛(ILSVRC)中获得了冠军，其出色的性能引起了广泛的关注。 ResNet的核心思想是残差学习。与传统神经网络直接学习输入到输出的映射不同，残差学习仅学习输入与输出之间的残差。假设存在映射H(x)，传统网络则直接学习H(x)，而ResNet学习一个残差函数F(x) H(x) x，原来的映射变成H(x) F(x)+x，如图2.2所示。当残差为0时，网络可以直接学习恒等映射，从而使得网络更容易训练。ResNet通过残差学习使得网络可以更深，从而能够学习到更复杂的特征表示。在ImageNet数据集上的实验表明，ResNet可以达到非常深的深度，例如ResNet 152有152层，而传统的网络很难达到这样的深度。 </div> ![图2.2残差学习](../static/images/Paper/ResLearning.svg) <p class \"note\">图2.2残差学习</p> <div class \"text\"> 残差网络通过叠加残差块，具有分层的特征提取机制。在浅层特征提取上，像ResNet 18网络的前三个残差块，主要聚焦于学习图像边缘、纹理这类基础视觉特征；而中层特征提取上，像ResNet 50网络的第4至6个残差块，会将局部特征进行整合，进而形成物体部件；到了深层特征提取上，像ResNet 152的后十个残差块，能够把部件特征进一步抽象，转化为物体整体结构以及类别语义信息。这种逐层推进的特征学习方式，让深层网络可以捕捉到更为复杂的视觉模式，进而增强分类任务的判别能力。 </div> ## 2.2 编码器 解码器架构 <div class \"text\"> 编码器 解码器架构本质是一种从序列到序列的算法，用于完成自然语言处理和计算机视觉等领域中的处理从序列到序列的任务。编码器 解码器架构包括编码器和解码器，编码器通常可以是卷积神经网络、循环神经网络、LSTM或者是GRU，将可变长度的输入序列编码成中间形态的向量；解码器则根据编码器得到的中间表示逐步解析生成输出序列。 在自然语言处理领域的机器翻译中，需要将一种语言序列转换到另一种语言序列；在计算机视觉领域的图像转换中，需要将具有某种特征序列的图像转换到另一种特征序列的图像。早期研究机器翻译时，Sutskever等人提出了使用两层LSTM来构建序列到序列的模型，通过编码器将输入序列转换成固定的中间向量，再由解码器生成目标序列，实现了长度不一致的序列转换^[31]^。但是由于编码器输出固定的向量，导致有些信息丢失，影响质量。在只使用循环神经网络的实现中，编码器的输出受到限制，容易导致信息丢失、长距离依赖捕捉不足等情况。 U Net是一个典型的编码器 解码器架构例子。U型网络结构由一个收缩路径和一个对称的扩展路径组成。收缩路径相当于编码器，与普通的卷积网络相同，通过一系列的卷积和池化下采样操作降低图像的分辨率，提取图像的特征；扩展路径与之相反，通过一系列的上采样操作和跳跃连接将收缩路径的浅层特征与扩展路径的深层特征拼接，最终达到任务的效果^[32]^。由于U Net具有跳跃连接的操作，在一定程度上缓解了信息丢失和长距离依赖捕捉不足的情况。U Net架构最初用于医学图像分割领域。但是后续出现了许多U Net变体，使得该架构在图像生成方面得到广泛应用。 </div> ## 2.3 可行性分析 ### 2.3.1 技术可行性 <div class \"text\"> 从早期以卷积神经网络(CNN)为核心的方法，到基于生成对抗网络(GAN)的方法，再到目前基于扩散模型和Transformer的融合技术，风格迁移技术呈现出多技术路线并行发展状况。 本文提出的基于MetaNet网络改进的图像风格迁移算法在技术层面具有较高的可行性。MetaNet网络基于元学习的技术框架，元级学习通过对大量的数据进行深度学习，能够识别出不同任务间的共性特征与模式，从而形成一套具有普适性的知识体系。当面对新的风格任务时，基本级利用元级所积累的知识快速生成适配该任务的图像转换网络参数，大大地提高了任务执行的效率与准确性。Munkhdalai等人关于Meta Networks的研究成果，有力地证实了元学习在小样本学习以及动态参数生成领域的显著成效，充分表明该技术框架在实际应用中的强大潜力与可行性。 风格图像经VGG预训练模型提取特征后，元学习器根据风格特征输出图像转换网络的权重参数，避免传统方法中针对每个风格单独训练的耗时问题。VGG预训练模型具有深度且结构稳定的卷积神经网络架构，在图像特征提取领域展现出卓越的技术可行性。它通过多层的卷积和池化操作，能够从风格图像中逐层抽象出从低级边缘到高级语义的丰富特征。大量公开数据集的预训练使得模型参数得到充分优化，对各类风格图像具有良好的泛化能力，可精准捕捉风格图像的关键特征。 本文改进的模型中还对图像转换网络进行优化，使用池化层代替原来的固定步幅卷积下采样避免信息丢失，使用双线性插值代替原来的固定缩放因子确保输入输出图像尺寸一致。另外元学习器还引入了注意力模块，提升对通道间依赖、长距离特征关联的建模能力，减少特征丢失。后续实验证明，本文改进模型推理效率高，满足实时需求，适合艺术创作、文创设计等场景。 </div> ### 2.3.2 经济可行性 <div class \"text\"> 本文提出的基于MetaNet网络改进的图像风格迁移算法初始硬件投入成本低、单图处理成本低、存储需求小，在经济层面同样具有较高的可行性。 在硬件支撑方面，本文改进模型的训练在海光32核CPU和16GB显存的DCU上完成。改进模型的元学习器仅含220M参数，模型空间占用846MB，可在主流的GPU(如RTX3090/4090)上部署推演。本文改进模型凭借较低的硬件资源需求，能够在实时交互场景中快速响应，具有良好的投入产出比。在软件环境方面，本文改进模型通过使用PyTorch开源深度学习框架以及广泛可用的GPU加速技术实现。PyTorch框架拥有丰富的工具包与便捷的操作接口，能够显著降低开发难度。 图像风格迁移技术可用于艺术创作、文创设计、影视特效、广告制作以及个性化内容生成等场景。许多在线平台与软件都纷纷提出了图像风格迁移功能，这些工具从免费到专业分层，覆盖艺术创作、电商设计、影视制作等场景。2019年，微软(亚洲)互联网工程院推出的人工智能绘画系统“微软小冰”具有独特的创作能力，它可以对已有的画面进行风格迁移或滤镜效果处理，最终生成100%原创的绘画作品。抖音、快手等短视频平台也推出了各种风格化特效，例如最近火热的AI漫画人脸特效。此外，“Snapchat”和“美图秀秀”也推出了各种美化照片的滤镜，让普通用户可以轻松参与图像风格化的创作^[33]^。阿里云提出的通义万相支持全局或局部风格化、线稿生图等功能，适合电商设计和文创领域。在游戏和电影创作领域，电影《梵高之眼》运用风格迁移算法，将梵高经典作品的绘画元素迁移到影片，成功呈现出独特的动画艺术效果。由此可见，图像风格化在商业领域有着大规模的落地应用，具有极高的商业价值。 </div> ## 2.4 本章小结 <div class \"text\"> 本章系统地介绍了图像风格迁移相关的理论基础、经典模型以及可行性分析。 在相关理论部分，首先介绍了卷积神经网络(CNN)的基本架构，包括卷积层、激活函数层、池化层和全连接层的功能与特性。接着介绍了两个经典的卷积神经网络模型：VGG和ResNet。VGG网络主要通过堆叠一系列的卷积层和池化层来构建深度网络，最后连接全连接层以完成目标任务。ResNet提出残差学习机制，通过快捷连接解决深层网络训练中的梯度消失与性能退化问题，使网络深度大幅增加。 在网络架构部分，介绍了编码器 解码器架构。该架构通过编码器将输入序列编码为中间向量，再由解码器逐步解析生成输出序列。以 U Net 为例，其对称结构结合跳跃连接，缓解了信息丢失问题，在医学图像分割和图像生成领域广泛应用。 在可行性分析部分，技术可行性上，基于MetaNet的改进算法利用元学习技术，结合VGG预训练模型提取风格特征，动态生成图像转换网络参数，避免了传统方法的重复训练问题。通过优化下采样和上采样操作、引入注意力模块，提升了模型效率与特征建模能力。经济可行性上，模型对硬件资源需求低，可在主流GPU上部署，结合PyTorch开源框架降低开发成本，且在艺术创作、影视特效、电商设计等场景具有广泛的商业应用价值，如微软小冰、阿里云通义万相等案例已验证其落地潜力。 本章从理论出发，了解图像风格迁移的核心技术，以及论述该技术的可行性，为后续研究提供了坚实的理论基础。 </div> [29] Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large Scale Image Recognition[J]. CoRR, 2014. [30] He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016 770 778. [31] Sutskever I, Vinyals O, Le Q V. Sequence to Sequence Learning with Neural Networks[J]. Advances in neural information processing systems, 2014, 27. [32] Ronneberger O, Fischer P, Brox T. U net Convolutional networks for biomedical image segmentation[C]//Medical image computing and computer assisted intervention–MICCAI 2015 18th international conference, Munich, Germany, October 5 9, 2015, proceedings, part III 18. Springer international publishing, 2015 234 241. [33] 张娜韦.基于深度学习的图像风格迁移方法研究[D].中国石油大学(北京),2023."}}
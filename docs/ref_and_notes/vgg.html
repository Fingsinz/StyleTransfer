<!DOCTYPE html>

<html lang="zh" id="ref_VGG" class="">


<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="keywords" content="VGG">
    
    
    <meta name="description" content="VGG文献及笔记">
    
    <meta name="generator" content="teedoc">
    <meta name="theme" content="teedoc-plugin-theme-default">
    
        
        <meta name="markdown-generator" content="teedoc-plugin-markdown-parser">
        
        <script>
MathJax = {"loader": {"load": ["output/svg"]}, "tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}, "svg": {"fontCache": "global"}};
</script>
        
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
        <script src="/StyleTransfer/static/js/theme_default/pre_main.js"></script>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/theme_default/prism.min.css" type="text/css"/>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/theme_default/viewer.min.css" type="text/css"/>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/theme_default/dark.css" type="text/css"/>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/theme_default/light.css" type="text/css"/>
        
        <script src="/StyleTransfer/static/js/theme_default/jquery.min.js"></script>
        
        <script src="/StyleTransfer/static/js/theme_default/split.js"></script>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/search/style.css" type="text/css"/>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/custom.css" type="text/css"/>
        
        <meta name="html-generator" content="teedoc-plugin-jupyter-notebook-parser">
        
    
    
    <title>VGG 网络 - Style Transfer - Fingsinz</title>
    
    <script type="text/javascript">js_vars = {}</script>
    <script type="text/javascript">metadata = {"tags": [], "date": "2025-03-05", "update": [], "ts": 1741104000, "author": "", "brief": "", "cover": "", "id": "ref_VGG"}</script>
</head>


<body class="type_doc">
    
    <div id="navbar">
        <div id="navbar_menu">
            <a class="site_title" href="/StyleTransfer/">
                
                
                    <h2>Style Transfer By Fingsinz</h2>
                
        </a>
            <a id="navbar_menu_btn"></a>
        </div>
        <div id="navbar_items">
            <div>
                <ul id="nav_left">
<li class=""><a  href="/StyleTransfer/supporting/">辅助材料</a></li>
<li class="active"><a  href="/StyleTransfer/ref_and_notes/">文献学习 & 笔记</a></li>
<li class=""><a  href="/StyleTransfer/paper/">论文正文</a></li>
</ul>

            </div>
            <div>
                <ul id="nav_right">
</ul>

                <ul class="nav_plugins"><li><a id="themes" class="light"></a></li></ul><ul class="nav_plugins"><li><a id="search"><span class="icon"></span><span class="placeholder">搜索</span>
                            <div id="search_hints">
                                <span id="search_input_hint">输入关键词，多关键词空格隔开</span>
                                <span id="search_loading_hint">正在加载，请稍候。。。</span>
                                <span id="search_download_err_hint">下载文件失败，请刷新重试或检查网络</span>
                                <span id="search_other_docs_result_hint">来自其它文档的结果</span>
                                <span id="search_curr_doc_result_hint">当前文档搜索结果</span>
                            </div></a></li></ul>
            </div>
        </div>
    </div>
    
    <div id="wrapper">
        <div id="sidebar_wrapper">
            <div id="sidebar">
                <div id="sidebar_title">
                    
                </div>
                <ul class="show">
<li class="not_active no_link"><a><span class="label">PyTorch 框架</span><span class="sub_indicator"></span></a><ul class="show">
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_install.html"><span class="label">PyTorch 环境安装</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_tensor.html"><span class="label">Tensor 张量</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_basic_workflow.html"><span class="label">PyTorch 基本工作流</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_classification.html"><span class="label">PyTorch 分类模型</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_computer_vision.html"><span class="label">PyTorch 中的计算机视觉</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_custom_datasets.html"><span class="label">PyTorch 中自定义数据集</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_modular.html"><span class="label">PyTorch 模块化</span><span class=""></span></a></li>
</ul>
</li>
<li class="active_parent no_link"><a><span class="label">理论学习</span><span class="sub_indicator"></span></a><ul class="show">
<li class="active with_link"><a href="/StyleTransfer/ref_and_notes/vgg.html"><span class="label">VGG 卷积网络</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/resnet.html"><span class="label">残差网络 ResNet</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/unet.html"><span class="label">U-Net 卷积网络</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/gan.html"><span class="label">生成对抗网络 GAN</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/cgan.html"><span class="label">cGAN 简介</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/patchgan.html"><span class="label">PatchGAN 到多尺度 PatchGAN</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">风格迁移实战</span><span class="sub_indicator"></span></a><ul class="show">
<li class="not_active no_link"><a><span class="label">Image Style Transfer Using Convolutional Neural Networks</span><span class=""></span></a></li>
</ul>
</li>
</ul>

            </div>
        </div>
        <div id="article">
            <div id="menu_wrapper">
                <div id="menu">
                </div>
            </div>
            <div id="content_wrapper">
                <div id="content_body">
                    <div id="article_head">
                        <div id="article_title">
                            
                            <h1>VGG 网络</h1>
                            
                        </div>
                        <div id="article_tags">
                            <ul>
                            
                            </ul>
                        </div>
                        <div id="article_info">
                        <div id="article_info_left">
                            <span class="article_author">
                                
                            </span>
                            
                                <span class="article_date" title="最后修改日期： 2025-03-05">
                                    2025-03-05
                                </span>
                            
                        </div>
                        <div id="article_info_right">
                            
                        </div>
                        </div>
                    </div>
                    <div id="article_tools">
                        <span></span>
                        <span id="toc_btn"></span>
                    </div>
                    <div id="update_history">
                        
                    </div>
                    <div id="article_content">
                        
                            <p><a href="https://arxiv.org/abs/1409.1556"  target="_blank">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></p>
<p><em>Simonyan K , Zisserman A .Very Deep Convolutional Networks for Large-Scale Image Recognition[J].Computer Science, 2014.DOI:10.48550/arXiv.1409.1556.</em></p>
<blockquote>
<p>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.</p>
</blockquote>
<p><strong>摘要</strong>：在这项工作中，我们研究了卷积网络深度对其在大规模图像识别设置中的准确性的影响。我们的主要贡献是使用具有非常小（3x3）卷积滤波器的架构对增加深度的网络进行全面评估，这表明通过将深度推至 16-19 权重层可以实现对现有技术配置的显着改进。这些发现是我们 2014 年 ImageNet 挑战赛提交的基础，我们的团队分别在本地化和分类轨道中获得了第一名和第二名。我们还表明，我们的方法可以很好地推广到其他数据集，从而获得最先进的结果。我们已经公开了两个性能最好的卷积神经网络模型，以促进对计算机视觉中深度视觉表示的进一步研究。</p>
<h2 id="VGG-%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84">VGG 网络架构</h2>
<p>架构设计创新：</p>
<ul>
<li>固定输入尺寸为 224 × 224，采用统一的小卷积核（3 × 3）和步长（1），通过填充保持空间分辨率，配合最大池化（2 × 2，步长 2）逐步下采样。</li>
<li>移除局部响应归一化（LRN），因其对性能提升有限且增加计算开销。</li>
<li>全连接层统一为 3 层（4096-4096-1000），最后一层为 SoftMax 分类器。</li>
</ul>
<h2 id="VGG-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE">VGG 网络配置</h2>
<p>翻译自论文中表格 1。</p>
<table style="text-align:center">
    <caption>ConvNet Configuration</caption>
    <tr>
        <th>A</th>
        <th>A-LRN</th>
        <th>B</th>
        <th>C</th>
        <th>D</th>
        <th>E</th>
    </tr>
    <tr>
        <td>11 层</td>
        <td>11 层</td>
        <td>13 层</td>
        <td>16 层</td>
        <td>16 层</td>
        <td>19 层</td>
    </tr>
    <tr>
        <td colspan="6">输入层 224 x 224 RGB 图像</td>
    </tr>
    <tr>
        <td>conv3-64</td>
        <td>conv3-64 <br /> <strong>LRN</strong> </td>
        <td>conv3-64 <br /> <strong>conv3-64</strong> </td>
        <td>conv3-64 <br /> conv3-64</td>
        <td>conv3-64 <br /> conv3-64</td>
        <td>conv3-64 <br /> conv3-64</td>
    </tr>
    <tr>
        <td colspan="6">最大池化</td>
    </tr>
    <tr>
        <td>conv3-128</td>
        <td>conv3-128</td>
        <td>conv3-128 <br /> <strong>conv3-128</strong> </td>
        <td>conv3-128 <br /> conv3-128 </td>
        <td>conv3-128 <br /> conv3-128 </td>
        <td>conv3-128 <br /> conv3-128 </td>
    </tr>
    <tr>
        <td colspan="6">最大池化</td>
    </tr>
    <tr>
        <td>conv3-256 <br /> conv3-256 </td>
        <td>conv3-256 <br /> conv3-256 </td>
        <td>conv3-256 <br /> conv3-256 </td>
        <td>conv3-256 <br /> conv3-256 <br /> <strong>conv1-256</strong></td>
        <td>conv3-256 <br /> conv3-256 <br /> <strong>conv3-256</strong></td>
        <td>conv3-256 <br /> conv3-256 <br /> conv3-256 <br /> <strong>conv3-256</strong></td>
    </tr>
    <tr>
        <td colspan="6">最大池化</td>
    </tr>
    <tr>
        <td>conv3-512 <br /> conv3-512 </td>
        <td>conv3-512 <br /> conv3-512 </td>
        <td>conv3-512 <br /> conv3-512 </td>
        <td>conv3-512 <br /> conv3-512 <br /> <strong>conv1-512</strong></td>
        <td>conv3-512 <br /> conv3-512 <br /> <strong>conv3-512</strong></td>
        <td>conv3-512 <br /> conv3-512 <br /> conv3-512 <br /> <strong>conv3-512</strong></td>
    </tr>
    <tr>
        <td colspan="6">最大池化</td>
    </tr>
    <tr>
        <td>conv3-512 <br /> conv3-512 </td>
        <td>conv3-512 <br /> conv3-512 </td>
        <td>conv3-512 <br /> conv3-512 </td>
        <td>conv3-512 <br /> conv3-512 <br /> <strong>conv1-512</strong></td>
        <td>conv3-512 <br /> conv3-512 <br /> <strong>conv3-512</strong></td>
        <td>conv3-512 <br /> conv3-512 <br /> conv3-512 <br /> <strong>conv3-512</strong></td>
    </tr>
    <tr>
        <td colspan="6">最大池化</td>
    </tr>
    <tr>
        <td colspan="6">全连接层（FC-4096）</td>
    </tr>
    <tr>
        <td colspan="6">全连接层（FC-4096）</td>
    </tr>
    <tr>
        <td colspan="6">全连接层（FC-1000）</td>
    </tr>
    <tr>
        <td colspan="6">soft-max</td>
    </tr>
</table>
<p>通过逐步增加卷积层数（11 层到 19 层），验证了网络深度的增加能显著提升分类精度。最佳模型（<strong>VGG-16</strong> 和 <strong>VGG-19</strong>）在 ImageNet 2014 挑战赛的分类和定位任务中分别取得第二和第一的成绩。</p>
<p>用三个 3 × 3 的卷积层代替一个 7 × 7 的卷积层：</p>
<ol>
<li>结合了三个非线性校正层而不是单个校正层，这使得决策函数更具区分性；</li>
<li>减少了参数的数量，三个 3 × 3 卷积层的叠加等效于一个 7 × 7 卷积层，但参数量减少 81%。</li>
<li>这可以看作是对 7 × 7 卷积滤波器施加正则化，迫使它们通过 3 × 3 滤波器（中间注入非线性）进行分解。</li>
</ol>
<p>1 × 1 卷积层（配置 C，表 1）的结合是一种在不影响卷积层感受野的情况下增加决策函数的非线性的方法。</p>
<h2 id="%E6%96%87%E4%B8%AD%E5%88%86%E7%B1%BB%E5%AE%9E%E9%AA%8C">文中分类实验</h2>
<p>实验证明，增加网络深度（16-19层）能显著提升模型表达能力。</p>
<ul>
<li>小卷积核的效益：通过叠加 3 × 3 卷积，在保持感受野的同时减少参数量，并引入更多非线性激活（ReLU），增强模型表达能力。</li>
<li>全卷积网络：测试时通过全卷积化处理任意尺寸输入，避免重复计算多个裁剪区域，显著提升效率。</li>
<li>多尺度策略：训练和测试时的尺度抖动有效提升模型对尺寸变化的适应能力。</li>
<li>参数共享：通过浅层网络预训练初始化深层网络的前几层和全连接层，加速收敛并缓解梯度不稳定问题。</li>
</ul>
<h2 id="%E4%BB%A3%E7%A0%81%E8%87%AA%E5%AE%9E%E7%8E%B0">代码自实现</h2>
<p>基于 VGG 网络的 CIFAR-10 分类实验代码在 <a href="https://github.com/Fingsinz/StyleTransfer/blob/main/src/01.ref_and_note/04.VGG.py"  target="_blank">Github</a>。</p>
<p>实验记录如下：</p>
<table>
    <tr>
        <td><img src="../static/images/VGG/fig1.png" /></td>
        <td><img src="../static/images/VGG/fig2.png" /></td>
    </tr>
</table>
<p>*<em>_my 表示为自实现，_py 表示调用 torchvision.models 的模型</em></p>
<details>
<summary>VGG16</summary>

<pre class="language-python"><code class="language-python">class VGG16(nn.Module):
    def __init__(self, in_channels=3, num_classes=10):
        super(VGG16, self).__init__()
        self.features = nn.Sequential(
           self._make_conv_block(in_channels, 64, 2),   # conv3-64 × 2
           nn.MaxPool2d(kernel_size=2, stride=2),       # maxpool

           self._make_conv_block(64, 128, 2),           # conv3-128 × 2
           nn.MaxPool2d(kernel_size=2, stride=2),       # maxpool

           self._make_conv_block(128, 256, 3),          # conv3-256 × 2
           nn.MaxPool2d(kernel_size=2, stride=2),       # maxpool

           self._make_conv_block(256, 512, 3),          # conv3-512 × 2
           nn.MaxPool2d(kernel_size=2, stride=2),       # maxpool

           self._make_conv_block(512, 512, 3),          # conv3-512 × 2
           nn.MaxPool2d(kernel_size=2, stride=2),       # maxpool
        )
        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))     # 全局平均池化
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),               # 全连接层（FC 1）
            nn.ReLU(inplace=True), 
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),                      # 全连接层（FC 2）
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, num_classes),               # 全连接层（FC 3）
        )                                               # nn.CrossEntropyLoss() 计算损失时隐式 soft-max

    def _make_conv_block(self, in_channels, out_channels, num_blocks, kernel_size=3, padding=1):
        layers = []
        for _ in range(num_blocks):
            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding))
            layers.append(nn.BatchNorm2d(out_channels))    # 每个卷积层后加入nn.BatchNorm2d，显著提升训练稳定性
            layers.append(nn.ReLU(inplace=True))
            in_channels = out_channels
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x
</code></pre>
</details>
<details>
<summary>VGG19</summary>

<pre class="language-python"><code class="language-python">class VGG19(nn.Module):
    def __init__(self, in_channels=3, num_classes=10):
        super(VGG19, self).__init__()
        self.features = nn.Sequential(
           self._make_conv_block(in_channels, 64, 2),   # conv3-64 × 2
           nn.MaxPool2d(kernel_size=2, stride=2),       # maxpool

           self._make_conv_block(64, 128, 2),           # conv3-128 × 2
           nn.MaxPool2d(kernel_size=2, stride=2),       # maxpool

           self._make_conv_block(128, 256, 4),          # conv3-256 × 4
           nn.MaxPool2d(kernel_size=2, stride=2),       # maxpool

           self._make_conv_block(256, 512, 4),          # conv3-512 × 4
           nn.MaxPool2d(kernel_size=2, stride=2),       # maxpool

           self._make_conv_block(512, 512, 4),          # conv3-512 × 4
           nn.MaxPool2d(kernel_size=2, stride=2),       # maxpool
        )
        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))     # 全局平均池化
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),               # FC 1
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),                      # FC 2
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, num_classes),               # FC 3
        )

    def _make_conv_block(self, in_channels, out_channels, num_blocks, kernel_size=3, padding=1):
        layers = []
        for _ in range(num_blocks):
            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding))
            layers.append(nn.BatchNorm2d(out_channels))     # 每个卷积层后加入nn.BatchNorm2d，显著提升训练稳定性
            layers.append(nn.ReLU(inplace=True))
            in_channels = out_channels
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x
</code></pre>
</details>

                        
                    </div>
                </div>
                <div id="previous_next">
                    <div id="previous">
                        
                        <a href="/StyleTransfer/ref_and_notes/pytorch_modular.html">
                            <span class="icon"></span>
                            <span class="label">PyTorch 模块化</span>
                        </a>
                        
                    </div>
                    <div id="next">
                        
                        <a href="/StyleTransfer/ref_and_notes/resnet.html">
                            <span class="label">残差网络 ResNet</span>
                            <span class="icon"></span>
                        </a>
                        
                    </div>
                </div>
                <div id="comments-container"></div>
            </div>
            <div id="toc_wrapper">
                <div id="toc">
                    <div id="toc_content">
                            
                    </div>
                </div>
            </div>
        </div>
    </div>
    <a id="to_top" href="#"></a>
    <div id="doc_footer">
        <div id="footer">
            <div id="footer_top">
                <ul>
<li><a></a><ul><li><a target="_blank" href="/StyleTransfer/#"></a></li>
</ul>
</li>
</ul>

            </div>
            <div id="footer_bottom">
                <ul>
<li><a target="_blank" href="https://github.com/teedoc/teedoc">Generated by teedoc - Fingsinz - 2024.12.29</a></li>
</ul>

            </div>
        </div>
    </div>
    
        <script src="/StyleTransfer/teedoc-plugin-markdown-parser/mermaid.min.js"></script>
    
        <script>mermaid.initialize({startOnLoad:true});</script>
    
        <script src="/StyleTransfer/static/js/theme_default/tocbot.min.js"></script>
    
        <script src="/StyleTransfer/static/js/theme_default/main.js"></script>
    
        <script src="/StyleTransfer/static/js/theme_default/viewer.min.js"></script>
    
        <script src="/StyleTransfer/static/css/theme_default/prism.min.js"></script>
    
        <script src="/StyleTransfer/static/js/search/search_main.js"></script>
    
        <script src="/StyleTransfer/static/js/custom.js"></script>
    
</body>

</html>
<!DOCTYPE html>

<html lang="zh" id="cGAN" class="">


<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="keywords" content="cGAN">
    
    
    <meta name="description" content="cGAN 简介">
    
    <meta name="generator" content="teedoc">
    <meta name="theme" content="teedoc-plugin-theme-default">
    
        
        <meta name="markdown-generator" content="teedoc-plugin-markdown-parser">
        
        <script>
MathJax = {"loader": {"load": ["output/svg"]}, "tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}, "svg": {"fontCache": "global"}};
</script>
        
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
        <script src="/StyleTransfer/static/js/theme_default/pre_main.js"></script>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/theme_default/prism.min.css" type="text/css"/>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/theme_default/viewer.min.css" type="text/css"/>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/theme_default/dark.css" type="text/css"/>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/theme_default/light.css" type="text/css"/>
        
        <script src="/StyleTransfer/static/js/theme_default/jquery.min.js"></script>
        
        <script src="/StyleTransfer/static/js/theme_default/split.js"></script>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/search/style.css" type="text/css"/>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/custom.css" type="text/css"/>
        
        <meta name="html-generator" content="teedoc-plugin-jupyter-notebook-parser">
        
    
    
    <title>cGAN：条件 GAN - Style Transfer - Fingsinz</title>
    
    <script type="text/javascript">js_vars = {}</script>
    <script type="text/javascript">metadata = {"tags": [], "date": "2025-03-06", "update": [], "ts": 1741190400, "author": "", "brief": "", "cover": "", "id": "cGAN"}</script>
</head>


<body class="type_doc">
    
    <div id="navbar">
        <div id="navbar_menu">
            <a class="site_title" href="/StyleTransfer/">
                
                
                    <h2>Style Transfer By Fingsinz</h2>
                
        </a>
            <a id="navbar_menu_btn"></a>
        </div>
        <div id="navbar_items">
            <div>
                <ul id="nav_left">
<li class=""><a  href="/StyleTransfer/supporting/">辅助材料</a></li>
<li class="active"><a  href="/StyleTransfer/ref_and_notes/">文献学习 & 笔记</a></li>
<li class=""><a  href="/StyleTransfer/paper/">论文正文</a></li>
</ul>

            </div>
            <div>
                <ul id="nav_right">
</ul>

                <ul class="nav_plugins"><li><a id="themes" class="light"></a></li></ul><ul class="nav_plugins"><li><a id="search"><span class="icon"></span><span class="placeholder">搜索</span>
                            <div id="search_hints">
                                <span id="search_input_hint">输入关键词，多关键词空格隔开</span>
                                <span id="search_loading_hint">正在加载，请稍候。。。</span>
                                <span id="search_download_err_hint">下载文件失败，请刷新重试或检查网络</span>
                                <span id="search_other_docs_result_hint">来自其它文档的结果</span>
                                <span id="search_curr_doc_result_hint">当前文档搜索结果</span>
                            </div></a></li></ul>
            </div>
        </div>
    </div>
    
    <div id="wrapper">
        <div id="sidebar_wrapper">
            <div id="sidebar">
                <div id="sidebar_title">
                    
                </div>
                <ul class="show">
<li class="not_active no_link"><a><span class="label">PyTorch 框架</span><span class="sub_indicator"></span></a><ul class="show">
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_install.html"><span class="label">PyTorch 环境安装</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_tensor.html"><span class="label">Tensor 张量</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_basic_workflow.html"><span class="label">PyTorch 基本工作流</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_classification.html"><span class="label">PyTorch 分类模型</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_computer_vision.html"><span class="label">PyTorch 中的计算机视觉</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_custom_datasets.html"><span class="label">PyTorch 中自定义数据集</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_modular.html"><span class="label">PyTorch 模块化</span><span class=""></span></a></li>
</ul>
</li>
<li class="active_parent no_link"><a><span class="label">理论学习</span><span class="sub_indicator"></span></a><ul class="show">
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/vgg.html"><span class="label">VGG 卷积网络</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/resnet.html"><span class="label">ResNet：残差网络</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/unet.html"><span class="label">U-Net 卷积网络</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/gan.html"><span class="label">GAN：生成对抗网络</span><span class=""></span></a></li>
<li class="active with_link"><a href="/StyleTransfer/ref_and_notes/cgan.html"><span class="label">cGAN：条件 GAN</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/patchgan.html"><span class="label">PatchGAN 到多尺度 PatchGAN</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/cyclegan.html"><span class="label">CycleGAN：循环GAN</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/metanet.html"><span class="label">Meta Networks</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">风格迁移实战</span><span class="sub_indicator"></span></a><ul class="show">
<li class="not_active no_link"><a><span class="label">Image Style Transfer Using Convolutional Neural Networks</span><span class=""></span></a></li>
</ul>
</li>
</ul>

            </div>
        </div>
        <div id="article">
            <div id="menu_wrapper">
                <div id="menu">
                </div>
            </div>
            <div id="content_wrapper">
                <div id="content_body">
                    <div id="article_head">
                        <div id="article_title">
                            
                            <h1>cGAN：条件 GAN</h1>
                            
                        </div>
                        <div id="article_tags">
                            <ul>
                            
                            </ul>
                        </div>
                        <div id="article_info">
                        <div id="article_info_left">
                            <span class="article_author">
                                
                            </span>
                            
                                <span class="article_date" title="最后修改日期： 2025-03-06">
                                    2025-03-06
                                </span>
                            
                        </div>
                        <div id="article_info_right">
                            
                        </div>
                        </div>
                    </div>
                    <div id="article_tools">
                        <span></span>
                        <span id="toc_btn"></span>
                    </div>
                    <div id="update_history">
                        
                    </div>
                    <div id="article_content">
                        
                            <p><a href="https://arxiv.org/abs/1411.1784"  target="_blank">Conditional Generative Adversarial Nets</a></p>
<p><em>Mirza M , Osindero S .Conditional Generative Adversarial Nets[J].Computer Science, 2014:2672-2680.DOI:10.48550/arXiv.1411.1784.</em></p>
<p><a href="https://ieeexplore.ieee.org/document/8100115"  target="_blank">Image-to-Image Translation with Conditional Adversarial Networks</a></p>
<p><em>Isola P , Zhu J Y , Zhou T ,et al.Image-to-Image Translation with Conditional Adversarial Networks[C]//IEEE Conference on Computer Vision &amp; Pattern Recognition.IEEE, 2016.DOI:10.1109/CVPR.2017.632.</em></p>
<h2 id="cGAN">cGAN</h2>
<p>条件生成对抗网络（Conditional Generative Adversarial Networks, cGAN）是生成对抗网络（GAN）的一种扩展形式，通过引入 <strong>条件信息</strong>（如标签、文本、图像等），使生成器和判别器能够根据特定条件生成或判别数据。</p>
<ul>
<li>核心思想是通过条件约束，控制生成内容的属性和结构，从而解决普通 GAN 生成结果不可控的问题。</li>
</ul>
<h3 id="cGAN-%E7%9A%84%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86">cGAN 的核心原理</h3>
<p>条件信息的引入：</p>
<ul>
<li>生成器（Generator）：输入不仅包含随机噪声 $z$，还包括条件信息 $c$（如类别标签、另一张图像等）。生成器需根据 $c$ 生成对应的数据 $G(z|c)$。</li>
<li>判别器（Discriminator）：输入包含真实数据 $x$ 或生成数据 $G(z|c)$，同时结合条件信息 $c$。判别器的任务是判断数据是否真实且与条件匹配，即 $D(x|c)$ 或 $D(G(z|c)|c)$。</li>
</ul>
<p>cGAN 的损失函数在普通 GAN 的基础上加入了条件约束：</p>
$$
\mathcal{L}_{cGAN}(G,D) = \mathbb{E}_{x,c}[\log D(x|c)] + \mathbb{E}_{z,c}[\log(1 - D(G(z|c)|c)]]
$$<ul>
<li>生成器 $G$ 的目标：生成与条件 $c$ 匹配的逼真数据，使 $D(G(z|c)|c)$ 趋近于1。</li>
<li>判别器 $D$ 的目标：区分真实数据 $x|c$ 和生成数据 $G(z|c)|c$。</li>
</ul>
<h3 id="cGAN-%E5%AF%B9%E6%AF%94%E6%99%AE%E9%80%9A-GAN">cGAN 对比普通 GAN</h3>
<table>
<thead>
<tr>
  <th style="text-align:center">特性</th>
  <th style="text-align:center">普通GAN</th>
  <th style="text-align:center">条件GAN（cGAN）</th>
</tr>
</thead>
<tbody>
<tr>
  <td style="text-align:center">输入</td>
  <td style="text-align:center">随机噪声 $z$</td>
  <td style="text-align:center">随机噪声 $z$ + 条件信息 $c$</td>
</tr>
<tr>
  <td style="text-align:center">生成控制</td>
  <td style="text-align:center">完全随机</td>
  <td style="text-align:center">通过条件 $c$ 控制生成内容</td>
</tr>
<tr>
  <td style="text-align:center">应用场景</td>
  <td style="text-align:center">无约束生成（如随机图像生成）</td>
  <td style="text-align:center">需特定条件生成（如根据文本生成图像）</td>
</tr>
<tr>
  <td style="text-align:center">典型任务</td>
  <td style="text-align:center">生成随机人脸、艺术品</td>
  <td style="text-align:center">图像到图像转换（pix2pix）、文本到图像生成、可控生成（如风格迁移）、图像修复、图像翻译（如黑白→彩色）</td>
</tr>
</tbody>
</table>
<h3 id="%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B">代码示例</h3>

<pre class="language-python"><code class="language-python"># 生成器（U-Net结构为例）
class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        # 输入：噪声z + 条件图像c
        self.encoder = Encoder()  # 下采样层
        self.decoder = Decoder()  # 上采样层（含跳跃连接）

    def forward(self, z, c):
        x = torch.cat([z, c], dim=1)  # 拼接噪声和条件
        return self.decoder(self.encoder(x))

# 判别器（PatchGAN结构为例）
class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        # 输入：真实/生成图像 + 条件图像c
        self.conv_blocks = nn.Sequential(
            nn.Conv2d(3 + 3, 64, kernel_size=4, stride=2),  # 假设条件c是3通道图像
            nn.LeakyReLU(0.2),
            # 更多卷积层...
        )

    def forward(self, x, c):
        x = torch.cat([x, c], dim=1)  # 拼接图像和条件
        return self.conv_blocks(x)
</code></pre>
<h2 id="Image-to-Image-Translation-with-cGAN">Image-to-Image Translation with cGAN</h2>
<p>本文提出使用条件 GANs 作为通用解决方案，通过对抗训练自动学习任务相关的损失函数，避免人工设计损失函数的复杂性。</p>
<blockquote>
<p><em>条件生成对抗网络（Conditional Generative Adversarial Networks, cGANs）最初由 Mehdi Mirza 和 Simon Osindero 在 2014 年的论文 《Conditional Generative Adversarial Nets》 中提出。这篇论文首次将条件信息（如类别标签或辅助数据）引入 GAN 框架，使生成器和判别器能够基于特定条件进行训练和生成。</em></p>
</blockquote>
<p>条件 GANs 的优势：</p>
<ul>
<li>条件输入：生成器和判别器均以输入图像为条件，确保输出与输入的结构对齐（<em>如下图输入边缘图生成对应照片案例中，生成器和判别器都观察输入的边缘</em>）。</li>
</ul>
<p><img src="../static/images/cGAN/fig1.png" alt="" /></p>
<ul>
<li>结合L1损失：在对抗损失基础上引入 L1 损失，保留低频信息（如整体布局），而对抗损失负责高频细节（如纹理和锐度），解决传统 L2 损失导致的模糊问题。</li>
</ul>
<h3 id="%E6%96%B9%E6%B3%95%E7%BB%86%E8%8A%82">方法细节</h3>
<h4 id="%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0">目标函数</h4>
<p>cGAN 的目标可以表示为：</p>
$$
\mathcal{L}_{cGAN}(G,D) = \mathbb{E}_{x,c}[\log D(x|c)] + \mathbb{E}_{z,c}[\log(1 - D(G(z|c)|c)]]
$$<p>目标函数上，总损失函数为对抗损失与 L1 损失的加权和：</p>
$$
G^* = \arg\min_G \max_D \mathcal{L}_{cGAN}(G, D) + \lambda \mathcal{L}_{L1}(G)
$$<ul>
<li>对抗损失：迫使生成器输出逼真的图像，判别器区分生成图像与真实图像。</li>
<li>L1 损失：约束生成图像与真实图像在像素级的一致性，减少模糊（所以不使用 L2 损失）。</li>
</ul>
<p>随机性的引入：生成器的输入包含随机噪声（通过 Dropout 实现），但实验表明生成结果仍具有较低随机性。这表明当前方法在建模条件分布的多样性方面仍有改进空间。</p>
<h4 id="%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84">网络架构</h4>
<p>在网络架构上：</p>
<ul>
<li>生成器：带跳跃连接。</li>
<li>判别器（马尔可夫随机场）：<a href="./patchgan.html"  >PatchGAN</a>，尝试对图像中的每个 N × N 块进行真假分类。在图像上卷积运行这个鉴别器，平均所有响应来提供 D 的最终输出。</li>
</ul>
<h4 id="%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%8E%A8%E6%BC%94%E8%BF%87%E7%A8%8B">训练与推演过程</h4>
<p>训练中：</p>
<ul>
<li>遵循 GAN 中的优化算法，交替 $D$ 和 $G$ 的 step 训练。</li>
<li>在优化 $D$ 时将目标函数除以 2，减慢 $D$ 相对于 $G$ 学习的速率。</li>
<li>使用小批量 SGD 并应用 Adam 求解器，学习率为 0.0002，动量参数 $\beta_1 = 0.5$，$\beta_2 = 0.999$。</li>
</ul>
<p>推演时：</p>
<ul>
<li>与训练阶段相同的方式运行生成器。</li>
</ul>
<h3 id="%E5%AE%9E%E9%AA%8C%E4%B8%8E%E9%AA%8C%E8%AF%81">实验与验证</h3>
<p>论文的第四部分通过广泛的实验验证了条件生成对抗网络（cGAN）在多种图像到图像转换任务中的有效性和通用性.</p>
<h4 id="%E5%AE%9E%E9%AA%8C%E4%BB%BB%E5%8A%A1%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%9B%86">实验任务与数据集</h4>
<p>作者在以下任务中测试了框架的通用性，涵盖图形和视觉任务：</p>
<ul>
<li>语义标签 ↔ 照片（Cityscapes数据集）：将语义分割标签转换为真实街景照片。</li>
<li>建筑标签 → 照片（CMP Facades数据集）：将建筑立面线框图转换为真实建筑照片。</li>
<li>地图 ↔ 航拍图（Google Maps数据）：实现卫星地图与航拍图的双向转换。</li>
<li>黑白 → 彩色（ImageNet数据）：为灰度图像自动着色。</li>
<li>边缘 → 照片（UT Zappos50K、Amazon Handbag数据）：从边缘图生成鞋类、手提包等实物图像。</li>
<li>草图 → 照片（人类手绘草图）：扩展边缘到照片的模型至非结构化输入。</li>
<li>白天 → 夜晚（Webcam数据）：转换场景光照条件。</li>
<li>热成像 → 彩色照片（多光谱行人检测数据集）：融合热成像与可见光信息。</li>
<li>图像修复（Paris StreetView数据集）：补全图像中缺失的像素区域。</li>
</ul>
<p>每个任务均使用相同架构（U-Net 生成器 + PatchGAN 判别器）和损失函数（L1 + cGAN），仅更换训练数据。<a href="https://phillipi.github.io/pix2pix/"  target="_blank">跳转链接</a></p>
<h4 id="%E6%95%B0%E6%8D%AE%E9%9C%80%E6%B1%82%E4%B8%8E%E8%AE%AD%E7%BB%83%E6%95%88%E7%8E%87">数据需求与训练效率</h4>
<p>小数据集表现：</p>
<ul>
<li>建筑标签任务仅需400张图像，训练时间不到2小时（单块Titan X GPU）。</li>
<li>昼夜转换任务使用91个摄像头的图像，训练17个周期即收敛。</li>
</ul>
<p>推理速度：</p>
<ul>
<li>所有模型在GPU上运行时间均小于1秒，支持实时应用。</li>
</ul>
<h4 id="%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95">评估方法</h4>
<p>(1) AMT 感知实验</p>
<ul>
<li>设计：通过 Amazon Mechanical Turk 平台进行“真实 vs. 生成”的二选一测试，参与者需在 1 秒内观察图像后判断真伪。</li>
<li>结果：<ul>
<li>地图→航拍图任务中，18.9%的生成图像被误认为真实（显著优于L1基线）。</li>
<li>航拍图→地图任务中，生成图像仅6.1%被误判（与L1基线无显著差异），因地图的几何结构更易暴露细节错误。</li>
</ul>
</li>
</ul>
<p>(2) FCN-score</p>
<ul>
<li>设计：使用预训练的FCN-8s模型（在Cityscapes上训练）对生成图像进行语义分割，计算分割精度（像素准确率、类别准确率、IoU）。</li>
<li>意义：衡量生成图像是否保留了输入标签的语义结构。</li>
<li>关键结果：<ul>
<li>L1 + cGAN 组合在Cityscapes任务中取得最高分数（像素准确率66%，接近真实数据的80%）。</li>
<li>单独使用 L1 损失会导致模糊，单独使用cGAN则可能生成结构错误但逼真的图像。</li>
</ul>
</li>
</ul>
<h4 id="%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E5%88%86%E6%9E%90">目标函数分析</h4>
<p>通过消融实验验证各损失组件的贡献：</p>
<ul>
<li>L1 损失：强制像素级匹配，减少模糊但导致色彩单调。</li>
<li>cGAN 损失：提升图像逼真性和高频细节（如边缘锐利、色彩丰富），但可能引入结构错误。</li>
<li>L1 + cGAN：结合二者优势，在逼真性和结构准确性间取得平衡。</li>
</ul>
<h4 id="%E7%94%9F%E6%88%90%E5%99%A8%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90">生成器架构分析</h4>
<p><img src="../static/images/cGAN/fig2.png" alt="" /></p>
<ul>
<li>U-Net vs. 编码器-解码器：<ul>
<li>U-Net 通过跳跃连接保留低级特征（如边缘位置），在图像着色等任务中显著优于普通编码器-解码器。</li>
<li>即使仅用 L1 损失训练，U-Net 仍优于编码器-解码器，证明跳跃连接对信息传递的关键作用。</li>
</ul>
</li>
</ul>
<h4 id="PatchGAN-%E5%B0%BA%E5%AF%B8%E5%88%86%E6%9E%90">PatchGAN 尺寸分析</h4>
<p><img src="../static/images/cGAN/fig3.png" alt="" /></p>
<p><em>Patch 大小的影响。对于不同的损失函数，输出中的不确定性表现不同。在 L1 下，不确定区域变得模糊和去饱和。1x1 PixelGAN 鼓励更大的色彩多样性，但对空间统计没有影响。16x16 PatchGAN 创建了局部清晰的结果，但也导致了超出其可观察范围的平铺伪影。70×70 PatchGAN强制输出在空间和光谱（色彩）维度上都是清晰的，即使不正确。完整的 286×286 ImageGAN 生成的结果在视觉上与 70×70 PatchGAN 相似，但根据FCN评分指标，质量略低。请参阅 <a href="https://phillipi.github.io/pix2pix/"  target="_blank">https://phillipi.github.io/pix2pix/</a> 了解更多示例。</em></p>
<p>测试不同感受野的判别器：</p>
<ul>
<li>1×1 PixelGAN：仅提升色彩多样性，对空间结构无影响。</li>
<li>16×16 PatchGAN：生成局部锐利图像，但出现拼贴伪影。</li>
<li>70×70 PatchGAN：最佳平衡，生成全局一致且细节清晰的图像。</li>
<li>286×286 ImageGAN（全图判别器）：参数量大、训练困难，且 FCN-score 下降。</li>
</ul>
<h4 id="%E5%85%A8%E5%8D%B7%E7%A7%AF%E6%89%A9%E5%B1%95%E6%80%A7%E6%B5%8B%E8%AF%95">全卷积扩展性测试</h4>
<p>PatchGAN 固定大小的 Patch 可以应用于任意大的图像。如在 256×256 分辨率训练生成器，直接应用于 512×512 图像。结果是生成高分辨率图像时仍保持质量，证明框架的扩展性。</p>
<h4 id="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1">语义分割任务</h4>
<p>实验设计：将 cGAN 应用于照片→语义标签的逆任务。<br />
结果：</p>
<ul>
<li>仅用 cGAN（无L1）可生成粗略标签，但准确率低于 L1 回归。</li>
<li>作者认为，结构化输出任务（如分割）因目标明确，更适合传统回归损失。</li>
</ul>
<h4 id="%E5%A4%B1%E8%B4%A5%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90">失败案例分析</h4>
<p><img src="../static/images/cGAN/fig4.png" alt="" /></p>
<p><em>每对图像中，左侧为输入，右侧为输出。</em></p>
<p>常见问题：</p>
<ul>
<li>输入稀疏或异常时，生成器产生伪影（如缺失边缘的区域）。</li>
<li>对非常规输入（如抽象草图）的泛化能力有限。</li>
</ul>
<h3 id="pix2pix-%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B">pix2pix 代码示例</h3>
<p><a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py"  target="_blank">Github</a></p>

<pre class="language-python"><code class="language-python">class Pix2PixModel(BaseModel):
    # ... 省略 ...
    def __init__(self, opt):
        BaseModel.__init__(self, opt)

        # 指定要打印的训练损失。训练/测试脚本将调用&lt;BaseModel.get_current_losses&gt;
        self.loss_names = ['G_GAN', 'G_L1', 'D_real', 'D_fake']

        # 指定要保存/显示的图像。训练/测试脚本将调用&lt;BaseModel.get_current_visuals&gt;
        self.visual_names = ['real_A', 'fake_B', 'real_B']

        # 指定要保存到磁盘的模型。训练/测试脚本将调用&lt;BaseModel.save_networks&gt;和&lt;BaseModel.load_networks&gt;
        if self.isTrain:
            self.model_names = ['G', 'D']
        else:  # 在测试期间，只加载G
            self.model_names = ['G']

        # 定义网络（生成器和鉴别器）
        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,
                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)
        if self.isTrain:
            # 定义一个鉴别器；条件gan需要同时获取输入和输出图像；因此，D 的 channels = input_nc + output_nc
            self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,
                                          opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)

        if self.isTrain:
            # 定义损失函数
            self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)
            self.criterionL1 = torch.nn.L1Loss()
            # 初始化优化器；优化器将由函数自动创建&lt;BaseModel.setup&gt;。
            self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
            self.optimizers.append(self.optimizer_G)
            self.optimizers.append(self.optimizer_D)

    # ... 省略 ...

    def forward(self):
        self.fake_B = self.netG(self.real_A)  # G(A)
</code></pre>
<h4 id="%E5%88%A4%E5%88%AB%E5%99%A8%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88%3Ccode%3Ebackward_D%3C/code%3E%EF%BC%89">判别器的反向传播（<code>backward_D</code>）</h4>
<ol>
<li><p>生成假样本输入判别器</p>
<ul>
<li>生成器生成假图像 <code>fake_B</code>，将其与输入图像 <code>real_A</code> 拼接为 <code>fake_AB</code>。</li>
<li>通过 <code>fake_AB.detach()</code> 切断梯度回传，防止生成器参数在判别器训练时被更新。</li>
<li>判别器对假样本的预测结果 <code>pred_fake</code> 与标签 <code>False</code> 计算损失 <code>loss_D_fake</code>。</li>
</ul>
</li>
<li><p>处理真实样本</p>
<ul>
<li>将真实图像对 <code>real_A</code> 和 <code>real_B</code> 拼接为 <code>real_AB</code>。</li>
<li>判别器对真实样本的预测结果 <code>pred_real</code> 与标签 <code>True</code> 计算损失 <code>loss_D_real</code>。</li>
</ul>
</li>
<li><p>计算总损失并反向传播</p>
<ul>
<li>总损失为真假样本损失的平均值：$\text{loss}_{\text{D}} = (\text{loss}_{\text{D_fake}} + \text{loss}_{\text{D_real}}) / 2$</li>
<li>执行 <code>loss_D.backward()</code> 计算梯度，通过 <code>optimizer_D.step()</code> 更新判别器参数。</li>
</ul>
</li>
</ol>

<pre class="language-python"><code class="language-python">    def backward_D(self):
        &quot;&quot;&quot;计算鉴别器的 GAN 损失&quot;&quot;&quot;
        # Fake；通过分离 fake_B 来停止对生成器的反向传播
        # 使用条件 GAN，需要将输入和输出都提供给网络
        fake_AB = torch.cat((self.real_A, self.fake_B), 1)
        pred_fake = self.netD(fake_AB.detach())
        self.loss_D_fake = self.criterionGAN(pred_fake, False)
        # Real
        real_AB = torch.cat((self.real_A, self.real_B), 1)
        pred_real = self.netD(real_AB)
        self.loss_D_real = self.criterionGAN(pred_real, True)
        # 结合损失和计算梯度
        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5
        self.loss_D.backward()
</code></pre>
<h4 id="%E7%94%9F%E6%88%90%E5%99%A8%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88%3Ccode%3Ebackward_G%3C/code%3E%EF%BC%89">生成器的反向传播（<code>backward_G</code>）</h4>
<ol>
<li><p>对抗损失（GAN Loss）</p>
<ul>
<li>将生成的假图像 <code>fake_B</code> 与输入图像 <code>real_A</code> 拼接为 <code>fake_AB</code>，输入判别器得到预测结果 <code>pred_fake</code>。</li>
<li>计算对抗损失 <code>loss_G_GAN</code>，目标是让判别器认为生成的图像为真（标签 <code>True</code>）。</li>
</ul>
</li>
<li><p>L1 重建损失</p>
<ul>
<li>计算生成图像 <code>fake_B</code> 与真实图像 <code>real_B</code> 的像素级 L1 损失 <code>loss_G_L1</code>，乘以权重系数 <code>lambda_L1</code>（通过 <code>opt.lambda_L1</code> 控制）。</li>
</ul>
</li>
<li><p>计算总损失并反向传播</p>
<ul>
<li>总损失为对抗损失与L1损失之和：$\text{loss}_{\text{G}} = \text{loss}_{\text{G_GAN}} + \text{loss}_{\text{G_L1}}$</li>
<li>执行 <code>loss_G.backward()</code> 计算梯度，通过 <code>optimizer_G.step()</code> 更新生成器参数。</li>
</ul>
</li>
</ol>

<pre class="language-python"><code class="language-python">    def backward_G(self):
        &quot;&quot;&quot;计算生成器的 GAN 和 L1 损失&quot;&quot;&quot;
        # 1. G(A) 应该骗过判别器
        fake_AB = torch.cat((self.real_A, self.fake_B), 1)
        pred_fake = self.netD(fake_AB)
        self.loss_G_GAN = self.criterionGAN(pred_fake, True)
        # 2. G(A) = B
        self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1
        # 结合损失并计算梯度
        self.loss_G = self.loss_G_GAN + self.loss_G_L1
        self.loss_G.backward()
</code></pre>
<h4 id="%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%EF%BC%88%3Ccode%3Eoptimize_parameters%3C/code%3E%EF%BC%89">训练流程（<code>optimize_parameters</code>）</h4>
<ol>
<li><p>前向传播生成假图像：<code>self.forward()</code> 调用生成器生成 <code>fake_B</code>。</p>
</li>
<li><p>更新判别器：</p>
<ul>
<li>解冻判别器参数（<code>set_requires_grad(self.netD, True)</code>）。</li>
<li>清零梯度（<code>optimizer_D.zero_grad()</code>）。</li>
<li>计算判别器损失并反向传播（<code>backward_D()</code>）。</li>
<li>更新参数（<code>optimizer_D.step()</code>）。</li>
</ul>
</li>
<li><p>更新生成器：</p>
<ul>
<li>冻结判别器参数（<code>set_requires_grad(self.netD, False)</code>）。</li>
<li>清零梯度（<code>optimizer_G.zero_grad()</code>）。</li>
<li>计算生成器损失并反向传播（<code>backward_G()</code>）。</li>
<li>更新参数（<code>optimizer_G.step()</code>）。</li>
</ul>
</li>
</ol>

<pre class="language-python"><code class="language-python">    def optimize_parameters(self):
        self.forward()                   # 计算生成器生成的假图像: G(A)
        # 更新 D
        self.set_requires_grad(self.netD, True)  # 启用 D 的反向传播
        self.optimizer_D.zero_grad() 
        self.backward_D()
        self.optimizer_D.step()
        # 更新 G
        self.set_requires_grad(self.netD, False) # D 在更新 G 时不需要梯度
        self.optimizer_G.zero_grad()
        self.backward_G()
        self.optimizer_G.step()
</code></pre>
<h3 id="%E5%B1%80%E9%99%90%E6%80%A7%E4%B8%8E%E5%90%AF%E7%A4%BA">局限性与启示</h3>
<ul>
<li>随机性不足：生成结果偏向确定性，难以建模多模态输出（如同一输入对应多种合理输出）。</li>
<li>复杂任务表现：在高度结构化任务（如语义分割）中，cGANs 效果不及纯 L1 回归，表明对抗训练更适用于需细节生成的图形任务。</li>
<li>社区应用：开源代码（pix2pix）被广泛用于艺术创作（如草图转肖像、背景去除），验证了其易用性和扩展性。</li>
</ul>

                        
                    </div>
                </div>
                <div id="previous_next">
                    <div id="previous">
                        
                        <a href="/StyleTransfer/ref_and_notes/gan.html">
                            <span class="icon"></span>
                            <span class="label">GAN：生成对抗网络</span>
                        </a>
                        
                    </div>
                    <div id="next">
                        
                        <a href="/StyleTransfer/ref_and_notes/patchgan.html">
                            <span class="label">PatchGAN 到多尺度 PatchGAN</span>
                            <span class="icon"></span>
                        </a>
                        
                    </div>
                </div>
                <div id="comments-container"></div>
            </div>
            <div id="toc_wrapper">
                <div id="toc">
                    <div id="toc_content">
                            
                    </div>
                </div>
            </div>
        </div>
    </div>
    <a id="to_top" href="#"></a>
    <div id="doc_footer">
        <div id="footer">
            <div id="footer_top">
                <ul>
<li><a></a><ul><li><a target="_blank" href="/StyleTransfer/#"></a></li>
</ul>
</li>
</ul>

            </div>
            <div id="footer_bottom">
                <ul>
<li><a target="_blank" href="https://github.com/teedoc/teedoc">Generated by teedoc - Fingsinz - 2024.12.29</a></li>
</ul>

            </div>
        </div>
    </div>
    
        <script src="/StyleTransfer/teedoc-plugin-markdown-parser/mermaid.min.js"></script>
    
        <script>mermaid.initialize({startOnLoad:true});</script>
    
        <script src="/StyleTransfer/static/js/theme_default/tocbot.min.js"></script>
    
        <script src="/StyleTransfer/static/js/theme_default/main.js"></script>
    
        <script src="/StyleTransfer/static/js/theme_default/viewer.min.js"></script>
    
        <script src="/StyleTransfer/static/css/theme_default/prism.min.js"></script>
    
        <script src="/StyleTransfer/static/js/search/search_main.js"></script>
    
        <script src="/StyleTransfer/static/js/custom.js"></script>
    
</body>

</html>
<!DOCTYPE html>

<html lang="zh" id="ref_ResNet" class="">


<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="keywords" content="ResNet">
    
    
    <meta name="description" content="ResNet文献及笔记">
    
    <meta name="generator" content="teedoc">
    <meta name="theme" content="teedoc-plugin-theme-default">
    
        
        <meta name="markdown-generator" content="teedoc-plugin-markdown-parser">
        
        <script>
MathJax = {"loader": {"load": ["output/svg"]}, "tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}, "svg": {"fontCache": "global"}};
</script>
        
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
        <script src="/StyleTransfer/static/js/theme_default/pre_main.js"></script>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/theme_default/prism.min.css" type="text/css"/>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/theme_default/viewer.min.css" type="text/css"/>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/theme_default/dark.css" type="text/css"/>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/theme_default/light.css" type="text/css"/>
        
        <script src="/StyleTransfer/static/js/theme_default/jquery.min.js"></script>
        
        <script src="/StyleTransfer/static/js/theme_default/split.js"></script>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/search/style.css" type="text/css"/>
        
        <link rel="stylesheet" href="/StyleTransfer/static/css/custom.css" type="text/css"/>
        
        <meta name="html-generator" content="teedoc-plugin-jupyter-notebook-parser">
        
    
    
    <title>ResNet：残差网络 - Style Transfer - Fingsinz</title>
    
    <script type="text/javascript">js_vars = {}</script>
    <script type="text/javascript">metadata = {"tags": [], "date": "2025-03-02", "update": [], "ts": 1740844800, "author": "", "brief": "", "cover": "", "id": "ref_ResNet"}</script>
</head>


<body class="type_doc">
    
    <div id="navbar">
        <div id="navbar_menu">
            <a class="site_title" href="/StyleTransfer/">
                
                
                    <h2>Style Transfer By Fingsinz</h2>
                
        </a>
            <a id="navbar_menu_btn"></a>
        </div>
        <div id="navbar_items">
            <div>
                <ul id="nav_left">
<li class=""><a  href="/StyleTransfer/supporting/">辅助材料</a></li>
<li class="active"><a  href="/StyleTransfer/ref_and_notes/">文献学习 & 笔记</a></li>
<li class=""><a  href="/StyleTransfer/paper/">论文正文</a></li>
</ul>

            </div>
            <div>
                <ul id="nav_right">
</ul>

                <ul class="nav_plugins"><li><a id="themes" class="light"></a></li></ul><ul class="nav_plugins"><li><a id="search"><span class="icon"></span><span class="placeholder">搜索</span>
                            <div id="search_hints">
                                <span id="search_input_hint">输入关键词，多关键词空格隔开</span>
                                <span id="search_loading_hint">正在加载，请稍候。。。</span>
                                <span id="search_download_err_hint">下载文件失败，请刷新重试或检查网络</span>
                                <span id="search_other_docs_result_hint">来自其它文档的结果</span>
                                <span id="search_curr_doc_result_hint">当前文档搜索结果</span>
                            </div></a></li></ul>
            </div>
        </div>
    </div>
    
    <div id="wrapper">
        <div id="sidebar_wrapper">
            <div id="sidebar">
                <div id="sidebar_title">
                    
                </div>
                <ul class="show">
<li class="not_active no_link"><a><span class="label">PyTorch 框架</span><span class="sub_indicator"></span></a><ul class="show">
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_install.html"><span class="label">PyTorch 环境安装</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_tensor.html"><span class="label">Tensor 张量</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_basic_workflow.html"><span class="label">PyTorch 基本工作流</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_classification.html"><span class="label">PyTorch 分类模型</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_computer_vision.html"><span class="label">PyTorch 中的计算机视觉</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_custom_datasets.html"><span class="label">PyTorch 中自定义数据集</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/pytorch_modular.html"><span class="label">PyTorch 模块化</span><span class=""></span></a></li>
</ul>
</li>
<li class="active_parent no_link"><a><span class="label">理论学习</span><span class="sub_indicator"></span></a><ul class="show">
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/vgg.html"><span class="label">VGG 卷积网络</span><span class=""></span></a></li>
<li class="active with_link"><a href="/StyleTransfer/ref_and_notes/resnet.html"><span class="label">ResNet：残差网络</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/unet.html"><span class="label">U-Net 卷积网络</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/gan.html"><span class="label">GAN：生成对抗网络</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/cgan.html"><span class="label">cGAN：条件 GAN</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/patchgan.html"><span class="label">PatchGAN 到多尺度 PatchGAN</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/cyclegan.html"><span class="label">CycleGAN：循环GAN</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/evaluation.html"><span class="label">风格迁移评价</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">风格迁移实战</span><span class="sub_indicator"></span></a><ul class="show">
<li class="not_active no_link"><a><span class="label">卷积神经网络的图像风格迁移（d2l）</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">特征变换 WCT</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">自适应实例归一化 AdaIN</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/StyleTransfer/ref_and_notes/metanet.html"><span class="label">Meta Networks</span><span class=""></span></a></li>
</ul>
</li>
</ul>

            </div>
        </div>
        <div id="article">
            <div id="menu_wrapper">
                <div id="menu">
                </div>
            </div>
            <div id="content_wrapper">
                <div id="content_body">
                    <div id="article_head">
                        <div id="article_title">
                            
                            <h1>ResNet：残差网络</h1>
                            
                        </div>
                        <div id="article_tags">
                            <ul>
                            
                            </ul>
                        </div>
                        <div id="article_info">
                        <div id="article_info_left">
                            <span class="article_author">
                                
                            </span>
                            
                                <span class="article_date" title="最后修改日期： 2025-03-02">
                                    2025-03-02
                                </span>
                            
                        </div>
                        <div id="article_info_right">
                            
                        </div>
                        </div>
                    </div>
                    <div id="article_tools">
                        <span></span>
                        <span id="toc_btn"></span>
                    </div>
                    <div id="update_history">
                        
                    </div>
                    <div id="article_content">
                        
                            <p><a href="https://arxiv.org/abs/1512.03385"  target="_blank">Deep Residual Learning for Image Recognition</a></p>
<p><em>He K , Zhang X , Ren S ,et al.Deep Residual Learning for Image Recognition[J].IEEE, 2016.DOI:10.1109/CVPR.2016.90.</em></p>
<blockquote>
<p>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.</p>
</blockquote>
<p><strong>摘要</strong>：更深的神经网络会更难进行训练。我们提出一个残差学习框架，可以简化比以前使用的网络深度更深的网络的训练。我们明确地将层重新表述为参考层输入的学习残差函数，而不是学习未参考的函数。我们提供了全面的经验证据，表明这些残差网络更容易优化，并且可以从相当大的深度中获得精度。在 ImageNet 数据集上，我们评估了深度高达 152 层的残差网络——比 VGG 网络深 8 倍，但仍然具有较低的复杂性。这些残差网络的集合在 ImageNet测试集上的误差达到 3.57%。该结果在 ILSVRC 2015 分类任务中获得第一名。我们还对 100 层和 1000 层的 CIFAR-10 进行了分析。表征的深度对于许多视觉识别任务至关重要。仅仅由于我们的深度表示，我们在 COCO 对象检测数据集上获得了 28% 的相对改进。深度残差网络是我们提交 ILSVRC 和 COCO 2015 竞赛的基础，我们还在 ImageNet 检测，ImageNet 本地化，COCO 检测和 COCO 分割任务中获得了第一名。</p>
<h2 id="%E6%AE%8B%E5%B7%AE%E5%9D%97%E6%A8%A1%E5%9E%8B">残差块模型</h2>
<p>通过引入残差块（Residual Block）和快捷连接（Shortcut Connection），解决了深度神经网络训练中的退化问题（随着深度增加，训练误差不降反升）。</p>
<ol>
<li><p><strong>残差块</strong>：将网络层的映射目标从 <strong>直接学习目标函数 $\mathcal{H}(x)$</strong> 变成 <strong>学习残差函数 $\mathcal{F}(x)=\mathcal{H}(x)-x$</strong>，最终输出为 $\mathcal{F}(x)+x$。</p>
<ul>
<li>若恒等映射（Identity Mapping）是最优解，则残差函数更容易趋近于零，简化了优化过程。</li>
</ul>
</li>
</ol>
<blockquote>
<p>注：恒等映射是一个输入与输出相同的函数，即 $f(x)=x$。在神经网络中，它表现为直接将输入传递到输出，不做任何变换。</p>
</blockquote>
<ol start="2">
<li><p><strong>快捷连接</strong>：在堆叠层之间添加跨层连接，直接将输入与输出相加，无需额外参数。</p>
<ul>
<li>快捷连接直接将输入 $x$ 加到残差函数 $\mathcal{F}(x)$ 的输出上，相当于强制网络仅学习输入与目标之间的 <strong>残差</strong>（差异），而非完整的映射。</li>
<li>整个网络仍然可以通过反向传播的 SGD 进行端到端训练。</li>
</ul>
</li>
</ol>
<p><img src="../static/images/ResNet/fig1.png" alt="残差块" /></p>
<h2 id="%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA">残差网络理论</h2>
<p>论文表明：</p>
<ul>
<li>极深残差网络很容易优化，但是当深度增加时，对应的“普通”网络（简单地堆叠层）会表现出更高的训练误差；</li>
<li>深度残差网络可以很容易地从显着增加的深度中获得准确度增益，产生的结果比以前的网络要好得多。</li>
</ul>
<p>与先前工作的关键区别：</p>
<ul>
<li>残差函数 vs 完整映射：ResNet显式学习残差（$\mathcal{F}(x) = \mathcal{H}(x) - x$），而传统网络直接学习 $\mathcal{H}(x) $。</li>
<li>无门控 vs 门控：高速公路网络依赖参数化的门控函数，而ResNet通过恒等映射，无需参数，保持信息无损传递。</li>
</ul>
<h3 id="%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0">残差学习</h3>
<p>如果假设多个非线性层可以渐近逼近复杂函数 $\mathcal{H}(x)$，则等价于假设这多个非线性层可以渐近逼近残差函数。残差学习将其重新定义为 $\mathcal{F}(x) = \mathcal{H}(x) - x$（假设输入和输出具有相同的维数）。</p>
<p>因此，不需要期望堆叠层来近似 $\mathcal{H}(x)$，而实际学习的是残差函数，最终输出为 $y=\mathcal{F}(x)+x$。</p>
<ul>
<li>若恒等映射 $\mathcal{H}(x)=x$ 是最优解，则残差函数 $\mathcal{F}(x)$ 只需趋近于零，比直接学习完整映射更简单。</li>
<li>在实际场景中，恒等映射不太可能是最优的，但我们的重新表达可能有助于解决这个问题。</li>
</ul>
<p>如果最优函数更接近于恒等映射而不是零映射，求解器应该更容易找到参考恒等映射的扰动，而不是学习函数作为新函数。</p>
<h3 id="%E5%BF%AB%E6%8D%B7%E8%BF%9E%E6%8E%A5">快捷连接</h3>
<p>输入 $x$ 直接跨过堆叠的层，与残差函数 $\mathcal{F}(x)$ 的输出相加。</p>
<ul>
<li>当输入输出维度相同时，快捷连接直接传递 $x$（无参数）；维度不同时，快捷连接执行线性投影 $W_s$ 以匹配维度：</li>
</ul>
$$
\begin{aligned}
\text{y}=&amp;\mathcal{F}(x)+x,&amp; 维度相同\\
\text{y}=&amp;\mathcal{F}(x)+W_s x,&amp; 维度不同
\end{aligned}
$$<p>残差块中，ReLU 激活函数位于残差函数 $\mathcal{F}(x)$ 之后，即：$y=\sigma(\mathcal{F}(x)+x)$，其中 $\sigma$ 表示 ReLU。</p>
<p>并且，残差函数是灵活的，可以包含多个堆叠层，全连接层、卷积层等等。但只有一个层时，并没有优势。</p>
<h3 id="%E7%93%B6%E9%A2%88%E7%BB%93%E6%9E%84">瓶颈结构</h3>
<p>为了降低计算复杂度，适用于极深网络（如ResNet-50/101/152）。</p>
<p>组成：1 × 1 卷积（降维）→ 3 × 3 卷积 → 1 × 1 卷积（恢复维度），形成“瓶颈”结构（下图右边）。</p>
<p><img src="../static/images/ResNet/fig2.png" alt="瓶颈结构" /></p>
<h3 id="%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84">网络架构</h3>
<p><img src="../static/images/ResNet/fig3.png" alt="ResNet" /></p>
<p>ResNet-9 适用于轻量级任务，如CIFAR-10。补充 ResNet-9 的大致架构：</p>
<table>
<thead>
<tr>
  <th style="text-align:center">层级</th>
  <th style="text-align:center">操作</th>
</tr>
</thead>
<tbody>
<tr>
  <td style="text-align:center">conv1</td>
  <td style="text-align:center">3 × 3 卷积，64 通道，步长为 1</td>
</tr>
<tr>
  <td style="text-align:center">conv2_x</td>
  <td style="text-align:center">1 个基础残差块（2 层 3 × 3 卷积， 64 通道）</td>
</tr>
<tr>
  <td style="text-align:center">conv3_x</td>
  <td style="text-align:center">1 个基础残差块（2 层 3 × 3 卷积， 128 通道，下采样）</td>
</tr>
<tr>
  <td style="text-align:center">conv4_x</td>
  <td style="text-align:center">1 个基础残差块（2 层 3 × 3 卷积， 256 通道，下采样）</td>
</tr>
<tr>
  <td style="text-align:center">全局平均池化</td>
  <td style="text-align:center">输出尺寸 1 × 1</td>
</tr>
<tr>
  <td style="text-align:center">全连接层</td>
  <td style="text-align:center">若干个神经元输出</td>
</tr>
</tbody>
</table>
<p>大致对比：</p>
<table>
<thead>
<tr>
  <th style="text-align:center">网络</th>
  <th style="text-align:center">残差块类型</th>
  <th style="text-align:center">阶段块数（conv2_x到conv5_x）</th>
</tr>
</thead>
<tbody>
<tr>
  <td style="text-align:center">ResNet-9</td>
  <td style="text-align:center">基础块</td>
  <td style="text-align:center">[1,1,1,0]</td>
</tr>
<tr>
  <td style="text-align:center">ResNet-18</td>
  <td style="text-align:center">基础块</td>
  <td style="text-align:center">[2,2,2,2]</td>
</tr>
<tr>
  <td style="text-align:center">ResNet-34</td>
  <td style="text-align:center">基础块</td>
  <td style="text-align:center">[3,4,6,3]</td>
</tr>
<tr>
  <td style="text-align:center">ResNet-50</td>
  <td style="text-align:center">瓶颈块</td>
  <td style="text-align:center">[3,4,6,3]</td>
</tr>
<tr>
  <td style="text-align:center">ResNet-101</td>
  <td style="text-align:center">瓶颈块</td>
  <td style="text-align:center">[3,4,23,3]</td>
</tr>
<tr>
  <td style="text-align:center">ResNet-152</td>
  <td style="text-align:center">瓶颈块</td>
  <td style="text-align:center">[3,8,36,3]</td>
</tr>
</tbody>
</table>
<h2 id="%E8%AE%BA%E6%96%87%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93">论文实验总结</h2>
<p>论文的第四部分 Experiments 通过大量实验验证了残差网络（ResNet）的性能优势，涵盖多个数据集和任务，并深入分析了不同设计选择的影响。</p>
<p><em>DeepSeek 总结</em></p>
<h3 id="ImageNet%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%AE%9E%E9%AA%8C">ImageNet图像分类实验</h3>
<p>实验设置：</p>
<ul>
<li>数据集：ImageNet 2012（1.28M 训练图像，50K 验证图像）。</li>
<li>网络结构：测试了 18 层、34 层、50 层、101 层、152 层的普通网络（Plain Net）和残差网络（ResNet）。</li>
<li>训练细节：SGD 优化器，初始学习率 0.1，分阶段衰减；数据增强包括随机裁剪、水平翻转、颜色扰动；使用批量归一化（BN）但无 Dropout。</li>
</ul>
<p>关键结果：</p>
<ul>
<li>退化问题验证：34 层普通网络的训练误差高于 18 层普通网络，而 34 层 ResNet 显著优于 18 层 ResNet，证明残差学习解决了退化问题。</li>
<li>极深网络表现：ResNet-152 在 ImageNet 上实现单模型 3.57% top-5 错误率（集成模型），赢得 ILSVRC 2015 分类任务冠军。</li>
<li>对比 VGG 和 GoogLeNet：ResNet-152 的复杂度（11.3 亿 FLOPs ）低于 VGG-19（19.6 亿 FLOPs），但性能显著更优。</li>
</ul>
<h3 id="CIFAR-10%E5%88%86%E6%9E%90%E5%AE%9E%E9%AA%8C">CIFAR-10分析实验</h3>
<p>实验目的：验证残差学习在更小数据集上的泛化能力。<br />
网络设计：堆叠简单残差块（每个残差块包含两个3×3卷积层），测试20层到1202层的网络。<br />
关键发现：</p>
<ul>
<li>极深网络的可行性：ResNet-110（170 万参数）达到 6.43% 测试错误率，优于当时的先进方法（如高速公路网络）。</li>
<li>1202层网络训练：虽然训练误差趋近于零，但测试误差因过拟合略升至 7.93%，表明需要更强正则化。</li>
</ul>
<h3 id="%E5%BF%AB%E6%8D%B7%E8%BF%9E%E6%8E%A5%E7%9A%84%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C">快捷连接的消融实验</h3>
<p>对比选项：</p>
<ul>
<li>选项 A：维度不匹配时用零填充（无参数）。</li>
<li>选项 B：维度不匹配时用 1 × 1 卷积调整（含参数）。</li>
<li>选项 C：所有快捷连接均为 1 × 1 卷积（大量参数）。</li>
</ul>
<p>结论：</p>
<ul>
<li>选项 B 略优于 A：因零填充无法学习残差。</li>
<li>选项 C 提升有限：参数过多但收益不高，最终选择选项 B 用于极深网络（如 ResNet-50/101/152）。</li>
</ul>
<h3 id="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%88%86%E5%89%B2%E5%AE%9E%E9%AA%8C">目标检测与分割实验</h3>
<p>任务与框架：</p>
<ul>
<li>目标检测：基于 Faster R-CNN，将 VGG-16 替换为 ResNet-101。</li>
<li>数据集：PASCAL VOC 2007/2012、MS COCO。</li>
</ul>
<p>关键结果：</p>
<ul>
<li>COCO 检测：ResNet-101 相比 VGG-16，mAP@[.5, .95] 提升 6%（相对提升28%），验证了深层特征的泛化能力。</li>
<li>竞赛表现：在 ILSVRC 2015 中，ResNet 赢得检测、定位、分割任务冠军。</li>
</ul>
<h3 id="%E6%9E%81%E6%B7%B1%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%8E%E5%88%86%E6%9E%90">极深网络的可视化与分析</h3>
<p>残差响应分析：</p>
<ul>
<li>ResNet的残差函数输出的响应值普遍较小，表明网络更倾向于学习接近恒等映射的微调。</li>
<li>网络越深，单个残差块的调整幅度越小，验证了残差学习的“微扰动”假设。</li>
</ul>
<h3 id="%E5%AF%B9%E6%AF%94%E5%85%B6%E4%BB%96%E5%85%88%E8%BF%9B%E6%A8%A1%E5%9E%8B">对比其他先进模型</h3>
<p>与高速公路网络对比：</p>
<ul>
<li>ResNet在超过100层时仍能提升性能，而高速公路网络（Highway Networks）在极深时表现下降。</li>
<li>ResNet的快捷连接无参数，计算更高效。</li>
</ul>
<h2 id="%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">代码实现</h2>
<p>基于残差网络的 CIFAR-10 分类实验代码在 <a href="https://github.com/Fingsinz/StyleTransfer/blob/main/src/01.ref_and_note/03.ResNet.py"  target="_blank">Github</a>。</p>
<p>实验记录如下：</p>
<table>
    <tr>
        <td><img src="../static/images/ResNet/fig4.png" /></td>
        <td><img src="../static/images/ResNet/fig5.png" /></td>
    </tr>
</table>
<p>部分代码如下：</p>
<details>
<summary>实现简单残差块</summary>

<pre class="language-python"><code class="language-python">class BasicBlock(nn.Module):
    &quot;&quot;&quot;残差块&quot;&quot;&quot;
    expansion = 1

    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(BasicBlock, self).__init__()

        # 第一层卷积 → 批量归一化 → ReLU → 第二层卷积 → 批量归一化
        self.conv1 = nn.Conv2d(
            in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False
        )
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(
            out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False
        )
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.downsample = downsample

    def forward(self, x):
        identity = x            # 恒等映射

        # 第一层卷积 → 批量归一化 → ReLU → 第二层卷积 → 批量归一化
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity         # 残差连接，F(x) + x
        out = self.relu(out)    # ReLU
        return out
</code></pre>
</details>
<details>
<summary>基于残差块构建 ResNet-9</summary>

<pre class="language-python"><code class="language-python">class ResNet9(nn.Module):
    &quot;&quot;&quot;ResNet9 model&quot;&quot;&quot;

    def __init__(self, num_classes):
        super(ResNet9, self).__init__()
        self.in_channels = 64

        # 初始卷积层
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # 残差块
        self.layer1 = self._make_layer(BasicBlock, 64, 1, stride=1)
        self.layer2 = self._make_layer(BasicBlock, 128, 1, stride=2)
        self.layer3 = self._make_layer(BasicBlock, 256, 1, stride=2)

        # 全局平均池化和全连接层
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256 * BasicBlock.expansion, num_classes)

    def _make_layer(self, block, out_channels, blocks, stride):
        downsample = None
        # 检查是否需要下采样：如果步长不为 1 或输入输出通道数不匹配，则创建下采样层。
        if stride != 1 or self.in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(
                    self.in_channels,
                    out_channels * block.expansion,
                    kernel_size=1,
                    stride=stride,
                    bias=False
                ),
                nn.BatchNorm2d(out_channels * block.expansion),
            ) 
        layers = []
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.in_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x
</code></pre>
</details>
<details>
<summary>基于残差块构建 ResNet-9</summary>

<pre class="language-python"><code class="language-python">class ResNet18(nn.Module):
    &quot;&quot;&quot;ResNet18 model&quot;&quot;&quot;

    def __init__(self, num_classes=1000):
        super(ResNet18, self).__init__()
        self.in_channels = 64
        # 初始卷积层
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        # 残差块
        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)
        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)
        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)
        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)
        # 全局平均池化和全连接层
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)

    def _make_layer(self, block, out_channels, blocks, stride):
        downsample = None
        if stride != 1 or self.in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(
                    self.in_channels,
                    out_channels * block.expansion,
                    kernel_size=1,
                    stride=stride,
                    bias=False
                ),
                nn.BatchNorm2d(out_channels * block.expansion),
            )
        layers = []
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.in_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x
</code></pre>
</details>

                        
                    </div>
                </div>
                <div id="previous_next">
                    <div id="previous">
                        
                        <a href="/StyleTransfer/ref_and_notes/vgg.html">
                            <span class="icon"></span>
                            <span class="label">VGG 卷积网络</span>
                        </a>
                        
                    </div>
                    <div id="next">
                        
                        <a href="/StyleTransfer/ref_and_notes/unet.html">
                            <span class="label">U-Net 卷积网络</span>
                            <span class="icon"></span>
                        </a>
                        
                    </div>
                </div>
                <div id="comments-container"></div>
            </div>
            <div id="toc_wrapper">
                <div id="toc">
                    <div id="toc_content">
                            
                    </div>
                </div>
            </div>
        </div>
    </div>
    <a id="to_top" href="#"></a>
    <div id="doc_footer">
        <div id="footer">
            <div id="footer_top">
                <ul>
<li><a></a><ul><li><a target="_blank" href="/StyleTransfer/#"></a></li>
</ul>
</li>
</ul>

            </div>
            <div id="footer_bottom">
                <ul>
<li><a target="_blank" href="https://github.com/teedoc/teedoc">Generated by teedoc - Fingsinz - 2024.12.29</a></li>
</ul>

            </div>
        </div>
    </div>
    
        <script src="/StyleTransfer/teedoc-plugin-markdown-parser/mermaid.min.js"></script>
    
        <script>mermaid.initialize({startOnLoad:true});</script>
    
        <script src="/StyleTransfer/static/js/theme_default/tocbot.min.js"></script>
    
        <script src="/StyleTransfer/static/js/theme_default/main.js"></script>
    
        <script src="/StyleTransfer/static/js/theme_default/viewer.min.js"></script>
    
        <script src="/StyleTransfer/static/css/theme_default/prism.min.js"></script>
    
        <script src="/StyleTransfer/static/js/search/search_main.js"></script>
    
        <script src="/StyleTransfer/static/js/custom.js"></script>
    
</body>

</html>